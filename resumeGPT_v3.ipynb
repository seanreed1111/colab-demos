{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4/JwtW6GMdWmItwfxKFzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanreed1111/colab-demos/blob/master/resumeGPT_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "based on https://github.com/openai/openai-cookbook/tree/main/examples"
      ],
      "metadata": {
        "id": "ZQxWNb57WrWz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCG2144SWqEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO  \n",
        "- Return a link to the full original resume instead of to chunks\n",
        "- Run experiments to find optimal chunking size of resume text\n",
        "- Figure out how to use Ada and Babbage to reduce costs\n",
        "- port to Azure Platform\n",
        "\n"
      ],
      "metadata": {
        "id": "58pU0G9FGWyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq loguru textract tiktoken openai azure-ai-ml mlflow azureml-sdk azureml-mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poRnH-oftTPb",
        "outputId": "0ccaded0-cd4b-4308-8a71-f1a8f1a303fe",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m814.0/814.0 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.9/173.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.2/136.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.7/245.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.7/245.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.4/779.4 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.9/965.9 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.4/141.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.4/245.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.1/31.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.7/191.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fusepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.18 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# before running this notebook, UPLOAD these files\n",
        "- openai.env\n",
        "- azure.env"
      ],
      "metadata": {
        "id": "gqPb-Nn776tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os,argparse,loguru, json, time, datetime, openai\n",
        "from pathlib import Path\n",
        "from loguru import logger"
      ],
      "metadata": {
        "id": "scY-DzUJquiS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_load_aml_env_vars(env_path=None):\n",
        "  import os, json\n",
        "  try:\n",
        "    with open(env_path, \"r\") as f:\n",
        "      env_vars = json.load(f)\n",
        "    os.environ[\"resource_group\"] = env_vars[\"resource_group\"]\n",
        "    os.environ[\"workspace_name\"] = env_vars[\"workspace_name\"]\n",
        "    os.environ[\"subscription_id\"] = env_vars[\"subscription_id\"]\n",
        "    if (os.getenv(\"resource_group\") and os.getenv(\"workspace_name\")\n",
        "    and os.getenv(\"subscription_id\")):\n",
        "      return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "    return False"
      ],
      "metadata": {
        "id": "A1d_9gbMZ78f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_open_ai_key(env_path=None):\n",
        "  import json, os\n",
        "  from pathlib import Path\n",
        "  try:\n",
        "    with open(env_path, \"r\") as f:\n",
        "        env_vars = json.load(f)\n",
        "    os.environ[\"OPENAI_API_KEY\"] = env_vars[\"OPENAI_API_KEY\"]\n",
        "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "    openai.Model.list() #test a random command on the openai API\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "  return False\n",
        "\n",
        "def test_set_open_ai_key(key_path=None):\n",
        "  openai.api_key = None #disconnect from api key if already registered\n",
        "  try:\n",
        "    set_open_ai_key(key_path)\n",
        "    openai.Model.list()\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "wwZXC6jUTLvh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_get_ml_client(env_path=None):\n",
        "  # this is a mix of sdk v1 and v2. Try to consolidate \n",
        "  import json, os, mlflow\n",
        "  from pathlib import Path\n",
        "  from azureml.core import Workspace \n",
        "  from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "  from azure.ai.ml import MLClient\n",
        "\n",
        "  if not env_path: return None\n",
        "\n",
        "  ws = Workspace.from_config(env_path)\n",
        "  tracking_uri = ws.get_mlflow_tracking_uri()\n",
        "  mlflow.set_tracking_uri(tracking_uri)\n",
        "\n",
        "  try:\n",
        "      credential = DefaultAzureCredential()\n",
        "  except Exception as ex:\n",
        "      # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not working\n",
        "      credential = InteractiveBrowserCredential()\n",
        "\n",
        "  is_loaded = maybe_load_aml_env_vars(env_path)\n",
        "  if is_loaded:\n",
        "    try:\n",
        "      ml_client = MLClient(\n",
        "          subscription_id=os.getenv(\"subscription_id\"),\n",
        "          resource_group_name=os.getenv(\"resource_group\"),\n",
        "          workspace_name=os.getenv(\"workspace_name\"),\n",
        "          credential=credential,\n",
        "      )\n",
        "      return ml_client\n",
        "    except Exception as e:\n",
        "      logger.error(f\"{e}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "zNLN8FKHzlAB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_setup_azure_env(azure_env_path=None):\n",
        "  # setup azure ml env if azure credentials are available.\n",
        "  if azure_env_path and azure_env_path.is_file() and maybe_load_aml_env_vars(azure_env_path):\n",
        "    ml_client = maybe_get_ml_client(azure_env_path)\n",
        "    if ml_client:\n",
        "      #do a random test to check that ml_client and mlflow are playing nicely together\n",
        "      import mlflow\n",
        "      experiment_name = 'mlflow-2'\n",
        "      mlflow.set_experiment(experiment_name)\n",
        "      from random import random\n",
        "\n",
        "      with mlflow.start_run() as mlflow_test_run:\n",
        "          mlflow.log_param(\"hello_param\", \"world\")\n",
        "          mlflow.log_metric(\"hello_metric2\", random())\n",
        "          os.system(f\"echo 'hello world2' > helloworld2.txt\")\n",
        "          mlflow.log_artifact(\"helloworld2.txt\")\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "pzzqgJ9k9f34"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "azure_env_path, openai_env_path, ml_client, openai.api_key = None, None, None , None\n",
        "cwd = Path.cwd()\n",
        "# resume_path = cwd / \"Resumes\"\n",
        "# resume_path.mkdir(exist_ok=True)\n",
        "\n",
        "azure_env_path = cwd / \"azure.env\" ##uncomment if providing azure env\n",
        "openai_env_path = cwd/ \"openai.env\"\n",
        "maybe_setup_azure_env(azure_env_path)\n",
        "set_open_ai_key(openai_env_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqHlEmz8CsZ",
        "outputId": "5dead50d-9e13-464c-8462-72d4ed36d5d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPLIT SECTIONS\n",
        "source: Embedding_Wikipedia_articles_for_search.ipynb \n",
        "https://colab.research.google.com/drive/1EJMtCmF8jZc2Y-c1RaBxFSCTPcjzjJf4#scrollTo=TOVSYkDur9zA"
      ],
      "metadata": {
        "id": "5kUr4TQLdnRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll recursively split long sections into smaller sections.\n",
        "\n",
        "There's no perfect recipe for splitting text into sections.\n",
        "\n",
        "Some tradeoffs include:\n",
        "- Longer sections may be better for questions that require more context\n",
        "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
        "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
        "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
        "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
        "\n",
        "Here, we'll use a simple approach and limit sections to 1,000 tokens each by default, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
      ],
      "metadata": {
        "id": "NVpjUKLwenCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### extract text from pdf"
      ],
      "metadata": {
        "id": "pK5mrZ8PJ4L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract, os, openai, tiktoken"
      ],
      "metadata": {
        "id": "fJ-sflJ77VjU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = 'gpt-3.5-turbo'  # only matters insofar as it selects which tokenizer to use\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "## TODO This needs more sophistication in the use of a delimiter\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # no delimiter found\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # no need to search for halfway point\n",
        "    else:\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        best_diff = halfway\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "def truncated_string(\n",
        "    string: str,\n",
        "    model: str,\n",
        "    max_tokens: int,\n",
        "    print_warning: bool = False,\n",
        "    TRUNCATION_WARNING_PERCENTAGE: float = 0.25\n",
        "\n",
        ") -> str:\n",
        "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    truncation_percentage = 1.0 - max_tokens*1.0 / len(encoded_string)\n",
        "    if print_warning and (len(encoded_string) > max_tokens) and (truncation_percentage > TRUNCATION_WARNING_PERCENTAGE):\n",
        "        logger.warning(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens. \\nOriginalString:{string} \\n\")\n",
        "\n",
        "    return truncated_string"
      ],
      "metadata": {
        "id": "u3CZ5MwnnspR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str], #legacy structure. \n",
        "    max_tokens: int = 1000,\n",
        "    model: str = GPT_MODEL,\n",
        "    max_recursion: int = 8,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
        "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # if length is fine, return string\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # if recursion hasn't found a split after X iterations, just truncate\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # otherwise, split in half and recurse\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \", \"●\", \"•\", \":\", \";\"]:\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # if either half is empty, retry with a more fine-grained delimiter\n",
        "                continue\n",
        "            else:\n",
        "                # recurse on each half\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1,\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # otherwise no split was found, so just truncate (should be very rare)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        " "
      ],
      "metadata": {
        "id": "9JmwS-cZdlvp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load resumes from csv file made by pdf parser and then split and create embeddings.\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/resume_books.csv\",usecols=[\"text\", \"source\"]);df.head()\n",
        "df = df.dropna()\n",
        "df.loc[:,\"name\"] = [f\"Name: {i}\" for i in df.index]\n",
        "df.loc[:,\"text2\"] = list(zip(df[\"name\"].to_list(), df[\"text\"].to_list()))\n",
        "df = df[[\"name\",\"text2\"]];df.head()\n",
        "df.to_csv(\"resume_books_v2.csv\")\n",
        "clean_texts = df[\"text2\"].to_list()\n",
        "clean_texts = [([x1],x2) for x1, x2 in clean_texts]"
      ],
      "metadata": {
        "id": "VAtkxHnlD6YQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_texts[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvhrram8ACkT",
        "outputId": "449f664b-55c6-477d-dfc2-9cb60640dc16"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Name: 2'],\n",
              " 'RUIZE CHEN  \\n(585) 540-6418 // ruize.chen@nyu.edu  // linkedin.com/in/ ruize-chen  \\nEDUCATION  \\nExpected 12/23 NEW YORK UNIVERSITY                 New York, NY \\nThe Courant Institute of Mathematical Sciences \\nM.S. in Mathematics in Finance \\n● Expected Coursework:  object-oriented programming (Java), stochastic calculus, Brownian motion, \\nFama-French, Black-Scholes, risk and portfolio management , data-driven modeling \\n08/18 - 05/22 UNIVERSITY OF ROCHESTER                               Rochester, NY \\n  B.A. in Mathematics and Statistics & B.S in Finance \\n● Coursework:  linear algebra, ordinary differential equations, real analysis, stochastic processes, \\nprobability theory, linear regression, mean-variance optimization, corporate finance \\n● Honors/Awards:  Dean’s List (3 years), Cum Laude, Beta Gamma Sigma Honor Society \\nEXPERIENCE  \\n06/21 - 08/21 NORTHEAST SECURITIES              Shenzhen, China  \\n(Top 25 Chinese securities firm)  \\nQuantitative Research Intern   \\n● Identified factors, from firm’s database, that better predicted stock returns, by calculating  \\ninformation coefficients (i.e., correlation between factor value and stock yield) \\n● Constructed new stock selection factors using principal component and cluster analyses \\n● Applied Python to carry out web crawler for acquiring Chinese real estate data (e.g., construction  \\narea, floor area ratio) to support research on future housing trends; stored data using MongoDB \\n● Preprocessed acquired data with log transformation and performed exploratory data analysis and \\ngraphed time series plots to examine housing construction patterns over past 10 years \\n01/21 - 02/21 INDUSTRIAL SECURITIES                    Guangzhou, China \\n(Top 7 Chinese securities firm) \\nQuantitative Research Intern  \\n● Employed quantitative stock selection methodology to healthcare stocks \\n● Reproduced factor construction process with random forest model to extract most influential ones; \\nbuilt linear model based on selected factors \\n● Achieved annualized returns of 28% and Sharpe ratio of 1.5 from derived factor model  \\nPROJECTS  \\n04/22 - 05/22 UNIVERSITY OF ROCHESTER                  Rochester, NY \\nStudy on Factors Affecting Likelihood of Having Heart Disease (Python) \\n● Built logistic regression, random forest, and artificial neural network via NumPy, pandas, and  \\nscikit-learn packages to explore possible impact of factors such as blood pressure \\n● Evaluated performance of each model and achieved recall of 97% \\n04/21 - 05/21 Optimal Risk and Return Portfolio Construction (Excel)  \\n● Collected 60 years’ monthly returns of 3 types of Fama-French risky assets; measured their  \\nvariances, covariances, and correlations to derive mean-variance efficient portfolios \\n● Created CAPM linear regression model in Excel; evaluated excess return rate and influential  \\ndegree brought by the 3 Fama-French assets \\n03/21 - 04/21 Analysis of Rochester Housing Market (R)  \\n● Performed linear regression, stepwise regression, ANOVA test, and Tukey’s HSD test to  \\nexamine how factors (e.g., architectural style, location) could affect Rochester home sales  \\nprices; utilized ggplot2 package to create statistical plots \\n● Derived best fit linear model with metrics including AIC and R-squared; constructed confidence  \\nand prediction intervals \\nCOMPUTATIONAL SKILLS / OTHER  \\nProgramming Languages:  Java, Python, R, VBA, Excel, Tableau, MongoDB \\nLanguages: English (fluent), Mandarin (native), Cantonese (native), German (intermediate) ')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_texts[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr_sTS7D_nkP",
        "outputId": "964328c2-ab3d-487d-cca5-99d748ec64cb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Name: 191'],\n",
              " 'MICHELLE L. DEMOTTE  \\nBronx, NY 10458  | (203) -241-330 1 | mdemotte@fordham.edu | www.linkedin.com/in/michelle -demotte   \\n  \\nEDUCATION  \\nFordham University, Gabelli School of Business           Bronx, New York  \\nBachelor of Sc ience in Business Administration , Major in Finance     September 2022  – May 2024  \\nProject:  Gabelli School of Business Consulting Cup Challenge  (2022)  \\n• Collaborat ing with a five -person team to develop a thorough financial and strategic analysis of Nordstrom  by \\nidentifying challenges, analyzing previous financial reports, and developing a marketing/implementation strategy   \\n \\nWestern Connecticut State University, Ancell College of Business             Danbury, Connecticut  \\nKathwari Honors Program ,           September 2021 - May 202 2 \\nGPA:  3.93/4.0, Dean’s List               \\n \\nSacred Heart University , Thomas More Honors Program              Fairfield, Connecticut  \\nGPA: 3.86/4.0, Dean’s List           September 2020 - May 2021  \\n \\nEXPERIENCE  \\nBrand Ambassador                  Danbury, Connecticut  \\nAerie                       August 2021 – Present  \\n• Ensured safe and accurate transfer of depository documents including witnessing, verifying, and transporting cash \\n• Supported  management and completed  tasks such as shipment, post -season transfers, and floor set  as well as \\ncashiering activities including purchases, returns, and exchanges in keeping with store policies \\n• Amplified guests’ experiences  with superior custome r service skills and professionalism  \\n \\nHome Delivery Shopper                      Danbury, Connecticut  \\nStop and Shop                     May 2020 - June 2021  \\n• Adapted to ever -changing COVID guidelines while performing fast -paced and time -sensitive  tasks  \\n• Initiated a leadership position t o support and uplift coworkers and increase workplace productivity \\n• Assembled grocery orders with timeliness and accuracy while exceeding the hourly fulfillment expectation  \\n Childcare Provider                             Brewster, New York  \\nSelf-employed              September 2018  – September 2022  \\n• Developed  and maintained accurate payroll documents stating reimbursements, credit, and payment schedules  \\n• Cultivated  lifelong relationships with 2  girls and completed daily  household duties thoughtfully \\n• Communicated clearly on a weekly basis about satisfying and exceeding family expectations and needs  \\n \\nVOLUNTEER EXPERIENCE  \\nTails of Courage                                Danbury, Connecticut  \\nFounder  and President , Volunteer         September 2018 - May 2019  \\n• Pioneered a Tails of Courage high school club with the corresponding local animal shelter  \\n• Collected and hand-deliver ed supplies including hundreds of dollars’ worth  of cash donations to the shelter  \\n• Organized  and oversaw bake sales, Chipotle fundraisers, and school -wide collections for materials and funding  \\n \\nSacred Heart University Journey                 Fairfield, Connecticut \\nVolunteer                                     Summer 2018, 2019  \\n• Centralized efforts to support lo cal gardens, housing developments, and communities in the greater Bridgeport area  \\n• Assessed community needs and developed solutions followed by insightful reflection in small group settings  \\n \\nACTIVITIES/INTERESTS  \\nSkills: Advanced Microsoft Excel, PowerPoint, Word, and Outlook \\nRelevant Coursework: Financial Accounting, Information Systems, Statistical Decision Making, Business Statistics  \\nInterests: Cooking, self-improvement, spending time outside, diversity, Minecraft, and traveling  ')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split resumes into chunks. Small chunks probably better when searching for skills? \n",
        "# maybe even shrink to individual sentences\n",
        "MAX_TOKENS = 50\n",
        "resume_strings = []\n",
        "for section in clean_texts:\n",
        "    resume_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(clean_texts)} resumes split into {len(resume_strings)} strings.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOlixxaYpCRC",
        "outputId": "1554c30c-d76c-43e5-e9ff-af3f33bfa4a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "191 resumes split into 4418 strings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_strings[-5:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsriC0x-_x5N",
        "outputId": "90b00a31-cd41-460b-c109-b08f6681c617"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Name: 191\\n\\n \\nSacred Heart University Journey                 Fairfield, Connecticut \\nVolunteer                                     Summer 2018, 2019  ',\n",
              " 'Name: 191\\n\\n• Centralized efforts to support lo cal gardens, housing developments, and communities in the greater Bridgeport area  ',\n",
              " 'Name: 191\\n\\n• Assessed community needs and developed solutions followed by insightful reflection in small group settings  \\n \\nACTIVITIES/INTERESTS  ',\n",
              " 'Name: 191\\n\\nSkills: Advanced Microsoft Excel, PowerPoint, Word, and Outlook \\nRelevant Coursework: Financial Accounting, Information Systems, Statistical Decision Making, Business Statistics  ',\n",
              " 'Name: 191\\n\\nInterests: Cooking, self-improvement, spending time outside, diversity, Minecraft, and traveling  ']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculate embeddings and store in dataframe"
      ],
      "metadata": {
        "id": "Izi2RX_O1u0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
        "MAX_BATCH_SIZE = 1000 # you can submit up to 2048 embedding inputs per request\n",
        "NUMBER_OF_STRINGS_TO_EMBED = len(resume_strings)\n",
        "\n",
        "if NUMBER_OF_STRINGS_TO_EMBED < MAX_BATCH_SIZE:\n",
        "  BATCH_SIZE = NUMBER_OF_STRINGS_TO_EMBED\n",
        "else: \n",
        "  BATCH_SIZE = MAX_BATCH_SIZE \n",
        "\n",
        "embeddings = []\n",
        "for batch_start in range(0, NUMBER_OF_STRINGS_TO_EMBED, BATCH_SIZE):\n",
        "    batch_end = batch_start + BATCH_SIZE\n",
        "    batch = resume_strings[batch_start:batch_end]\n",
        "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
        "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
        "    for i, be in enumerate(response[\"data\"]):\n",
        "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
        "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
        "    embeddings.extend(batch_embeddings)\n",
        "\n",
        "df = pd.DataFrame({\"text\": resume_strings, \"embedding\": embeddings})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hciff-Ul1t1B",
        "outputId": "dc7b9694-dcd6-4891-dd24-2317fcc729ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 to 999\n",
            "Batch 1000 to 1999\n",
            "Batch 2000 to 2999\n",
            "Batch 3000 to 3999\n",
            "Batch 4000 to 4999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store embeddings\n",
        "df.to_csv(\"embeddings_v2.csv\", index=False)"
      ],
      "metadata": {
        "id": "4WOqGbzmViZc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8xSg_aVQK8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "m9GE6wLE_iLh",
        "outputId": "b07e928c-8e7a-477c-9995-06350c95c45a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0   Name: 0\\n\\n/content/resume_book_2023_courant.pdf   \n",
              "1              Name: 1\\n\\nRESUME BOOK\\nCLASS OF 2023   \n",
              "2  Name: 2\\n\\nRUIZE CHEN  \\n(585) 540-6418 // rui...   \n",
              "3  Name: 2\\n\\nExpected 12/23 NEW YORK UNIVERSITY ...   \n",
              "4  Name: 2\\n\\nM.S. in Mathematics in Finance \\n● ...   \n",
              "\n",
              "                                           embedding  \n",
              "0  [0.00010535831097513437, -0.012883378192782402...  \n",
              "1  [-0.006130422931164503, -0.006197203416377306,...  \n",
              "2  [-0.0007784886402077973, 0.0017219326691702008...  \n",
              "3  [-0.013850468210875988, 0.01317818183451891, 0...  \n",
              "4  [-0.001279508345760405, -0.0063638705760240555...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54b3cf1e-929a-4313-b754-a36dc92dbb19\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Name: 0\\n\\n/content/resume_book_2023_courant.pdf</td>\n",
              "      <td>[0.00010535831097513437, -0.012883378192782402...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Name: 1\\n\\nRESUME BOOK\\nCLASS OF 2023</td>\n",
              "      <td>[-0.006130422931164503, -0.006197203416377306,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Name: 2\\n\\nRUIZE CHEN  \\n(585) 540-6418 // rui...</td>\n",
              "      <td>[-0.0007784886402077973, 0.0017219326691702008...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Name: 2\\n\\nExpected 12/23 NEW YORK UNIVERSITY ...</td>\n",
              "      <td>[-0.013850468210875988, 0.01317818183451891, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Name: 2\\n\\nM.S. in Mathematics in Finance \\n● ...</td>\n",
              "      <td>[-0.001279508345760405, -0.0063638705760240555...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54b3cf1e-929a-4313-b754-a36dc92dbb19')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-54b3cf1e-929a-4313-b754-a36dc92dbb19 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-54b3cf1e-929a-4313-b754-a36dc92dbb19');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lgAJB8t3_hgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# search documents using query and text embeddings and and retrieve relevant consultant name from resume information using GPT"
      ],
      "metadata": {
        "id": "BMXkZkHWJE_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Search (once per query) - Given a user question, generate an embedding for the query from the OpenAI API\n",
        "1. Using the embeddings, rank the text sections by relevance to the query\n",
        "1. Ask (once per query)\n",
        "  1. Insert the question and the most relevant sections into a message to GPT\n",
        "  1. Return GPT's answer"
      ],
      "metadata": {
        "id": "ZF5Y0pILMG_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\""
      ],
      "metadata": {
        "id": "Qwu5742ONHQ5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search function\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 3\n",
        ") -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = openai.Embedding.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=query,\n",
        "    )\n",
        "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ],
      "metadata": {
        "id": "DCLYKb9iUpab"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_strings_ranked_by_relatedness(query, df, top_n=10):\n",
        "  strings, relatednesses = strings_ranked_by_relatedness(query, df, top_n)\n",
        "  for string, relatedness in zip(strings, relatednesses):\n",
        "      print(f\"{relatedness=:.3f}\")\n",
        "      display(string)"
      ],
      "metadata": {
        "id": "TIhPySTfWR0v"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"strong in math\"\n",
        "strings_ranked_by_relatedness(query, df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMpxgKyBJOHN",
        "outputId": "fa6ade2c-bcdb-4245-8a45-d3ee5a496ba7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Name: 18\\n\\nin Mathematics and Applied Mathematics ',\n",
              "  'Name: 16\\n\\nin Mathematics in Finance ',\n",
              "  'Name: 23\\n\\nMath Department Grader \\n●\\nGraded homework for more than 300 students in upper-division courses including real analysis, \\nlinear algebra, abstract algebra, and probability \\n●'),\n",
              " (0.8197180259857161, 0.8182264010521288, 0.8150523976996884))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"understands pytorch\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "id": "gCWRzTkWJN_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5078e0b2-9656-413f-80a9-97aa69986ff0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Name: 4\\n\\nSklearn, Tensorflow), R, Java, C++\\nLanguages:\\nEnglish (fluent), Mandarin (native)\\nInterests:',\n",
              "  'Name: 6\\n\\n●\\nUsed natural language understanding; designed model that learned image generation from \\ntext data with 1M-word vocabulary, producing high-level generic sentence representations \\n●\\nImproved model by employing distributed text encoder conditioned with generative ',\n",
              "  'Name: 6\\n\\n08/16 - 02/17\\nINDIAN INSTITUTE OF TECHNOLOGY ROORKEE\\nRoorkee, India \\nText-Image Synthesis with Uni-Skip Vectors (Python, Deep Learning) '),\n",
              " (0.7835732057219097, 0.7830662985290144, 0.7830618617194605))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" understands law\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "id": "Cz-KCpslJN0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e164cc05-61f2-40de-dd40-d7e1411fc321"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Name: 81\\n\\nCompeted as a pre-trial attorney in a mock criminal law case \\n●\\nAnalyzed legal documents pertaining to the case and argued for the permittance of evidence ',\n",
              "  'Name: 161\\n\\nCompeted as a pre-trial attorney in a mock criminal law case \\n●\\nAnalyzed legal documents pertaining to the case and argued for the permittance of evidence ',\n",
              "  'Name: 110\\n\\n● Attended court hearings involving, commercial law, traffic violation, family law, and data breach cases  '),\n",
              " (0.8110228926750287, 0.8092794685151007, 0.8082582811210965))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" azure databricks\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN6Gyb11m65U",
        "outputId": "cacaaa0a-4722-45ce-a858-4092203ecafd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Name: 13\\n\\nwith Azure HDInsight; prepared data visualization for industry report\\n●',\n",
              "  'Name: 122\\n\\nTools: Jupyter Notebook, Tableau, Lucid Chart, Anaconda Navigator and Android Studio. Big Data and Cloud: Heroku, AWS',\n",
              "  'Name: 42\\n\\nTools: Jupyter Notebook, Tableau, Lucid Chart, Anaconda Navigator and Android Studio. Big Data and Cloud: Heroku, AWS'),\n",
              " (0.757380922555399, 0.7565015675875518, 0.7511625982697887))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Ask\n",
        "\n",
        "With our database of resumes turned into vector embeddings, we can use the vector search function above to automatically retrieve relevant knowledge from the resumes and feed the query plus our knowledge base into GPT.\n",
        "\n",
        "Below, we define a function `ask` that:\n",
        "- Takes a user query\n",
        "- Searches for text relevant to the query\n",
        "- Stuffs that text into a message for GPT\n",
        "- Sends the message to GPT\n",
        "- Returns GPT's answer"
      ],
      "metadata": {
        "id": "MvV6e7bZno4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = 'gpt-3.5-turbo'\n",
        "def num_tokens(text: str, model: str = 'gpt-3.5-turbo') -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "#using v1 search function\n",
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str,\n",
        "    token_budget: int\n",
        ") -> str:\n",
        "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
        "    logger.info(f\"Strings Found From Search\\n:{strings}\\n Relatednesses:{relatednesses}\\n\")\n",
        "    introduction = ' You are a Human Resources agent looking for skills in resumes'\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "    message = introduction\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nresume section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (\n",
        "            num_tokens(message + next_article + question, model=model)\n",
        "            > token_budget\n",
        "        ):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "@logger.catch\n",
        "def ask(\n",
        "    query: str,\n",
        "    df: pd.DataFrame = df,\n",
        "    model: str = GPT_MODEL,\n",
        "    token_budget: int = 4096 - 500,\n",
        "    print_message: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    logger.info(f\"{message}\")\n",
        "    content = \"Construct a list of Name fields from the documents given. Remove all duplicates from the list\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": content},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return response_message\n",
        "\n"
      ],
      "metadata": {
        "id": "HwIQfHT5ncvI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask('who knows law')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "H4_3qNXCpLhV",
        "outputId": "f6d153de-e790-4660-f1d3-0b5ecf4f9bbd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-05-12 17:31:48.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mquery_message\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mStrings Found From Search\n",
            ":('Name: 81\\n\\nCompeted as a pre-trial attorney in a mock criminal law case \\n●\\nAnalyzed legal documents pertaining to the case and argued for the permittance of evidence ', 'Name: 110\\n\\n● Attended court hearings involving, commercial law, traffic violation, family law, and data breach cases  ', 'Name: 161\\n\\nCompeted as a pre-trial attorney in a mock criminal law case \\n●\\nAnalyzed legal documents pertaining to the case and argued for the permittance of evidence ')\n",
            " Relatednesses:(0.7960334080871797, 0.7936805124322088, 0.7932960803294815)\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-12 17:31:48.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m You are a Human Resources agent looking for skills in resumes\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 81\n",
            "\n",
            "Competed as a pre-trial attorney in a mock criminal law case \n",
            "●\n",
            "Analyzed legal documents pertaining to the case and argued for the permittance of evidence \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 110\n",
            "\n",
            "● Attended court hearings involving, commercial law, traffic violation, family law, and data breach cases  \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 161\n",
            "\n",
            "Competed as a pre-trial attorney in a mock criminal law case \n",
            "●\n",
            "Analyzed legal documents pertaining to the case and argued for the permittance of evidence \n",
            "\"\"\"\n",
            "\n",
            "Question: who knows law\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Possible answer:\\n\\n- 81\\n- 161'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask('who majored in physics')"
      ],
      "metadata": {
        "id": "0Gvwt0TZpS-R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "b0be056b-ea6b-42bf-b518-c9b185a1890e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-05-12 17:32:10.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mquery_message\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mStrings Found From Search\n",
            ":('Name: 76\\n\\nBachelor of Science in Engineering Physics with Concentration in Mechanical Engineering\\nRelevant Coursework: Solidworks, AutoCAD, Machine Dynamics & Design, Mechanics of Materials, ', 'Name: 156\\n\\nBachelor of Science in Engineering Physics with Concentration in Mechanical Engineering\\nRelevant Coursework: Solidworks, AutoCAD, Machine Dynamics & Design, Mechanics of Materials, ', 'Name: 5\\n\\nB.S. in Physics and B.S. in Financial Math & Statistics \\n●\\nCoursework:\\nmultivariable calculus,\\nprobability and\\nstatistics,')\n",
            " Relatednesses:(0.8106379068250826, 0.8095400054922902, 0.8052200089651524)\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-12 17:32:10.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m You are a Human Resources agent looking for skills in resumes\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 76\n",
            "\n",
            "Bachelor of Science in Engineering Physics with Concentration in Mechanical Engineering\n",
            "Relevant Coursework: Solidworks, AutoCAD, Machine Dynamics & Design, Mechanics of Materials, \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 156\n",
            "\n",
            "Bachelor of Science in Engineering Physics with Concentration in Mechanical Engineering\n",
            "Relevant Coursework: Solidworks, AutoCAD, Machine Dynamics & Design, Mechanics of Materials, \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 5\n",
            "\n",
            "B.S. in Physics and B.S. in Financial Math & Statistics \n",
            "●\n",
            "Coursework:\n",
            "multivariable calculus,\n",
            "probability and\n",
            "statistics,\n",
            "\"\"\"\n",
            "\n",
            "Question: who majored in physics\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The following individuals majored in Physics:\\n- Resume section 3 (Name: 5)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = 'who is a consultant'\n",
        "ask(QUERY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "PyVhRVwlX0Qk",
        "outputId": "e531832e-7bd6-4ce7-a992-62ca0851a7ee"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-05-12 17:33:09.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mquery_message\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mStrings Found From Search\n",
            ":('Name: 35\\n\\nPublic Consulting Group , Contact Tracer for New York State , Long Island Region , NY 12/2020 - 03/2022  ', 'Name: 115\\n\\nPublic Consulting Group , Contact Tracer for New York State , Long Island Region , NY 12/2020 - 03/2022  ', 'Name: 85\\n\\nAcquired industry-based knowledge from workshops regarding tax, consulting, audit, and advisory services and gained a \\ndeeper perspective on the consulting services industry\\nWE ARE ONE Campaign\\nSaco, ME ')\n",
            " Relatednesses:(0.7516799652320376, 0.7498786336834139, 0.7467068908859048)\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-12 17:33:09.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m You are a Human Resources agent looking for skills in resumes\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 35\n",
            "\n",
            "Public Consulting Group , Contact Tracer for New York State , Long Island Region , NY 12/2020 - 03/2022  \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 115\n",
            "\n",
            "Public Consulting Group , Contact Tracer for New York State , Long Island Region , NY 12/2020 - 03/2022  \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 85\n",
            "\n",
            "Acquired industry-based knowledge from workshops regarding tax, consulting, audit, and advisory services and gained a \n",
            "deeper perspective on the consulting services industry\n",
            "WE ARE ONE Campaign\n",
            "Saco, ME \n",
            "\"\"\"\n",
            "\n",
            "Question: who is a consultant\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It is not possible to determine who is a consultant based on the given resume sections as there is no clear indication of a specific person being referred to as a consultant.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = 'who knows SAP'\n",
        "ask(QUERY)"
      ],
      "metadata": {
        "id": "PiKK-wOraNca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "2a0eb28e-0737-40d7-b03d-7166b029fbfa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-05-12 17:34:19.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mquery_message\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mStrings Found From Search\n",
            ":('Name: 3\\n\\nin largest global SAP S/4HANA ERP project at EY Singapore in 2019 for client, DyStar Group \\n●\\nConducted international localization workshops for franchises in 8 countries; communicated ', 'Name: 29\\n\\n12/20 - 02/21\\nACCENTURE\\nBeijing, China \\nTechnology Consulting Assistant (SAP) \\n●', 'Name: 29\\n\\nCollaborated with business planning and consolidation consultant to construct expense budget \\ntable in SAP; created 23 logical carding diagrams of cost allocation configuration rules ')\n",
            " Relatednesses:(0.7974384286051879, 0.7935589698321063, 0.7872619620330196)\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-12 17:34:19.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m You are a Human Resources agent looking for skills in resumes\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 3\n",
            "\n",
            "in largest global SAP S/4HANA ERP project at EY Singapore in 2019 for client, DyStar Group \n",
            "●\n",
            "Conducted international localization workshops for franchises in 8 countries; communicated \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 29\n",
            "\n",
            "12/20 - 02/21\n",
            "ACCENTURE\n",
            "Beijing, China \n",
            "Technology Consulting Assistant (SAP) \n",
            "●\n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 29\n",
            "\n",
            "Collaborated with business planning and consolidation consultant to construct expense budget \n",
            "table in SAP; created 23 logical carding diagrams of cost allocation configuration rules \n",
            "\"\"\"\n",
            "\n",
            "Question: who knows SAP\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The first and third resumes mention SAP, indicating that those candidates have experience with SAP.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0uiiEhDlBw5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}