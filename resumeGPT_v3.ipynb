{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTJCV4EfF4rVU/J5rK2O+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanreed1111/colab-demos/blob/master/resumeGPT_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "based on https://github.com/openai/openai-cookbook/tree/main/examples"
      ],
      "metadata": {
        "id": "ZQxWNb57WrWz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCG2144SWqEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO  \n",
        "- Return a link to the full original resume instead of to chunks\n",
        "- Run experiments to find optimal chunking size of resume text\n",
        "- Figure out how to use Ada and Babbage to reduce costs\n",
        "- port to Azure Platform\n",
        "\n"
      ],
      "metadata": {
        "id": "58pU0G9FGWyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq loguru textract tiktoken openai azure-ai-ml mlflow azureml-sdk azureml-mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poRnH-oftTPb",
        "outputId": "bc2aca71-1804-4feb-938e-14bb7ea5bb3d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m814.0/814.0 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.9/173.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.2/136.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.5/135.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.7/245.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.7/245.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.4/779.4 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.9/965.9 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.4/141.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.4/245.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.1/31.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.7/191.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fusepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.18 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# before running this notebook, UPLOAD these files\n",
        "- openai.env\n",
        "- azure.env"
      ],
      "metadata": {
        "id": "gqPb-Nn776tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os,argparse,loguru, json, time, datetime, openai\n",
        "from pathlib import Path\n",
        "from loguru import logger"
      ],
      "metadata": {
        "id": "scY-DzUJquiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_load_aml_env_vars(env_path=None):\n",
        "  import os, json\n",
        "  try:\n",
        "    with open(env_path, \"r\") as f:\n",
        "      env_vars = json.load(f)\n",
        "    os.environ[\"resource_group\"] = env_vars[\"resource_group\"]\n",
        "    os.environ[\"workspace_name\"] = env_vars[\"workspace_name\"]\n",
        "    os.environ[\"subscription_id\"] = env_vars[\"subscription_id\"]\n",
        "    if (os.getenv(\"resource_group\") and os.getenv(\"workspace_name\")\n",
        "    and os.getenv(\"subscription_id\")):\n",
        "      return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "    return False"
      ],
      "metadata": {
        "id": "A1d_9gbMZ78f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_open_ai_key(env_path=None):\n",
        "  import json, os\n",
        "  from pathlib import Path\n",
        "  try:\n",
        "    with open(env_path, \"r\") as f:\n",
        "        env_vars = json.load(f)\n",
        "    os.environ[\"OPENAI_API_KEY\"] = env_vars[\"OPENAI_API_KEY\"]\n",
        "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "    openai.Model.list() #test a random command on the openai API\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "  return False\n",
        "\n",
        "def test_set_open_ai_key(key_path=None):\n",
        "  openai.api_key = None #disconnect from api key if already registered\n",
        "  try:\n",
        "    set_open_ai_key(key_path)\n",
        "    openai.Model.list()\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "wwZXC6jUTLvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_get_ml_client(env_path=None):\n",
        "  # this is a mix of sdk v1 and v2. Try to consolidate \n",
        "  import json, os, mlflow\n",
        "  from pathlib import Path\n",
        "  from azureml.core import Workspace \n",
        "  from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "  from azure.ai.ml import MLClient\n",
        "\n",
        "  if not env_path: return None\n",
        "\n",
        "  ws = Workspace.from_config(env_path)\n",
        "  tracking_uri = ws.get_mlflow_tracking_uri()\n",
        "  mlflow.set_tracking_uri(tracking_uri)\n",
        "\n",
        "  try:\n",
        "      credential = DefaultAzureCredential()\n",
        "  except Exception as ex:\n",
        "      # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not working\n",
        "      credential = InteractiveBrowserCredential()\n",
        "\n",
        "  is_loaded = maybe_load_aml_env_vars(env_path)\n",
        "  if is_loaded:\n",
        "    try:\n",
        "      ml_client = MLClient(\n",
        "          subscription_id=os.getenv(\"subscription_id\"),\n",
        "          resource_group_name=os.getenv(\"resource_group\"),\n",
        "          workspace_name=os.getenv(\"workspace_name\"),\n",
        "          credential=credential,\n",
        "      )\n",
        "      return ml_client\n",
        "    except Exception as e:\n",
        "      logger.error(f\"{e}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "zNLN8FKHzlAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_setup_azure_env(azure_env_path=None):\n",
        "  # setup azure ml env if azure credentials are available.\n",
        "  if azure_env_path and azure_env_path.is_file() and maybe_load_aml_env_vars(azure_env_path):\n",
        "    ml_client = maybe_get_ml_client(azure_env_path)\n",
        "    if ml_client:\n",
        "      #do a random test to check that ml_client and mlflow are playing nicely together\n",
        "      import mlflow\n",
        "      experiment_name = 'mlflow-2'\n",
        "      mlflow.set_experiment(experiment_name)\n",
        "      from random import random\n",
        "\n",
        "      with mlflow.start_run() as mlflow_test_run:\n",
        "          mlflow.log_param(\"hello_param\", \"world\")\n",
        "          mlflow.log_metric(\"hello_metric2\", random())\n",
        "          os.system(f\"echo 'hello world2' > helloworld2.txt\")\n",
        "          mlflow.log_artifact(\"helloworld2.txt\")\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "pzzqgJ9k9f34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "azure_env_path, openai_env_path, ml_client, openai.api_key = None, None, None , None\n",
        "cwd = Path.cwd()\n",
        "# resume_path = cwd / \"Resumes\"\n",
        "# resume_path.mkdir(exist_ok=True)\n",
        "\n",
        "azure_env_path = cwd / \"azure.env\" ##uncomment if providing azure env\n",
        "openai_env_path = cwd/ \"openai.env\"\n",
        "maybe_setup_azure_env(azure_env_path)\n",
        "set_open_ai_key(openai_env_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqHlEmz8CsZ",
        "outputId": "3472bd7b-9a7a-4178-e4bf-481f94dc7fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPLIT SECTIONS\n",
        "source: Embedding_Wikipedia_articles_for_search.ipynb \n",
        "https://colab.research.google.com/drive/1EJMtCmF8jZc2Y-c1RaBxFSCTPcjzjJf4#scrollTo=TOVSYkDur9zA"
      ],
      "metadata": {
        "id": "5kUr4TQLdnRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll recursively split long sections into smaller sections.\n",
        "\n",
        "There's no perfect recipe for splitting text into sections.\n",
        "\n",
        "Some tradeoffs include:\n",
        "- Longer sections may be better for questions that require more context\n",
        "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
        "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
        "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
        "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
        "\n",
        "Here, we'll use a simple approach and limit sections to 1,000 tokens each by default, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
      ],
      "metadata": {
        "id": "NVpjUKLwenCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### extract text from pdf"
      ],
      "metadata": {
        "id": "pK5mrZ8PJ4L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract, os, openai, tiktoken"
      ],
      "metadata": {
        "id": "fJ-sflJ77VjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #TODO walk the directory to get all the filenames, use as names of the people in tagging and document retrieval\n",
        "# #TODO use regex to get rid of excess spaces and new lines. only one new line needed per line.\n",
        "# file_names = [\"Jesse_Jayant.pdf\", \"Nadia_Smythe.pdf\"]\n",
        "# file_paths = [(resume_path / file) for file in file_names];print(file_paths)\n",
        "# names = [path.stem.lower() for path in file_paths];\n",
        "\n",
        "# # Extract the raw text from each PDF using textract\n",
        "\n",
        "# texts =[textract.process((file_path), method='pdfminer').decode('utf-8') for file_path in file_paths]\n",
        "\n",
        "# #TODO Do more cleaning with regex\n",
        "# texts = [text.strip().replace(\"  \", \" \") for text in texts]\n",
        "# #create tuple[list[str],str]\n",
        "# clean_texts = [(['NameOnResume: '+ item1], item2) for item1,item2 in zip(names,texts)] "
      ],
      "metadata": {
        "id": "d5EkHdPqircB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = 'gpt-3.5-turbo'  # only matters insofar as it selects which tokenizer to use\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "## TODO This needs more sophistication in the use of a delimiter\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # no delimiter found\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # no need to search for halfway point\n",
        "    else:\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        best_diff = halfway\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "def truncated_string(\n",
        "    string: str,\n",
        "    model: str,\n",
        "    max_tokens: int,\n",
        "    print_warning: bool = True,\n",
        "    TRUNCATION_WARNING_PERCENTAGE: float = 0.25\n",
        "\n",
        ") -> str:\n",
        "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    truncation_percentage = 1.0 - max_tokens*1.0 / len(encoded_string)\n",
        "    if print_warning and (len(encoded_string) > max_tokens) and (truncation_percentage > TRUNCATION_WARNING_PERCENTAGE):\n",
        "        logger.warning(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens. \\nOriginalString:{string} \\n\")\n",
        "\n",
        "    return truncated_string"
      ],
      "metadata": {
        "id": "u3CZ5MwnnspR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str], #legacy structure. \n",
        "    max_tokens: int = 1000,\n",
        "    model: str = GPT_MODEL,\n",
        "    max_recursion: int = 8,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
        "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # if length is fine, return string\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # if recursion hasn't found a split after X iterations, just truncate\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # otherwise, split in half and recurse\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \", \"●\", \"•\", \":\", \";\"]:\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # if either half is empty, retry with a more fine-grained delimiter\n",
        "                continue\n",
        "            else:\n",
        "                # recurse on each half\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1,\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # otherwise no split was found, so just truncate (should be very rare)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        " "
      ],
      "metadata": {
        "id": "9JmwS-cZdlvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load resumes from csv file made by pdf parser and then split and create embeddings.\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/resume_books.csv\",usecols=[\"text\", \"source\"]);df.head()\n",
        "df = df.dropna()\n",
        "df.loc[:,\"name\"] = [f\"Name: {i}\" for i in df.index]\n",
        "df.loc[:,\"text2\"] = list(zip(df[\"name\"].to_list(), df[\"text\"].to_list()))\n",
        "df = df[[\"name\",\"text2\"]];df.head()\n",
        "df.to_csv(\"resume_books_v2.csv\")\n",
        "clean_texts = df[\"text2\"].to_list()\n",
        "clean_texts = [([x1],x2) for x1, x2 in clean_texts]"
      ],
      "metadata": {
        "id": "VAtkxHnlD6YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split resumes into chunks. Small chunks probably better when searching for skills? \n",
        "# maybe even shrink to individual sentences\n",
        "MAX_TOKENS = 100\n",
        "resume_strings = []\n",
        "for section in clean_texts:\n",
        "    resume_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(clean_texts)} resumes split into {len(resume_strings)} strings.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOlixxaYpCRC",
        "outputId": "a49e7f45-fc1b-402f-cd7f-63f6d70e79e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-05-10 13:48:40.145\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtruncated_string\u001b[0m:\u001b[36m47\u001b[0m - \u001b[33m\u001b[1mWarning: Truncated string from 182 tokens to 100 tokens. \n",
            "OriginalString:Name: 90\n",
            "\n",
            " The Ground Floor, Basic Microeconomics, Statistics, Principles of Management, Marketing Principles,  Consulting Cup, Information Systems, Principles of Financial Accounting, Business Communications  EXPERIENCE Maimonides Medical Center Department of Surgery                                                                                              Brooklyn, NY Admin Intern                                                                                                                                                      July 2022 – August 2022 ! Oversaw 35 surgical residents and formulated day assignment sheet and monthly attestation from an Excel block schedule  ! Evaluated 35 surgical residents’ files weekly on New Innovations for missing certifications and recertifications  ! Updated surgical residents’ and faculty’s PubMed academic activity  ! Emailed 10-15 medical students their acceptance email to elective surgeries weekly and confirmed their elective surgeries   Gabelli Consulting Cup                                                                                                                                            New York, NY Team Member                                                                                                                                                        September 2022 –present ! Member of a six-person team who is performing a study of Wendy’s Co \n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-10 13:48:40.155\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtruncated_string\u001b[0m:\u001b[36m47\u001b[0m - \u001b[33m\u001b[1mWarning: Truncated string from 137 tokens to 100 tokens. \n",
            "OriginalString:Name: 90\n",
            "\n",
            "Barnabas Hospital Gabelli Marketing Project                                                         Bronx, NY  Project Manager                                                                                                                                                    September 2022 – present ! Visited SBH Health & Wellness Center to find strategies to promote the wellness center to the local Bronx community  ! Debrief weekly with the head supervisor on project status and delegated project tasks based on team members’ competencies  ! Researched and developed a full communications program, including B.R.A.G Boxing Tournament and Holiday Cooking       Competition, to highlight the Fitness Center’s and Teaching Kitchen’s impact on a healthy lifestyle                   Fordham Commuting Students Association                                                                                                                 Bronx, NY Sophomore Program Coordinator                                                                                                                                 September 2022-present  \n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-10 13:48:41.928\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtruncated_string\u001b[0m:\u001b[36m47\u001b[0m - \u001b[33m\u001b[1mWarning: Truncated string from 182 tokens to 100 tokens. \n",
            "OriginalString:Name: 170\n",
            "\n",
            " The Ground Floor, Basic Microeconomics, Statistics, Principles of Management, Marketing Principles,  Consulting Cup, Information Systems, Principles of Financial Accounting, Business Communications  EXPERIENCE Maimonides Medical Center Department of Surgery                                                                                              Brooklyn, NY Admin Intern                                                                                                                                                      July 2022 – August 2022 ! Oversaw 35 surgical residents and formulated day assignment sheet and monthly attestation from an Excel block schedule  ! Evaluated 35 surgical residents’ files weekly on New Innovations for missing certifications and recertifications  ! Updated surgical residents’ and faculty’s PubMed academic activity  ! Emailed 10-15 medical students their acceptance email to elective surgeries weekly and confirmed their elective surgeries   Gabelli Consulting Cup                                                                                                                                            New York, NY Team Member                                                                                                                                                        September 2022 –present ! Member of a six-person team who is performing a study of Wendy’s Co \n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-10 13:48:41.943\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtruncated_string\u001b[0m:\u001b[36m47\u001b[0m - \u001b[33m\u001b[1mWarning: Truncated string from 137 tokens to 100 tokens. \n",
            "OriginalString:Name: 170\n",
            "\n",
            "Barnabas Hospital Gabelli Marketing Project                                                         Bronx, NY  Project Manager                                                                                                                                                    September 2022 – present ! Visited SBH Health & Wellness Center to find strategies to promote the wellness center to the local Bronx community  ! Debrief weekly with the head supervisor on project status and delegated project tasks based on team members’ competencies  ! Researched and developed a full communications program, including B.R.A.G Boxing Tournament and Holiday Cooking       Competition, to highlight the Fitness Center’s and Teaching Kitchen’s impact on a healthy lifestyle                   Fordham Commuting Students Association                                                                                                                 Bronx, NY Sophomore Program Coordinator                                                                                                                                 September 2022-present  \n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "191 resumes split into 2127 strings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculate embeddings and store in dataframe"
      ],
      "metadata": {
        "id": "Izi2RX_O1u0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
        "MAX_BATCH_SIZE = 1000 # you can submit up to 2048 embedding inputs per request\n",
        "NUMBER_OF_STRINGS_TO_EMBED = len(resume_strings)\n",
        "\n",
        "if NUMBER_OF_STRINGS_TO_EMBED < MAX_BATCH_SIZE:\n",
        "  BATCH_SIZE = NUMBER_OF_STRINGS_TO_EMBED\n",
        "else: \n",
        "  BATCH_SIZE = MAX_BATCH_SIZE \n",
        "\n",
        "embeddings = []\n",
        "for batch_start in range(0, NUMBER_OF_STRINGS_TO_EMBED, BATCH_SIZE):\n",
        "    batch_end = batch_start + BATCH_SIZE\n",
        "    batch = resume_strings[batch_start:batch_end]\n",
        "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
        "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
        "    for i, be in enumerate(response[\"data\"]):\n",
        "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
        "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
        "    embeddings.extend(batch_embeddings)\n",
        "\n",
        "df = pd.DataFrame({\"text\": resume_strings, \"embedding\": embeddings})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hciff-Ul1t1B",
        "outputId": "3fdaa85d-ac68-4443-f8ef-43adcc33f7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 to 999\n",
            "Batch 1000 to 1999\n",
            "Batch 2000 to 2999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store embeddings\n",
        "df.to_csv(\"embeddings_v2.csv\", index=False)"
      ],
      "metadata": {
        "id": "4WOqGbzmViZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# search documents using query and text embeddings and and retrieve relevant consultant name from resume information using GPT"
      ],
      "metadata": {
        "id": "BMXkZkHWJE_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Search (once per query) - Given a user question, generate an embedding for the query from the OpenAI API\n",
        "1. Using the embeddings, rank the text sections by relevance to the query\n",
        "1. Ask (once per query)\n",
        "  1. Insert the question and the most relevant sections into a message to GPT\n",
        "  1. Return GPT's answer"
      ],
      "metadata": {
        "id": "ZF5Y0pILMG_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial"
      ],
      "metadata": {
        "id": "Qwu5742ONHQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL = \"text-embedding-ada-002\""
      ],
      "metadata": {
        "id": "q7z5Ji5uJOUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search function\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 3\n",
        ") -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = openai.Embedding.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=query,\n",
        "    )\n",
        "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ],
      "metadata": {
        "id": "DCLYKb9iUpab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_strings_ranked_by_relatedness(query, df, top_n=10):\n",
        "  strings, relatednesses = strings_ranked_by_relatedness(query, df, top_n)\n",
        "  for string, relatedness in zip(strings, relatednesses):\n",
        "      print(f\"{relatedness=:.3f}\")\n",
        "      display(string)"
      ],
      "metadata": {
        "id": "TIhPySTfWR0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"strong in math\"\n",
        "strings_ranked_by_relatedness(query, df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMpxgKyBJOHN",
        "outputId": "d371ea4e-c74d-464a-882b-71907bb0a183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((\"Name: 8\\n\\nlinear algebra, partial differential equations,\\nItô's lemma, real analysis, Bayesian\\nstatistics, CAPM, WACC, options, data structure (Python), stochastic processes, linear regression \\n●\\nHonors:\\nDean’s Honors (top 5% of GPA in department),\\nPresident’s Scholarship\",\n",
              "  'Name: 2\\n\\n● Coursework:  linear algebra, ordinary differential equations, real analysis, stochastic processes, \\nprobability theory, linear regression, mean-variance optimization, corporate finance \\n● Honors/Awards:  Dean’s List (3 years), Cum Laude, Beta Gamma Sigma Honor Society ',\n",
              "  'Name: 14\\n\\nCoursework:\\nlinear algebra, mathematical statistics,\\nBrownian motion, law of large numbers, \\nmachine learning, data structures, algorithms, databases \\n●\\nHonors/Awards:\\nDean’s list for 4 years, Latin Honors\\nCum Laude'),\n",
              " (0.8062711795862255, 0.7996338658385634, 0.7995175364806416))"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"understands pytorch\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "id": "gCWRzTkWJN_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d2aa42-4631-46c9-a55a-a7d5db437c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Name: 6\\n\\n08/16 - 02/17\\nINDIAN INSTITUTE OF TECHNOLOGY ROORKEE\\nRoorkee, India \\nText-Image Synthesis with Uni-Skip Vectors (Python, Deep Learning) \\n●\\nUsed natural language understanding; designed model that learned image generation from \\ntext data with 1M-word vocabulary, producing high-level generic sentence representations \\n●\\nImproved model by employing distributed text encoder conditioned with generative ',\n",
              "  'Name: 21\\n\\nProgramming Languages:\\nPython (NumPy, Pandas, Sklearn,\\nSciPy), SQL, Java, R, MATLAB\\nLanguages:\\nEnglish (fluent); Mandarin (native)',\n",
              "  'Name: 7\\n\\nC O M P U T A T I O N A L  S K I L L S  /  O T H E R\\nProgramming Languages:\\nPython (Numpy, Pandas, Scikit-learn,\\nMatplotlib), Java, SQL, R\\nLanguages:\\nEnglish (fluent), Mandarin (native)\\nPublication:\\nOption Mispricing & Arbitrage Opportunity\\n,\\nICSET 2021 Taiwan\\nActivities:\\nDiscrete Structure Teaching Assistant\\nat Northeastern University'),\n",
              " (0.7885447575940931, 0.7751890320298489, 0.7745399880406452))"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" understands law\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "id": "Cz-KCpslJN0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce05421-b949-4d32-e96f-934c5f903754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Name: 110\\n\\nIntroduction to the American Legal System, Constitutional Law, American Constitution, Eur opean Politics  \\n \\nRELEVANT EXPERIENCE Whitestone Chambers, Legal Intern London, UK             January -April 2022  ',\n",
              "  'Name: 190\\n\\nIntroduction to the American Legal System, Constitutional Law, American Constitution, Eur opean Politics  \\n \\nRELEVANT EXPERIENCE Whitestone Chambers, Legal Intern London, UK             January -April 2022  ',\n",
              "  'Name: 110\\n\\n● Conducted in depth legal research and prepared arguments for active data breach cases  \\n● Worked directly with head barrister to prepare for various stages of commercial and civil litigation  \\n● Attended court hearings involving, commercial law, traffic violation, family law, and data breach cases  '),\n",
              " (0.8022047376928069, 0.8016164332607587, 0.7965876491636508))"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" azure databricks\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN6Gyb11m65U",
        "outputId": "8b314c90-6933-4bcd-a1c1-593ba187276b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Name: 13\\n\\nwith Azure HDInsight; prepared data visualization for industry report\\n●\\nBuilt large-scale database  from daily news and data for 3,000 clean energy automobile stocks\\nfrom 2018 to 2019, using R and SQL\\n●\\nUsed  feature extraction on news about 1,000 selected stocks in 2019; improved stock prediction\\nbased on sentiment analysis with RNN; average accuracy increased by 7%\\nP R O J E C T',\n",
              "  'Name: 122\\n\\nTools: Jupyter Notebook, Tableau, Lucid Chart, Anaconda Navigator and Android Studio. Big Data and Cloud: Heroku, AWS.  WORK EXPERIENCE Research Assistant in Quantum Computing at Fordham University                   (August 2022 – Present) Tools: Python, Qiskit',\n",
              "  'Name: 42\\n\\nTools: Jupyter Notebook, Tableau, Lucid Chart, Anaconda Navigator and Android Studio. Big Data and Cloud: Heroku, AWS.  WORK EXPERIENCE Research Assistant in Quantum Computing at Fordham University                   (August 2022 – Present) Tools: Python, Qiskit'),\n",
              " (0.7623053836758877, 0.7443740487810785, 0.7410629886184242))"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" azure AND databricks\"\n",
        "strings_ranked_by_relatedness(query, df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k17cktBSnNuz",
        "outputId": "b53228c7-f4d4-4796-a196-65836f5d9c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('NameOnResume: seanreed\\n\\n2009 \\n\\n1990 \\n\\nDatabricks, Azure ML, Python, Pandas, Spark, PyTorch, TensorFlow, Keras, Git, Computer Vision, \\nNatural Language Processing, Medical Image Segmentation, Deep Learning, Machine Learning, \\nGLMs, SQL, Linux, Bayesian Statistics, Data Pipelines, GCP, AWS, Docker, Kubernetes, Pandas, \\nNumPy, Random Forests, Gradient Boosting, SVMs, GLMs, Recommender Systems, Graph \\nDatabases, Neo4j',\n",
              "  'NameOnResume: seanreed\\n\\n● Built distributed Swin-UNETR encoder-decoder model using Azure ML, PyTorch, Docker, and \\nLightning AI that can pretrain on unlabeled 3D CT scans, eliminating substantial future labeling \\ntime and expense incurred by company radiologists. \\nImplemented computer vision model from innovative academic research paper and completely \\nrefactored model’s existing Python code to solve client’s business problem. \\n\\n● ',\n",
              "  'NameOnResume: seanreed\\n\\nPython model to production environment and usage with Ray, Python, and PySpark on Azure \\nDatabricks. \\n\\n \\nGalvanize, New York, NY \\nSenior Data Scientist \\n● Served as Customer Data Scientist, tasked to identify new markets and clients most likely to \\n\\n2018 - 2021 \\n\\nqualify and benefit from company’s educational program. '),\n",
              " (0.7617906803714984, 0.7449896278855399, 0.7420735098729273))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Ask\n",
        "\n",
        "With the search function above, we can now automatically retrieve relevant knowledge and insert it into messages to GPT.\n",
        "\n",
        "Below, we define a function `ask` that:\n",
        "- Takes a user query\n",
        "- Searches for text relevant to the query\n",
        "- Stuffs that text into a message for GPT\n",
        "- Sends the message to GPT\n",
        "- Returns GPT's answer"
      ],
      "metadata": {
        "id": "MvV6e7bZno4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = 'gpt-3.5-turbo'\n",
        "def num_tokens(text: str, model: str = 'gpt-3.5-turbo') -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "#using v1 search function\n",
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str,\n",
        "    token_budget: int\n",
        ") -> str:\n",
        "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
        "    logger.info(f\"Strings Found From Search\\n:{strings}\\n Relatednesses:{relatednesses}\\n\")\n",
        "    introduction = ' You are a Human Resources agent looking for skills in resumes'\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "    message = introduction\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nresume section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (\n",
        "            num_tokens(message + next_article + question, model=model)\n",
        "            > token_budget\n",
        "        ):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "@logger.catch\n",
        "def ask(\n",
        "    query: str,\n",
        "    df: pd.DataFrame = df,\n",
        "    model: str = GPT_MODEL,\n",
        "    token_budget: int = 4096 - 500,\n",
        "    print_message: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    logger.info(f\"{message}\")\n",
        "    content = \"Construct a list of Name fields from the documents given. Remove all duplicates from the list\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": content},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return response_message\n",
        "\n"
      ],
      "metadata": {
        "id": "HwIQfHT5ncvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask('who knows law')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "H4_3qNXCpLhV",
        "outputId": "591a7be5-3303-4584-f991-7374d1000d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-05-09 16:35:14.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mquery_message\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mStrings Found From Search\n",
            ":('Name: 110\\n\\nIntroduction to the American Legal System, Constitutional Law, American Constitution, Eur opean Politics  \\n \\nRELEVANT EXPERIENCE Whitestone Chambers, Legal Intern London, UK             January -April 2022  ', 'Name: 190\\n\\nIntroduction to the American Legal System, Constitutional Law, American Constitution, Eur opean Politics  \\n \\nRELEVANT EXPERIENCE Whitestone Chambers, Legal Intern London, UK             January -April 2022  ', 'Name: 81\\n\\nCompeted as a pre-trial attorney in a mock criminal law case \\n●\\nAnalyzed legal documents pertaining to the case and argued for the permittance of evidence \\nutilizing Supreme Court cases \\nLeapStart\\nCupertino, CA \\nTeacher\\nJune 2020- June 2022 \\n●')\n",
            " Relatednesses:(0.7921320362469393, 0.7905199568274273, 0.7850107571639804)\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-09 16:35:14.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m You are a Human Resources agent looking for skills in resumes\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 110\n",
            "\n",
            "Introduction to the American Legal System, Constitutional Law, American Constitution, Eur opean Politics  \n",
            " \n",
            "RELEVANT EXPERIENCE Whitestone Chambers, Legal Intern London, UK             January -April 2022  \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 190\n",
            "\n",
            "Introduction to the American Legal System, Constitutional Law, American Constitution, Eur opean Politics  \n",
            " \n",
            "RELEVANT EXPERIENCE Whitestone Chambers, Legal Intern London, UK             January -April 2022  \n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 81\n",
            "\n",
            "Competed as a pre-trial attorney in a mock criminal law case \n",
            "●\n",
            "Analyzed legal documents pertaining to the case and argued for the permittance of evidence \n",
            "utilizing Supreme Court cases \n",
            "LeapStart\n",
            "Cupertino, CA \n",
            "Teacher\n",
            "June 2020- June 2022 \n",
            "●\n",
            "\"\"\"\n",
            "\n",
            "Question: who knows law\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The following individuals have knowledge of law based on their resumes:\\n\\n- 110\\n- 190\\n- 81'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask('who majored in physics')"
      ],
      "metadata": {
        "id": "0Gvwt0TZpS-R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "8904af22-4ab7-453d-db7a-fe17aef7509f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-05-09 16:37:22.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mquery_message\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mStrings Found From Search\n",
            ":('Name: 5\\n\\nB.S. in Physics and B.S. in Financial Math & Statistics \\n●\\nCoursework:\\nmultivariable calculus,\\nprobability and\\nstatistics,\\nlinear algebra,\\nODE&PDEs,\\ncomplex analysis, numerical methods, regression, stochastic process, machine learning \\n●\\nHonors/Awards:', \"Name: 30\\n\\nB.S. in Material Science and Engineering, Minor in Economics \\n●\\nCoursework:\\nstochastic process, probability theory,\\nlinear algebra, MLE, machine learning,\\npartial differential equation, corporate finance, game theory, Hamilton's equations,\", 'Name: 130\\n\\nBachelor of Science in General Science, Minor in Business Administration, GPA: 3.42\\nHigh School Diploma:\\nSt. Jean Baptiste High School\\nJune 2017\\nRelevant Coursework\\nGeneral Chemistry, Macroeconomics, Computer Science, Philosophical Ethics\\nAwards\\nFordham Jogues Scholarship, Fordham ASPIRES Scholarship\\nExperience')\n",
            " Relatednesses:(0.8035927163360886, 0.7955498162198213, 0.7939262468168876)\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-09 16:37:23.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m You are a Human Resources agent looking for skills in resumes\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 5\n",
            "\n",
            "B.S. in Physics and B.S. in Financial Math & Statistics \n",
            "●\n",
            "Coursework:\n",
            "multivariable calculus,\n",
            "probability and\n",
            "statistics,\n",
            "linear algebra,\n",
            "ODE&PDEs,\n",
            "complex analysis, numerical methods, regression, stochastic process, machine learning \n",
            "●\n",
            "Honors/Awards:\n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 30\n",
            "\n",
            "B.S. in Material Science and Engineering, Minor in Economics \n",
            "●\n",
            "Coursework:\n",
            "stochastic process, probability theory,\n",
            "linear algebra, MLE, machine learning,\n",
            "partial differential equation, corporate finance, game theory, Hamilton's equations,\n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "Name: 130\n",
            "\n",
            "Bachelor of Science in General Science, Minor in Business Administration, GPA: 3.42\n",
            "High School Diploma:\n",
            "St. Jean Baptiste High School\n",
            "June 2017\n",
            "Relevant Coursework\n",
            "General Chemistry, Macroeconomics, Computer Science, Philosophical Ethics\n",
            "Awards\n",
            "Fordham Jogues Scholarship, Fordham ASPIRES Scholarship\n",
            "Experience\n",
            "\"\"\"\n",
            "\n",
            "Question: who majored in physics\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The first resume section mentions a B.S. in Physics, so the person in that section majored in physics.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(clean_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyVhRVwlX0Qk",
        "outputId": "9e7b4dca-a6c3-412a-93f5-a2426ba7d654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiKK-wOraNca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}