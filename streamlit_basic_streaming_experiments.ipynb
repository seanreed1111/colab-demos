{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanreed1111/colab-demos/blob/master/streamlit_basic_streaming_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA2qPZe9Uk9C"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_101/101-1-streamlit_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLMrWVStFdAx"
      },
      "outputs": [],
      "source": [
        "%pip install -q langchain langchain_community langchain_openai tavily-python streamlit loguru sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload allconfig.json, DDL_for_LLM_upload_sample.sql"
      ],
      "metadata": {
        "id": "NWMAuo7V1Rmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAm3euIZFLSB"
      },
      "outputs": [],
      "source": [
        "# %%writefile streaming_app.py\n",
        "# import os, json\n",
        "# from pprint import pprint\n",
        "# from pathlib import Path\n",
        "# from loguru import logger\n",
        "# from langchain.callbacks.base import BaseCallbackHandler\n",
        "# from langchain.schema import ChatMessage\n",
        "# # from langchain_openai import ChatOpenAI\n",
        "# from langchain_community.chat_models.azure_openai import AzureChatOpenAI #deprecated class, fix later\n",
        "# import streamlit as st\n",
        "\n",
        "# LANGCHAIN_PROJECT = \"Experiment #1: Chat with Azure OpenAI\"\n",
        "# st.set_page_config(page_title=LANGCHAIN_PROJECT, page_icon=\"\")\n",
        "# st.title(LANGCHAIN_PROJECT)\n",
        "\n",
        "# def run_config():\n",
        "#     config_dir = Path.cwd()\n",
        "#     openai_config_file_path = config_dir / \"allconfig.json\"\n",
        "#     config_files = [openai_config_file_path]\n",
        "#     config = {}\n",
        "#     for file in config_files:\n",
        "#         with open(file) as json_config:\n",
        "#             config.update(json.load(json_config))\n",
        "#     for k in config:\n",
        "#         os.environ[k] = config[k]\n",
        "\n",
        "#     os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
        "\n",
        "# with st.sidebar:\n",
        "#     config_load_state = st.text('Loading config...')\n",
        "#     run_config()\n",
        "#     config_load_state.text('Loading config...done!')\n",
        "\n",
        "\n",
        "# class StreamHandler(BaseCallbackHandler):\n",
        "#     def __init__(self, container, initial_text=\"\"):\n",
        "#         self.container = container\n",
        "#         self.text = initial_text\n",
        "\n",
        "#     def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "#         self.text += token\n",
        "#         self.container.markdown(self.text)\n",
        "\n",
        "\n",
        "# # with st.sidebar:\n",
        "# #     openai_api_key = st.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "\n",
        "# if \"messages\" not in st.session_state:\n",
        "#     st.session_state[\"messages\"] = [ChatMessage(role=\"assistant\", content=\"How can I help you?\")]\n",
        "\n",
        "# for msg in st.session_state.messages:\n",
        "#     st.chat_message(msg.role).write(msg.content)\n",
        "\n",
        "# if prompt := st.chat_input():\n",
        "#     st.session_state.messages.append(ChatMessage(role=\"user\", content=prompt))\n",
        "#     st.chat_message(\"user\").write(prompt)\n",
        "\n",
        "#     # if not openai_api_key:\n",
        "#     #     st.info(\"Please add your OpenAI API key to continue.\")\n",
        "#     #     st.stop()\n",
        "\n",
        "#     with st.chat_message(\"assistant\"):\n",
        "#         stream_handler = StreamHandler(st.empty())\n",
        "#         llm = AzureChatOpenAI(\n",
        "#             temperature=0,\n",
        "#             streaming=True,\n",
        "#             max_tokens=100,\n",
        "#             azure_deployment=os.environ[\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"],\n",
        "#             azure_endpoint=os.environ[\"AZURE_OPENAI_API_ENDPOINT\"],\n",
        "#             model_name=os.environ[\"MODEL_NAME\"],\n",
        "#             openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "#             request_timeout=45,\n",
        "#             verbose=True,\n",
        "#             callbacks=[stream_handler]\n",
        "#         )\n",
        "\n",
        "#         response = llm.invoke(st.session_state.messages)\n",
        "#         st.session_state.messages.append(ChatMessage(role=\"assistant\", content=response.content))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streaming_app.py\n",
        "#streaming_with_schema_context.py\n",
        "import os, json\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "from loguru import logger\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ChatMessage\n",
        ")\n",
        "# from langchain_openai import ChatOpenAI\n",
        "from langchain_community.chat_models.azure_openai import AzureChatOpenAI #deprecated class, fix later\n",
        "import streamlit as st\n",
        "\n",
        "LANGCHAIN_PROJECT = \"Experiment #2: Provide SQL Schema for Queries - GPT4\"\n",
        "st.set_page_config(page_title=LANGCHAIN_PROJECT, page_icon=\"\")\n",
        "st.title(LANGCHAIN_PROJECT)\n",
        "\n",
        "def run_config():\n",
        "    config_dir = Path.cwd()\n",
        "    openai_config_file_path = config_dir / \"allconfig.json\"\n",
        "    config_files = [openai_config_file_path]\n",
        "    config = {}\n",
        "    for file in config_files:\n",
        "        with open(file) as json_config:\n",
        "            config.update(json.load(json_config))\n",
        "    for k in config:\n",
        "        os.environ[k] = config[k]\n",
        "\n",
        "    os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
        "\n",
        "def load_schema_from_file(file):\n",
        "    try:\n",
        "        with open(file, 'r') as f:\n",
        "            schema = f.read()\n",
        "        assert schema is not None\n",
        "        return schema\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "with st.sidebar:\n",
        "    # config_load_state = st.text('Loading config...')\n",
        "    run_config()\n",
        "    # config_load_state.text('Loading config...done!')\n",
        "    SCHEMA_FILEPATH  = (Path.cwd()) / \"DDL_for_LLM_upload_sample.sql\"\n",
        "    uploaded_schema = f\"{load_schema_from_file(SCHEMA_FILEPATH)}\"\n",
        "    schema = st.text(f\"{uploaded_schema}\")\n",
        "    st.session_state[\"schema\"] = uploaded_schema\n",
        "\n",
        "\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    # see https://blog.langchain.dev/llms-and-sql/\n",
        "    system_message = f\"\"\"\n",
        "    You are an expert at writing Mircosoft SQL database queries and T-SQL code.\n",
        "      When asked to write SQL queries use the following schema\n",
        "     \\n\\n\\n\n",
        "    {uploaded_schema}\n",
        "     \\n\\n\\n\n",
        "     After writing a query, score its estimated accuracy in answering the\n",
        "      user's question on a scale of 1-5, with 1 the lowest score and 5 the\n",
        "      highest possible. Respond with the query and the accuracy score. If you give\n",
        "      an accuracy score of 1 or 2, briefly state your reason.\n",
        "     \"\"\"\n",
        "    st.session_state[\"system_message\"] = SystemMessage(content=system_message)\n",
        "    st.session_state[\"messages\"] = [ChatMessage(role=\"system\", content=system_message),\n",
        "                                    ChatMessage(role=\"assistant\", content=\"How can I help you?\")\n",
        "                                    ]\n",
        "\n",
        "for msg in st.session_state.messages:\n",
        "    if msg.role != \"system\":\n",
        "        st.chat_message(msg.role).write(msg.content)\n",
        "\n",
        "if prompt := st.chat_input():\n",
        "    st.session_state.messages.append(ChatMessage(role=\"user\", content=prompt))\n",
        "    st.chat_message(\"user\").write(prompt)\n",
        "\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        stream_handler = StreamHandler(st.empty())\n",
        "        llm = AzureChatOpenAI(\n",
        "            temperature=0,\n",
        "            streaming=True,\n",
        "            max_tokens=1000,\n",
        "            azure_deployment=os.environ[\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"],\n",
        "            azure_endpoint=os.environ[\"AZURE_OPENAI_API_ENDPOINT\"],\n",
        "            model_name=os.environ[\"MODEL_NAME\"],\n",
        "            openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "            request_timeout=45,\n",
        "            verbose=True,\n",
        "            callbacks=[stream_handler]\n",
        "        )\n",
        "\n",
        "        response = llm.invoke(st.session_state.messages)\n",
        "        st.session_state.messages.append(ChatMessage(role=\"assistant\", content=response.content))"
      ],
      "metadata": {
        "id": "XD4oeXv2a9Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile streaming_app.py\n",
        "# experiment-3-chat-with-local-db\n",
        "# import streamlit as st\n",
        "# from pathlib import Path\n",
        "# import os, json\n",
        "# from langchain_community.chat_models.azure_openai import AzureChatOpenAI #deprecated class, fix later\n",
        "# from langchain.agents import create_sql_agent\n",
        "# from langchain.sql_database import SQLDatabase\n",
        "# from langchain.agents.agent_types import AgentType\n",
        "# from langchain.callbacks import StreamlitCallbackHandler\n",
        "# from langchain.agents.agent_toolkits import SQLDatabaseToolkit #https://python.langchain.com/docs/integrations/toolkits/sql_database#use-sqldatabasetoolkit-within-an-agent\n",
        "# from sqlalchemy import create_engine\n",
        "# import sqlite3\n",
        "\n",
        "\n",
        "# LANGCHAIN_PROJECT = \"Experiment #3 Chat With SQL DB - GPT4\"\n",
        "# st.set_page_config(page_title=LANGCHAIN_PROJECT, page_icon=\"\")\n",
        "# st.title(LANGCHAIN_PROJECT)\n",
        "\n",
        "# def run_config():\n",
        "#     config_dir = Path.cwd()\n",
        "#     openai_config_file_path = config_dir / \"allconfig.json\"\n",
        "#     config_files = [openai_config_file_path]\n",
        "#     config = {}\n",
        "#     for file in config_files:\n",
        "#         with open(file) as json_config:\n",
        "#             config.update(json.load(json_config))\n",
        "#     for k in config:\n",
        "#         os.environ[k] = config[k]\n",
        "\n",
        "#     os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
        "\n",
        "# run_config()\n",
        "# LOCALDB = \"USE_LOCALDB\"\n",
        "\n",
        "# # User inputs\n",
        "# radio_opt = [\"Use sample database - Chinook.db\", \"Connect to your own SQL database\"]\n",
        "# selected_opt = st.sidebar.radio(label=\"Choose suitable option\", options=radio_opt)\n",
        "# if radio_opt.index(selected_opt) == 1:\n",
        "#     db_uri = st.sidebar.text_input(\n",
        "#         label=\"Database URI\", placeholder=\"mysql://user:pass@hostname:port/db\"\n",
        "#     )\n",
        "# else:\n",
        "#     db_uri = LOCALDB\n",
        "\n",
        "# # Check user inputs\n",
        "# if not db_uri:\n",
        "#     st.info(\"Please enter database URI to connect to your database.\")\n",
        "#     st.stop()\n",
        "\n",
        "# # Setup agent\n",
        "# llm = AzureChatOpenAI(\n",
        "#             temperature=0,\n",
        "#             streaming=True,\n",
        "#             max_tokens=900,\n",
        "#             azure_deployment=os.environ[\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"],\n",
        "#             azure_endpoint=os.environ[\"AZURE_OPENAI_API_ENDPOINT\"],\n",
        "#             model_name=os.environ[\"MODEL_NAME\"],\n",
        "#             openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "#             request_timeout=45,\n",
        "#             verbose=True,\n",
        "#             # callbacks=[stream_handler] PUSH THE HANDLING OF CALLBACKS TO THE AGENT\n",
        "#         )\n",
        "\n",
        "\n",
        "# @st.cache_resource(ttl=\"2h\")\n",
        "# def configure_db(db_uri):\n",
        "#     if db_uri == LOCALDB:\n",
        "#         # Make the DB connection read-only to reduce risk of injection attacks\n",
        "#         # See: https://python.langchain.com/docs/security\n",
        "#         db_filepath = Path.cwd() / \"Chinook.db\"\n",
        "#         creator = lambda: sqlite3.connect(f\"file:{db_filepath}?mode=ro\", uri=True)\n",
        "#         return SQLDatabase(create_engine(\"sqlite:///\", creator=creator))\n",
        "#     return SQLDatabase.from_uri(database_uri=db_uri)\n",
        "\n",
        "\n",
        "# db = configure_db(db_uri)\n",
        "\n",
        "# toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "\n",
        "# agent = create_sql_agent(\n",
        "#     llm=llm,\n",
        "#     toolkit=toolkit,\n",
        "#     verbose=True,\n",
        "#     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "# )\n",
        "\n",
        "# if \"messages\" not in st.session_state or st.sidebar.button(\"Clear message history\"):\n",
        "#     st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n",
        "\n",
        "# for msg in st.session_state.messages:\n",
        "#     st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
        "\n",
        "# user_query = st.chat_input(placeholder=\"Ask me anything!\")\n",
        "\n",
        "# if user_query:\n",
        "#     st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "#     st.chat_message(\"user\").write(user_query)\n",
        "\n",
        "#     with st.chat_message(\"assistant\"):\n",
        "#         st_cb = StreamlitCallbackHandler(st.container())\n",
        "#         response = agent.run(user_query, callbacks=[st_cb])\n",
        "#         st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "#         st.write(response)\n"
      ],
      "metadata": {
        "id": "H0gq7uZwHwPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifux-mmzcTQK"
      },
      "source": [
        "## Find the IP of your instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQz0WaTTcTQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bee3f0-35f9-43fc-e768-fc26d5cabcd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.73.145.113\n",
            "Copy this IP into the webpage that opens below\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.68s\n",
            "your url is: https://proud-eagles-prove.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run streaming_app.py &>/content/logs.txt &\n",
        "!curl ipv4.icanhazip.com\n",
        "!echo \"Copy this IP into the webpage that opens below\"\n",
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTHeT-689TLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}