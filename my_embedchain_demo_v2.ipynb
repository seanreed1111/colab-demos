{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanreed1111/colab-demos/blob/master/my_embedchain_demo_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedchain\n",
        "\n",
        "Embedchain is a framework to easily create LLM powered bots over any dataset.\n",
        "\n",
        "Here is a very simple demo about how it work!"
      ],
      "metadata": {
        "id": "8OnF6qsbDG1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we install the dependencies:"
      ],
      "metadata": {
        "id": "-VQE9nGwDaiG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qtqSnCgwDDCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0fb920-9743-4512-c48e-c3de5b06ec08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.5/415.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.9/271.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq --upgrade embedchain loguru python-dotenv tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we import the dependencies:"
      ],
      "metadata": {
        "id": "DrrlU4QqDmxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, json\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from embedchain import App, CustomApp\n",
        "from embedchain.config import(AppConfig,\n",
        "                              CustomAppConfig,\n",
        "                              AddConfig,\n",
        "                              QueryConfig,\n",
        "                              ChatConfig,\n",
        "                              ChunkerConfig\n",
        "                              )\n",
        "from embedchain.models import Providers, EmbeddingFunctions\n",
        "from string import Template\n",
        "from typing import Optional\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\"/content/deployment.env\")\n",
        "from loguru import logger\n",
        "logger.add(sys.stderr, colorize=True, format=\"<green>{time}</green> <level>{message}</level>\", level=\"DEBUG\")\n"
      ],
      "metadata": {
        "id": "ohPMkz4zDqxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae126175-6829-4a39-876c-22311d578619"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(EmbeddingFunctions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHSzcXVKvgRF",
        "outputId": "4adede7f-691f-4bac-960f-5e866848f564"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GPT4ALL',\n",
              " 'HUGGING_FACE',\n",
              " 'OPENAI',\n",
              " 'VERTEX_AI',\n",
              " '__class__',\n",
              " '__doc__',\n",
              " '__members__',\n",
              " '__module__']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I need to write my own class that inherits from CustomAppConfig"
      ],
      "metadata": {
        "id": "cUbVkHr_1v7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Optional\n",
        "\n",
        "from chromadb.api.types import Documents, Embeddings\n",
        "from embedchain.config.apps.BaseAppConfig import BaseAppConfig\n",
        "from embedchain.models import (EmbeddingFunctions, Providers, VectorDatabases,\n",
        "                               VectorDimensions)\n",
        "\n",
        "# load_dotenv(\"/content/deployment.env\")\n",
        "\n",
        "class MyAzureOpenAIAppConfig(BaseAppConfig):\n",
        "    \"\"\"\n",
        "    Config to initialize an embedchain custom `CustomAppConfig` instance that adds embedding_chunk_size=16\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_chunk_size=16,\n",
        "        log_level='DEBUG',\n",
        "        embedding_fn=None,\n",
        "        db=None,\n",
        "        host=None,\n",
        "        port=None,\n",
        "        id=None,\n",
        "        collection_name=None,\n",
        "        provider= Providers.OPENAI,\n",
        "        deployment_name=None,\n",
        "        collect_metrics= None,\n",
        "        db_type= None,\n",
        "        es_config=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param embedding_chunk_size: Optional. (Int) accounts for Azure OPENAI throttling\n",
        "        :param log_level: Optional. (String) Debug level\n",
        "        ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'].\n",
        "        :param embedding_fn: Optional. Embedding function to use.\n",
        "        :param embedding_fn_model: Optional. Model name to use for embedding function.\n",
        "        :param db: Optional. (Vector) database to use for embeddings.\n",
        "        :param host: Optional. Hostname for the database server.\n",
        "        :param port: Optional. Port for the database server.\n",
        "        :param id: Optional. ID of the app. Document metadata will have this id.\n",
        "        :param collection_name: Optional. Collection name for the database.\n",
        "        :param provider: Optional. (Providers): LLM Provider to use.\n",
        "        :param open_source_app_config: Optional. Config instance needed for open source apps.\n",
        "        :param collect_metrics: Defaults to True. Send anonymous telemetry to improve embedchain.\n",
        "        :param db_type: Optional. type of Vector database to use.\n",
        "        :param es_config: Optional. elasticsearch database config to be used for connection\n",
        "        \"\"\"\n",
        "        if provider:\n",
        "            self.provider = provider\n",
        "        else:\n",
        "            raise ValueError(\"CustomApp must have a provider assigned.\")\n",
        "\n",
        "        super().__init__(\n",
        "            log_level=log_level,\n",
        "            embedding_fn=MyAzureOpenAIAppConfig.my_embedding_function(\n",
        "                deployment_name=deployment_name,\n",
        "                embedding_chunk_size = embedding_chunk_size\n",
        "            ),\n",
        "            db=db,\n",
        "            host=host,\n",
        "            port=port,\n",
        "            id=id,\n",
        "            collection_name=collection_name,\n",
        "            collect_metrics=collect_metrics,\n",
        "            db_type=db_type,\n",
        "            vector_dim=CustomAppConfig.get_vector_dimension(embedding_function=embedding_fn),\n",
        "            es_config=es_config,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def langchain_default_concept(embeddings: Any):\n",
        "        \"\"\"\n",
        "        Langchains default function layout for embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        def embed_function(texts: Documents) -> Embeddings:\n",
        "            return embeddings.embed_documents(texts)\n",
        "\n",
        "        return embed_function\n",
        "\n",
        "    @staticmethod\n",
        "    def get_vector_dimension(embedding_function: EmbeddingFunctions):\n",
        "      return VectorDimensions.OPENAI.value\n",
        "\n",
        "    @staticmethod\n",
        "    def my_embedding_function(deployment_name: str = None,\n",
        "                              embedding_chunk_size: int = 16\n",
        "                              ):\n",
        "      if deployment_name:\n",
        "          embeddings = OpenAIEmbeddings(deployment=deployment_name, chunk_size=embedding_chunk_size)\n",
        "      else:\n",
        "          embeddings = OpenAIEmbeddings(chunk_size=embedding_chunk_size)\n",
        "      return MyAzureOpenAIAppConfig.langchain_default_concept(embeddings)"
      ],
      "metadata": {
        "id": "yrbVzCdEuqCE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKwUtpPj2jRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup My CustomAppConfig"
      ],
      "metadata": {
        "id": "mxiXJ1qw7141"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I need to add embedding_chunk_size = 16 into the embedding_function. manually!\n",
        "@logger.catch\n",
        "def get_azure_openai_app_config():\n",
        "  return MyAzureOpenAIAppConfig(\n",
        "      embedding_fn=EmbeddingFunctions.OPENAI,\n",
        "      provider=Providers.AZURE_OPENAI,\n",
        "      log_level=\"DEBUG\",\n",
        "      deployment_name=os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")\n",
        "  )\n",
        "\n",
        "bot = CustomApp(get_azure_openai_app_config())"
      ],
      "metadata": {
        "id": "PzDddNvIgVyF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up a default prompt"
      ],
      "metadata": {
        "id": "4c03mOSulbBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import Template\n",
        "\n",
        "DEFAULT_PROMPT = \"\"\"\n",
        "  You are a chatbot having a conversation. You are given chat\n",
        "  history and context.\n",
        "  You need to answer the query considering context,\n",
        "  chat history and your knowledge base.\n",
        "  If you don't know the answer or the answer is neither contained in the context\n",
        "  nor in history, then simply say \"No idea, bro\".\n",
        "\n",
        "  $context\n",
        "\n",
        "  History: $history\n",
        "\n",
        "  Query: $query\n",
        "\n",
        "  Helpful Answer:\n",
        "\"\"\"  # noqa:E501\n",
        "\n",
        "TEMPLATE = Template(DEFAULT_PROMPT)"
      ],
      "metadata": {
        "id": "arkNS1K_jHD7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, add different data sources using embedchain's `.add()` method:"
      ],
      "metadata": {
        "id": "WRKWFWSGD1tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@logger.catch\n",
        "def void_add_databases():\n",
        "  chunker_config = AddConfig(chunker=ChunkerConfig(chunk_size=300, chunk_overlap=30)) #for TEXT\n",
        "  bot.add(\"https://en.wikipedia.org/wiki/A._W._Peet\", config=chunker_config)\n",
        "  bot.add(\"https://www.trinity.utoronto.ca/directory/peet-a-w/\", config=chunker_config)\n",
        "  bot.add(\"https://www.youtube.com/watch?v=gBYcM9fe8YA\",\"youtube_video\")\n",
        "\n",
        "void_add_databases()"
      ],
      "metadata": {
        "id": "MhByl8oxEJFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473700b0-d177-487f-f260-5af73e0cfeb1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved https://en.wikipedia.org/wiki/A._W._Peet (DataType.WEB_PAGE). New chunks count: 25\n",
            "Successfully saved https://www.trinity.utoronto.ca/directory/peet-a-w/ (DataType.WEB_PAGE). New chunks count: 5\n",
            "Successfully saved https://www.youtube.com/watch?v=gBYcM9fe8YA (DataType.YOUTUBE_VIDEO). New chunks count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the ChatConfig"
      ],
      "metadata": {
        "id": "q6q4HHDcpeCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 0\n",
        "MAX_TOKENS = 1500\n",
        "NUMBER_DOCUMENTS = 3 #how many documents to retrieve from the database\n",
        "\n",
        "chat_config = ChatConfig(\n",
        "    template=TEMPLATE,\n",
        "    number_documents=NUMBER_DOCUMENTS,# default is set to 1 by parent class QueryConfig\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=TEMPERATURE, # default is set to 0 by parent class QueryConfig\n",
        "    max_tokens=MAX_TOKENS, # default is set to 1000 by parent class QueryConfig\n",
        "    top_p=None,\n",
        "    stream=False,\n",
        "    deployment_name=os.getenv(\"DEPLOYMENT_NAME\")\n",
        ")"
      ],
      "metadata": {
        "id": "uaelViCWkZiB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fovHVUcEpcLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your bot is ready now. Ask your bot any questions using `.query()` or `.chat()`  methods\n",
        "\n",
        "If you have a `ChatConfig`, be sure to include it in the call to `.chat()`."
      ],
      "metadata": {
        "id": "aeTOQettEfWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@logger.catch\n",
        "def response():\n",
        "  return bot.chat(\"What are does Professor Peet research?\", config=chat_config)\n",
        "\n",
        "response()"
      ],
      "metadata": {
        "id": "HZnx0V18EkzS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f825d803-e336-4876-e3d4-525382ee963b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As per the information available, Professor A.W. Peet is a physics professor at the University of Toronto and an affiliate of the Perimeter Institute for Theoretical Physics. However, there is no specific information available about their research area.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pBCQj8wh4CL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}