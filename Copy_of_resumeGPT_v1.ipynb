{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2vfXtwInXKDPYnUoagoJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanreed1111/colab-demos/blob/master/Copy_of_resumeGPT_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qqq loguru textract tiktoken openai azure-ai-ml mlflow azureml-sdk azureml-mlflow #ast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poRnH-oftTPb",
        "outputId": "b71b610a-71e8-4dc1-85ee-dbc270eb8f60",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m814.0/814.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.9/173.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.2/136.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.5/135.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.7/245.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.7/245.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.9/965.9 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.4/779.4 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.4/141.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.4/245.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.7/191.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.1/31.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fusepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.18 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# before running this notebook, UPLOAD these files\n",
        "- openai.env\n",
        "- azure.env\n",
        "- Resumes stored in \"Resumes\" dir\n",
        " "
      ],
      "metadata": {
        "id": "gqPb-Nn776tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os,argparse,loguru, json, time, datetime, openai\n",
        "from pathlib import Path\n",
        "from loguru import logger"
      ],
      "metadata": {
        "id": "scY-DzUJquiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_load_aml_env_vars(env_path=None):\n",
        "  import os, json\n",
        "  try:\n",
        "    with open(env_path, \"r\") as f:\n",
        "      env_vars = json.load(f)\n",
        "    os.environ[\"resource_group\"] = env_vars[\"resource_group\"]\n",
        "    os.environ[\"workspace_name\"] = env_vars[\"workspace_name\"]\n",
        "    os.environ[\"subscription_id\"] = env_vars[\"subscription_id\"]\n",
        "    if (os.getenv(\"resource_group\") and os.getenv(\"workspace_name\")\n",
        "    and os.getenv(\"subscription_id\")):\n",
        "      return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "    return False"
      ],
      "metadata": {
        "id": "A1d_9gbMZ78f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_open_ai_key(env_path=None):\n",
        "  import json, os\n",
        "  from pathlib import Path\n",
        "  try:\n",
        "    with open(env_path, \"r\") as f:\n",
        "        env_vars = json.load(f)\n",
        "    os.environ[\"OPENAI_API_KEY\"] = env_vars[\"OPENAI_API_KEY\"]\n",
        "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "    openai.Model.list() #test a random command on the openai API\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "  return False\n",
        "\n",
        "def test_set_open_ai_key(key_path=None):\n",
        "  openai.api_key = None #disconnect from api key if already registered\n",
        "  try:\n",
        "    set_open_ai_key(key_path)\n",
        "    openai.Model.list()\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    logger.error(f\"{e}\")\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "wwZXC6jUTLvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_get_ml_client(env_path=None):\n",
        "  # this is a mix of sdk v1 and v2. Try to consolidate \n",
        "  import json, os, mlflow\n",
        "  from pathlib import Path\n",
        "  from azureml.core import Workspace \n",
        "  from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "  from azure.ai.ml import MLClient\n",
        "\n",
        "  if not env_path: return None\n",
        "\n",
        "  ws = Workspace.from_config(env_path)\n",
        "  tracking_uri = ws.get_mlflow_tracking_uri()\n",
        "  mlflow.set_tracking_uri(tracking_uri)\n",
        "\n",
        "  try:\n",
        "      credential = DefaultAzureCredential()\n",
        "  except Exception as ex:\n",
        "      # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not working\n",
        "      credential = InteractiveBrowserCredential()\n",
        "\n",
        "  is_loaded = maybe_load_aml_env_vars(env_path)\n",
        "  if is_loaded:\n",
        "    try:\n",
        "      ml_client = MLClient(\n",
        "          subscription_id=os.getenv(\"subscription_id\"),\n",
        "          resource_group_name=os.getenv(\"resource_group\"),\n",
        "          workspace_name=os.getenv(\"workspace_name\"),\n",
        "          credential=credential,\n",
        "      )\n",
        "      return ml_client\n",
        "    except Exception as e:\n",
        "      logger.error(f\"{e}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "zNLN8FKHzlAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_setup_azure_env(azure_env_path=None):\n",
        "  # setup azure ml env if azure credentials are available.\n",
        "  if azure_env_path and azure_env_path.is_file() and maybe_load_aml_env_vars(azure_env_path):\n",
        "    ml_client = maybe_get_ml_client(azure_env_path)\n",
        "    if ml_client:\n",
        "      #do a random test to check that ml_client and mlflow are playing nicely together\n",
        "      import mlflow\n",
        "      experiment_name = 'mlflow-2'\n",
        "      mlflow.set_experiment(experiment_name)\n",
        "      from random import random\n",
        "\n",
        "      with mlflow.start_run() as mlflow_test_run:\n",
        "          mlflow.log_param(\"hello_param\", \"world\")\n",
        "          mlflow.log_metric(\"hello_metric2\", random())\n",
        "          os.system(f\"echo 'hello world2' > helloworld2.txt\")\n",
        "          mlflow.log_artifact(\"helloworld2.txt\")\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "pzzqgJ9k9f34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "azure_env_path, openai_env_path, ml_client, openai.api_key = None, None, None , None\n",
        "cwd = Path.cwd()\n",
        "resume_path = cwd / \"Resumes\"\n",
        "resume_path.mkdir(exist_ok=True)\n",
        "\n",
        "# azure_env_path = cwd / \"azure.env\" ##uncomment if providing azure env\n",
        "openai_env_path = cwd/ \"openai.env\"\n",
        "maybe_setup_azure_env(azure_env_path)\n",
        "set_open_ai_key(openai_env_path)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCqHlEmz8CsZ",
        "outputId": "62353c00-b8d5-43f1-f299-b55af00d4cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPLIT SECTIONS\n",
        "source: Embedding_Wikipedia_articles_for_search.ipynb \n",
        "https://colab.research.google.com/drive/1EJMtCmF8jZc2Y-c1RaBxFSCTPcjzjJf4#scrollTo=TOVSYkDur9zA"
      ],
      "metadata": {
        "id": "5kUr4TQLdnRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll recursively split long sections into smaller sections.\n",
        "\n",
        "There's no perfect recipe for splitting text into sections.\n",
        "\n",
        "Some tradeoffs include:\n",
        "- Longer sections may be better for questions that require more context\n",
        "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
        "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
        "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
        "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
        "\n",
        "Here, we'll use a simple approach and limit sections to 1,000 tokens each by default, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
      ],
      "metadata": {
        "id": "NVpjUKLwenCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### extract text from pdf"
      ],
      "metadata": {
        "id": "pK5mrZ8PJ4L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract, os, openai, tiktoken"
      ],
      "metadata": {
        "id": "fJ-sflJ77VjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO walk the directory to get all the filenames, use as names of the people in tagging and document retrieval\n",
        "#TODO use regex to get rid of excess spaces and new lines. only one new line needed per line.\n",
        "file_names = [\"Jesse_Jayant.pdf\", \"Nadia_Smythe.pdf\", \"Naimal_Chisti.pdf\", \"SeanReed.pdf\"]\n",
        "file_paths = [(resume_path / file) for file in file_names];print(file_paths)\n",
        "names = [path.stem.lower() for path in file_paths];print(names)\n",
        "\n",
        "# Extract the raw text from each PDF using textract\n",
        "\n",
        "texts =[textract.process((file_path), method='pdfminer').decode('utf-8') for file_path in file_paths]\n",
        "\n",
        "#TODO Do more cleaning with regex\n",
        "texts = [text.strip().replace(\"  \", \" \") for text in texts]\n",
        "#create tuple[list[str],str]\n",
        "clean_texts = [(['NameOnResume: '+ item1], item2) for item1,item2 in zip(names,texts)] \n",
        "\n",
        "len(clean_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5EkHdPqircB",
        "outputId": "522cd4ab-c2b1-42ac-a71d-1192b17daa8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PosixPath('/content/Resumes/Jesse_Jayant.pdf'), PosixPath('/content/Resumes/Nadia_Smythe.pdf'), PosixPath('/content/Resumes/Naimal_Chisti.pdf'), PosixPath('/content/Resumes/SeanReed.pdf')]\n",
            "['jesse_jayant', 'nadia_smythe', 'naimal_chisti', 'seanreed']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = 'text-ada-001'  # only matters insofar as it selects which tokenizer to use\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # no delimiter found\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # no need to search for halfway point\n",
        "    else:\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        best_diff = halfway\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "def truncated_string(\n",
        "    string: str,\n",
        "    model: str,\n",
        "    max_tokens: int,\n",
        "    print_warning: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    if print_warning and len(encoded_string) > max_tokens:\n",
        "        logger.warning(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
        "    return truncated_string"
      ],
      "metadata": {
        "id": "u3CZ5MwnnspR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str],\n",
        "    max_tokens: int = 1000,\n",
        "    model: str = GPT_MODEL,\n",
        "    max_recursion: int = 5,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
        "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # if length is fine, return string\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # if recursion hasn't found a split after X iterations, just truncate\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # otherwise, split in half and recurse\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # if either half is empty, retry with a more fine-grained delimiter\n",
        "                continue\n",
        "            else:\n",
        "                # recurse on each half\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1,\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # otherwise no split was found, so just truncate (should be very rare)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        " "
      ],
      "metadata": {
        "id": "9JmwS-cZdlvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split resumes into chunks. Small chunks probably better when searching for skills? \n",
        "# maybe even shrink to individual sentences\n",
        "MAX_TOKENS = 150\n",
        "resume_strings = []\n",
        "for section in clean_texts:\n",
        "    resume_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(clean_texts)} resumes split into {len(resume_strings)} strings.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOlixxaYpCRC",
        "outputId": "8bc561a4-9067-4ffb-f6fa-09c5dfb4b2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 resumes split into 50 strings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculate embeddings and store in dataframe"
      ],
      "metadata": {
        "id": "Izi2RX_O1u0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
        "MAX_BATCH_SIZE = 1000 # you can submit up to 2048 embedding inputs per request\n",
        "NUMBER_OF_STRINGS_TO_EMBED = len(resume_strings)\n",
        "\n",
        "if NUMBER_OF_STRINGS_TO_EMBED < MAX_BATCH_SIZE:\n",
        "  BATCH_SIZE = NUMBER_OF_STRINGS_TO_EMBED\n",
        "else: \n",
        "  BATCH_SIZE = MAX_BATCH_SIZE \n",
        "\n",
        "embeddings = []\n",
        "for batch_start in range(0, NUMBER_OF_STRINGS_TO_EMBED, BATCH_SIZE):\n",
        "    batch_end = batch_start + BATCH_SIZE\n",
        "    batch = resume_strings[batch_start:batch_end]\n",
        "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
        "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
        "    for i, be in enumerate(response[\"data\"]):\n",
        "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
        "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
        "    embeddings.extend(batch_embeddings)\n",
        "\n",
        "df = pd.DataFrame({\"text\": resume_strings, \"embedding\": embeddings})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hciff-Ul1t1B",
        "outputId": "ce525862-d8f7-4ace-cc9a-db82fab37ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 to 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6HrUTQA42mkv",
        "outputId": "0b2bbcf6-b671-4491-ffc0-4087951dd29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  NameOnResume: jesse_jayant\\n\\nJesse Jayant \\n\\...   \n",
              "1  NameOnResume: jesse_jayant\\n\\n•\\n•\\n•\\n\\nBudge...   \n",
              "2  NameOnResume: jesse_jayant\\n\\n•\\n•\\n• M&A\\n\\nE...   \n",
              "3  NameOnResume: jesse_jayant\\n\\nTracked spending...   \n",
              "4  NameOnResume: jesse_jayant\\n\\n•\\n• As a member...   \n",
              "\n",
              "                                           embedding  \n",
              "0  [-0.02786160260438919, -0.011775648221373558, ...  \n",
              "1  [-0.004550006706267595, -0.01587117835879326, ...  \n",
              "2  [-0.011240021325647831, -0.03403883054852486, ...  \n",
              "3  [-0.014598352834582329, -0.00543337594717741, ...  \n",
              "4  [-0.006679536309093237, -0.014260951429605484,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0fda3ade-02bc-40d3-abb1-d384904e9473\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NameOnResume: jesse_jayant\\n\\nJesse Jayant \\n\\...</td>\n",
              "      <td>[-0.02786160260438919, -0.011775648221373558, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NameOnResume: jesse_jayant\\n\\n•\\n•\\n•\\n\\nBudge...</td>\n",
              "      <td>[-0.004550006706267595, -0.01587117835879326, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NameOnResume: jesse_jayant\\n\\n•\\n•\\n• M&amp;A\\n\\nE...</td>\n",
              "      <td>[-0.011240021325647831, -0.03403883054852486, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NameOnResume: jesse_jayant\\n\\nTracked spending...</td>\n",
              "      <td>[-0.014598352834582329, -0.00543337594717741, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NameOnResume: jesse_jayant\\n\\n•\\n• As a member...</td>\n",
              "      <td>[-0.006679536309093237, -0.014260951429605484,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fda3ade-02bc-40d3-abb1-d384904e9473')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0fda3ade-02bc-40d3-abb1-d384904e9473 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0fda3ade-02bc-40d3-abb1-d384904e9473');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# search documents using query and text embeddings and and retrieve relevant consultant name from resume information using GPT"
      ],
      "metadata": {
        "id": "BMXkZkHWJE_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. TODO Store: Embeddings are saved (for large datasets, use a vector database)\n",
        "1. Search (once per query) - Given a user question, generate an embedding for the query from the OpenAI API\n",
        "1. Using the embeddings, rank the text sections by relevance to the query\n",
        "1. Ask (once per query)\n",
        "  1. Insert the question and the most relevant sections into a message to GPT\n",
        "  1. Return GPT's answer"
      ],
      "metadata": {
        "id": "ZF5Y0pILMG_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial"
      ],
      "metadata": {
        "id": "Qwu5742ONHQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = 'text-ada-001'\n",
        "\n",
        "# #TODO Modify so that it can return and parse more than one response\n",
        "# def extract_chunk(document,template_prompt, model='text-ada-001'):\n",
        "    \n",
        "#     prompt=template_prompt.replace('<document>',document)\n",
        "\n",
        "#     response = openai.Completion.create(\n",
        "#     model=model, \n",
        "#     prompt=prompt,\n",
        "#     temperature=0,\n",
        "#     max_tokens=100,\n",
        "#     top_p=1,\n",
        "#     frequency_penalty=0,\n",
        "#     presence_penalty=0\n",
        "#     )\n",
        "#     return \"1.\" + response['choices'][0]['text']"
      ],
      "metadata": {
        "id": "q7z5Ji5uJOUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### getting items from csv file\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n",
        "\n",
        "# df = pd.read_csv(datafile_path)\n",
        "# df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)"
      ],
      "metadata": {
        "id": "mqAWdIpj1u2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search function\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 3\n",
        ") -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = openai.Embedding.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=query,\n",
        "    )\n",
        "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ],
      "metadata": {
        "id": "DCLYKb9iUpab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_strings_ranked_by_relatedness(query, df, top_n=3):\n",
        "  strings, relatednesses = strings_ranked_by_relatedness(query, df, top_n)\n",
        "  for string, relatedness in zip(strings, relatednesses):\n",
        "      print(f\"{relatedness=:.3f}\")\n",
        "      display(string)"
      ],
      "metadata": {
        "id": "TIhPySTfWR0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"strong in math\"\n",
        "strings_ranked_by_relatedness(query, df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMpxgKyBJOHN",
        "outputId": "1d2282bc-ae12-4ba8-ef27-2e7c4635660f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('NameOnResume: naimal_chisti\\n\\nHarlem Children’s Zone: Program Aide/Teacher: 2nd grade, New York, NY\\n\\nJuly 2022-August 2022\\n\\n● Lead tutor sessions which aided students with their homework\\n● Facilitate activities that would engage students and enhance their creativity\\n● Strategize ways for students to comprehend math problems and reading courses\\n● Taught students from disadvantaged neighborhoods how to use the computer and Google software',\n",
              "  'NameOnResume: seanreed\\n\\n2017 - 2018 \\n\\ntechnology education company. \\n\\n \\nPython + Data Science Meetup, New York, NY \\nData Scientist, Lead Instructor \\n● Grew membership from 200 to over 8,600 technologists by personally preparing and delivering \\n\\n2016 - 2017 \\n\\nconsistent, high-quality academic content. ',\n",
              "  'NameOnResume: jesse_jayant\\n\\nJesse Jayant \\n\\n555-555-5555/ you@post.harvard.edu\\n\\nSummary \\n\\nResults-oriented finance professional with over 10 years of experience in publicly traded and privately held \\nenterprises. Proven track record in complex and capital-intensive global industries, delivering value and innovation in \\nFinance, Strategy, and Corporate Planning. '),\n",
              " (0.7643677315559635, 0.7503989847218114, 0.7497359188275412))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"understands pytorch\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "id": "gCWRzTkWJN_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f442cc-2180-40aa-a58b-c36ac3ebed40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('NameOnResume: seanreed\\n\\n● Built distributed Swin-UNETR encoder-decoder model using Azure ML, PyTorch, Docker, and \\nLightning AI that can pretrain on unlabeled 3D CT scans, eliminating substantial future labeling \\ntime and expense incurred by company radiologists. \\nImplemented computer vision model from innovative academic research paper and completely \\nrefactored model’s existing Python code to solve client’s business problem. \\n\\n● ',\n",
              "  'NameOnResume: seanreed\\n\\n2009 \\n\\n1990 \\n\\nDatabricks, Azure ML, Python, Pandas, Spark, PyTorch, TensorFlow, Keras, Git, Computer Vision, \\nNatural Language Processing, Medical Image Segmentation, Deep Learning, Machine Learning, \\nGLMs, SQL, Linux, Bayesian Statistics, Data Pipelines, GCP, AWS, Docker, Kubernetes, Pandas, \\nNumPy, Random Forests, Gradient Boosting, SVMs, GLMs, Recommender Systems, Graph \\nDatabases, Neo4j',\n",
              "  'NameOnResume: seanreed\\n\\n\\x0cPractical Programming, New York, NY \\nData Scientist, Lead Instructor \\n● Developed and taught SQL, neural networks, machine learning in Spark, and Python curriculum \\nat startup programming school, focusing on CNNs and natural language programming in Keras. \\n● Clearly communicated complicated machine learning concepts to senior executives for growing '),\n",
              " (0.7923564405903968, 0.7847126558380846, 0.7748526793209968))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" python programming\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "id": "Cz-KCpslJN0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3190f9a5-1b23-48ac-ce32-2d1a646e420d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('NameOnResume: seanreed\\n\\nPython model to production environment and usage with Ray, Python, and PySpark on Azure \\nDatabricks. \\n\\n \\nGalvanize, New York, NY \\nSenior Data Scientist \\n● Served as Customer Data Scientist, tasked to identify new markets and clients most likely to \\n\\n2018 - 2021 \\n\\nqualify and benefit from company’s educational program. ',\n",
              "  'NameOnResume: seanreed\\n\\n\\x0cPractical Programming, New York, NY \\nData Scientist, Lead Instructor \\n● Developed and taught SQL, neural networks, machine learning in Spark, and Python curriculum \\nat startup programming school, focusing on CNNs and natural language programming in Keras. \\n● Clearly communicated complicated machine learning concepts to senior executives for growing ',\n",
              "  'NameOnResume: seanreed\\n\\n2009 \\n\\n1990 \\n\\nDatabricks, Azure ML, Python, Pandas, Spark, PyTorch, TensorFlow, Keras, Git, Computer Vision, \\nNatural Language Processing, Medical Image Segmentation, Deep Learning, Machine Learning, \\nGLMs, SQL, Linux, Bayesian Statistics, Data Pipelines, GCP, AWS, Docker, Kubernetes, Pandas, \\nNumPy, Random Forests, Gradient Boosting, SVMs, GLMs, Recommender Systems, Graph \\nDatabases, Neo4j'),\n",
              " (0.7419232856908374, 0.7339036353540598, 0.7335944881274238))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" azure databricks\"\n",
        "strings_ranked_by_relatedness(query, df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN6Gyb11m65U",
        "outputId": "b7ca9c2c-0b9a-4fa2-90c7-2596346fd22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('NameOnResume: seanreed\\n\\n2009 \\n\\n1990 \\n\\nDatabricks, Azure ML, Python, Pandas, Spark, PyTorch, TensorFlow, Keras, Git, Computer Vision, \\nNatural Language Processing, Medical Image Segmentation, Deep Learning, Machine Learning, \\nGLMs, SQL, Linux, Bayesian Statistics, Data Pipelines, GCP, AWS, Docker, Kubernetes, Pandas, \\nNumPy, Random Forests, Gradient Boosting, SVMs, GLMs, Recommender Systems, Graph \\nDatabases, Neo4j',\n",
              "  'NameOnResume: seanreed\\n\\nPython model to production environment and usage with Ray, Python, and PySpark on Azure \\nDatabricks. \\n\\n \\nGalvanize, New York, NY \\nSenior Data Scientist \\n● Served as Customer Data Scientist, tasked to identify new markets and clients most likely to \\n\\n2018 - 2021 \\n\\nqualify and benefit from company’s educational program. ',\n",
              "  'NameOnResume: seanreed\\n\\n● Built distributed Swin-UNETR encoder-decoder model using Azure ML, PyTorch, Docker, and \\nLightning AI that can pretrain on unlabeled 3D CT scans, eliminating substantial future labeling \\ntime and expense incurred by company radiologists. \\nImplemented computer vision model from innovative academic research paper and completely \\nrefactored model’s existing Python code to solve client’s business problem. \\n\\n● '),\n",
              " (0.7519660098818066, 0.7440154296376791, 0.7327313082238651))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" azure AND databricks\"\n",
        "strings_ranked_by_relatedness(query, df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k17cktBSnNuz",
        "outputId": "b53228c7-f4d4-4796-a196-65836f5d9c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('NameOnResume: seanreed\\n\\n2009 \\n\\n1990 \\n\\nDatabricks, Azure ML, Python, Pandas, Spark, PyTorch, TensorFlow, Keras, Git, Computer Vision, \\nNatural Language Processing, Medical Image Segmentation, Deep Learning, Machine Learning, \\nGLMs, SQL, Linux, Bayesian Statistics, Data Pipelines, GCP, AWS, Docker, Kubernetes, Pandas, \\nNumPy, Random Forests, Gradient Boosting, SVMs, GLMs, Recommender Systems, Graph \\nDatabases, Neo4j',\n",
              "  'NameOnResume: seanreed\\n\\n● Built distributed Swin-UNETR encoder-decoder model using Azure ML, PyTorch, Docker, and \\nLightning AI that can pretrain on unlabeled 3D CT scans, eliminating substantial future labeling \\ntime and expense incurred by company radiologists. \\nImplemented computer vision model from innovative academic research paper and completely \\nrefactored model’s existing Python code to solve client’s business problem. \\n\\n● ',\n",
              "  'NameOnResume: seanreed\\n\\nPython model to production environment and usage with Ray, Python, and PySpark on Azure \\nDatabricks. \\n\\n \\nGalvanize, New York, NY \\nSenior Data Scientist \\n● Served as Customer Data Scientist, tasked to identify new markets and clients most likely to \\n\\n2018 - 2021 \\n\\nqualify and benefit from company’s educational program. '),\n",
              " (0.7617906803714984, 0.7449896278855399, 0.7420735098729273))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aside: v2 of search and vector similarity funciton. does not use scipy.spatial"
      ],
      "metadata": {
        "id": "PkhmbBsd0Fk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "\n",
        "# search function\n",
        "def search_resumes(df, query, top_n=10, pprint=False):\n",
        "    query_embedding = get_embedding(\n",
        "        query,\n",
        "        engine=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, query_embedding))\n",
        "\n",
        "    results = (\n",
        "        df.sort_values(\"similarity\", ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    if pprint:\n",
        "        for r in results:\n",
        "            print(r[:200])\n",
        "            print()\n",
        "    return results"
      ],
      "metadata": {
        "id": "jvOOo8yn0FP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"python skills\"\n",
        "search_resumes(df,query, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "E-mA2JLd0FIk",
        "outputId": "ac3d96b4-a6b5-436f-ca31-30d74394a351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 text  \\\n",
              "45  NameOnResume: seanreed\\n\\n\n",
              "Practical Programmi...   \n",
              "46  NameOnResume: seanreed\\n\\n2017 - 2018 \\n\\ntech...   \n",
              "49  NameOnResume: seanreed\\n\\n2009 \\n\\n1990 \\n\\nDa...   \n",
              "\n",
              "                                            embedding  similarity  \n",
              "45  [-0.004207970108836889, 0.005816794466227293, ...    0.774659  \n",
              "46  [0.0005816532066091895, -0.005294321104884148,...    0.773287  \n",
              "49  [-0.0005839008954353631, -0.006026691291481256...    0.772440  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bbbfdbf7-2ca4-4306-a221-aa198c8a7919\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>embedding</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>NameOnResume: seanreed\\n\\n\fPractical Programmi...</td>\n",
              "      <td>[-0.004207970108836889, 0.005816794466227293, ...</td>\n",
              "      <td>0.774659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>NameOnResume: seanreed\\n\\n2017 - 2018 \\n\\ntech...</td>\n",
              "      <td>[0.0005816532066091895, -0.005294321104884148,...</td>\n",
              "      <td>0.773287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>NameOnResume: seanreed\\n\\n2009 \\n\\n1990 \\n\\nDa...</td>\n",
              "      <td>[-0.0005839008954353631, -0.006026691291481256...</td>\n",
              "      <td>0.772440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbbfdbf7-2ca4-4306-a221-aa198c8a7919')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bbbfdbf7-2ca4-4306-a221-aa198c8a7919 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bbbfdbf7-2ca4-4306-a221-aa198c8a7919');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-PNX7KO0DEwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Ask\n",
        "\n",
        "With the search function above, we can now automatically retrieve relevant knowledge and insert it into messages to GPT.\n",
        "\n",
        "Below, we define a function `ask` that:\n",
        "- Takes a user query\n",
        "- Searches for text relevant to the query\n",
        "- Stuffs that text into a message for GPT\n",
        "- Sends the message to GPT\n",
        "- Returns GPT's answer"
      ],
      "metadata": {
        "id": "MvV6e7bZno4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = 'gpt-3.5-turbo'\n",
        "def num_tokens(text: str, model: str = 'gpt-3.5-turbo') -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "#using v1 search function\n",
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str,\n",
        "    token_budget: int\n",
        ") -> str:\n",
        "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
        "    introduction = ' You are a Human Resources agent looking for skills in resumes'\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "    message = introduction\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nresume section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (\n",
        "            num_tokens(message + next_article + question, model=model)\n",
        "            > token_budget\n",
        "        ):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "@logger.catch\n",
        "def ask(\n",
        "    query: str,\n",
        "    df: pd.DataFrame = df,\n",
        "    model: str = GPT_MODEL,\n",
        "    token_budget: int = 4096 - 500,\n",
        "    print_message: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    logger.debug(f\"{message}\")\n",
        "    content = \"Construct a list of NameOnResume fields from the documents given. Remove all duplicates from the list\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": content},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return response_message\n",
        "\n"
      ],
      "metadata": {
        "id": "HwIQfHT5ncvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask('who knows law')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "H4_3qNXCpLhV",
        "outputId": "bb68d2c6-74c2-46a4-8f24-0dc3c4a43397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-05-08 19:50:15.430\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1m You are a Human Resources agent looking for skills in resumes\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "NameOnResume: naimal_chisti\n",
            "\n",
            "Changemaker Certificate: Fordham Social Innovation Summer Experience: Systems Thinking (2020) and Fordham Business\n",
            "Development Collaboratory (2020), Global Outreach Certificate Colombia (2022), Matteo Ricci Seminar Alumni (2021),\n",
            "Dean's List (2021-2022)\n",
            "EXPERIENCE\n",
            "Fordham Law School Development of Law Office/Alumni Relations:\n",
            "Student Worker, New York, NY\n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "NameOnResume: naimal_chisti\n",
            "\n",
            "Near star, Guiding question, and Framing the question in mind\n",
            "\n",
            "● Meditated on the fundamental forces that impact how the system in Yemen works (including influential figures,\n",
            "\n",
            "events, norms/beliefs, institutions, laws/policies, etc.)\n",
            "\n",
            "Fordham Business Development Collaboratory: Financial research participant, New York, NY    June 2020-August 2020\n",
            "\"\"\"\n",
            "\n",
            "resume section:\n",
            "\"\"\"\n",
            "NameOnResume: naimal_chisti\n",
            "\n",
            "● Studied cases of reparations, mass incarceration, police brutality, etc.\n",
            "● Discussed and wrote essays regarding Ta-Nehisi Coates's essay, The Case for Reparations, The New Jim Crow:\n",
            "\n",
            "Mass Incarceration in the Age of Colorblindness, by Michelle Alexander, and Caste: The Origins of Our\n",
            "Discontents, by Isabel Wilkerson\n",
            "\"\"\"\n",
            "\n",
            "Question: who knows law\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The NameOnResume field in all three resumes is \"naimal_chisti\". Therefore, the list of NameOnResume fields is [\"naimal_chisti\"]. There are no duplicates to remove.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Gvwt0TZpS-R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}