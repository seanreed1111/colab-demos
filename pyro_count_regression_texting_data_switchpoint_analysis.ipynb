{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agI4ToZPmQc_"
      },
      "source": [
        "source: https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_Pyro.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVP7xmlom-Kr",
        "outputId": "eae1ee6e-028f-4332-9fdf-ce6c02d62c54"
      },
      "outputs": [],
      "source": [
        "pip install pyro-ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVthELNLlX1u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import torch\n",
        "from torch.distributions import constraints\n",
        "from torch import tensor\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI,Trace_ELBO\n",
        "from pyro.infer import MCMC, NUTS, HMC\n",
        "from pyro.infer.autoguide  import AutoMultivariateNormal, AutoNormal, init_to_mean\n",
        "from pyro.optim import ClippedAdam\n",
        "\n",
        "assert pyro.__version__.startswith('1.8')\n",
        "pyro.set_rng_seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Set matplotlib settings\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = [15, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Mimm-QWkG4zL",
        "outputId": "c8238540-9a9b-40bb-f24f-02334a0843f3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/data/txtdata.csv\", header=None)\n",
        "df.columns = ['messages_received']\n",
        "df.loc[:,'day'] = df.index +1\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "hep8Jn2hdEpa",
        "outputId": "b869b54f-bee3-42a4-b21d-73879cee6fff"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "HPgRzxnJnmai",
        "outputId": "91a8f577-b90a-4e41-f142-a83791a31b1b"
      },
      "outputs": [],
      "source": [
        "sns.displot(x = df['messages_received']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HLowq6RsZ75"
      },
      "source": [
        "You are given a series of daily text-message counts from a user of your system. The data, plotted over time, appears in the chart below. How would you model this data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "hTn8Ope0oupO",
        "outputId": "ea475a72-473a-4450-dcd2-630e395d3b0a"
      },
      "outputs": [],
      "source": [
        "sns.barplot(y='messages_received', x='day', data=df, palette='colorblind')\n",
        "plt.xticks(fontsize=8);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzNsRFNu3oDk"
      },
      "outputs": [],
      "source": [
        "X = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrnKvqrQJoO9",
        "outputId": "d7daa9c9-5a8b-4253-dfde-04e95594ba44"
      },
      "outputs": [],
      "source": [
        "X['messages_received'].mean(), X['messages_received'].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfcZyuzbci8r"
      },
      "outputs": [],
      "source": [
        "# Let's look at an actual poisson distribution fit to the mean of the data\n",
        "samples = dist.Poisson(X['messages_received'].mean()).sample(tensor([74])) # create sample distribution where Poisson mean = data mean\n",
        "sample_df = pd.DataFrame({'sample_texts_received':samples.numpy(), 'day':X['day'].copy()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "cOX166O-dnYN",
        "outputId": "f990d8b2-1dd4-4635-f8e3-31a3167e12d2"
      },
      "outputs": [],
      "source": [
        "sns.displot(x = sample_df['sample_texts_received']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gZzNA2DSaaW"
      },
      "source": [
        "### So, comparing the actual data with the above two poisson sampling distributions above, it appears our data is overdispersed. Our data is heavily skewed to the right, ie., it has many more texts received far away from the mean, and much greater variability spikes much bigger than a poisson distribution would suggest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zywWoOx7xFk2"
      },
      "source": [
        "### Despite the fact that we know the poisson is not the 'correct' distribution to handle the outliers here, let's continue with the analysis now, and we can try other distributions in future work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WexhpjMOopGp"
      },
      "outputs": [],
      "source": [
        "data = tensor(X['messages_received'].values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPrUKt-RA5sQ",
        "outputId": "75c840ca-d195-4c94-8410-c3dc722399f6"
      },
      "outputs": [],
      "source": [
        "data.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XY-snfZCKM8"
      },
      "outputs": [],
      "source": [
        "# this is a trick to marginalize over all 74 days of the discrete variable\n",
        "def model(data):\n",
        "    alpha = 1.0 / data.mean()\n",
        "    lambda_1 = pyro.sample(\"lambda_1\", dist.Exponential(alpha))\n",
        "    lambda_2 = pyro.sample(\"lambda_2\", dist.Exponential(alpha))\n",
        "    \n",
        "    tau = pyro.sample(\"tau\", dist.Uniform(0, 1))\n",
        "    lambda1_size = (tau * data.size(0) + 1).long()\n",
        "    lambda2_size = data.size(0) - lambda1_size\n",
        "    lambda_ = torch.cat([lambda_1.expand((lambda1_size,)),\n",
        "                         lambda_2.expand((lambda2_size,))])\n",
        "\n",
        "    with pyro.plate(\"data\", data.size(0)):\n",
        "        pyro.sample(\"obs\", dist.Poisson(lambda_), obs=data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHi8kA5IjnJP"
      },
      "outputs": [],
      "source": [
        "from pyro.infer.autoguide import AutoMultivariateNormal\n",
        "\n",
        "guide = AutoMultivariateNormal(model, init_loc_fn=init_to_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD5qX-XqjnFe"
      },
      "outputs": [],
      "source": [
        "def train(model, guide, lr=0.01, n_steps=4000):\n",
        "    pyro.set_rng_seed(1)\n",
        "    pyro.clear_param_store()\n",
        "    \n",
        "    gamma = 0.01  # final learning rate will be gamma * initial_lr\n",
        "    lrd = gamma ** (1 / n_steps)\n",
        "    adam = pyro.optim.ClippedAdam({'lr': lr, 'lrd': lrd})\n",
        "\n",
        "    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n",
        "\n",
        "    for i in range(n_steps):\n",
        "        elbo = svi.step(data)\n",
        "        if i % 500 == 0:\n",
        "          print(f\"Elbo loss: {elbo}\")\n",
        "    print(f\"Final Elbo loss: {elbo}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksAgS-PSjm99",
        "outputId": "8b049bb2-30d7-459a-caab-7eb9c275bc4d"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train(model, guide)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9yCC7NWlpp6"
      },
      "outputs": [],
      "source": [
        "from pyro.infer import Predictive\n",
        "\n",
        "num_samples = 1000\n",
        "predictive = Predictive(model, guide=guide, num_samples=num_samples)\n",
        "\n",
        "svi_samples = {k: v.reshape((num_samples,-1)).detach().cpu().numpy()\n",
        "               for k, v in predictive(data).items()\n",
        "               if k != \"obs\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFy-1qjzlpjb",
        "outputId": "51852880-b135-47c1-c28d-ac02b2bcbfad"
      },
      "outputs": [],
      "source": [
        "svi_samples.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2YvlMdvlpee",
        "outputId": "6fb8e0d7-37ee-4887-f275-fc5b064280b5"
      },
      "outputs": [],
      "source": [
        "guide.quantiles([0.05,0.50,0.95])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1-96uEioiT6"
      },
      "source": [
        "Can see that SVI does not work in this instance, lambda_1 and lambda_2 overlap substantially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B163uuzMoy9f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOV2fXNoogOx"
      },
      "outputs": [],
      "source": [
        "## Let's do MCMC instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHSjIG1ikwHT",
        "outputId": "b3570695-d1c5-4ca4-fb88-721621f7da5a"
      },
      "outputs": [],
      "source": [
        "kernel = NUTS(model, jit_compile=True, ignore_jit_warnings=True, max_tree_depth=3)\n",
        "posterior = MCMC(kernel, num_samples=5000, warmup_steps=500)\n",
        "posterior.run(data);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GUdrzFokwD1"
      },
      "outputs": [],
      "source": [
        "hmc_samples = {k: v.detach().cpu().numpy() for k, v in posterior.get_samples().items()}\n",
        "lambda_1_samples = hmc_samples['lambda_1']\n",
        "lambda_2_samples = hmc_samples['lambda_2']\n",
        "tau_samples = (hmc_samples['tau'] * data.size(0) + 1).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbT28ur7mFLK",
        "outputId": "2be0730c-bcdf-49c1-f31d-61bb322e83b2"
      },
      "outputs": [],
      "source": [
        "data.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "Uf4XX4pwkwAG",
        "outputId": "2e69fd51-d1c0-41e6-cefb-834f2515508f"
      },
      "outputs": [],
      "source": [
        "# plt.figsize(12.5, 10)\n",
        "#histogram of the samples:\n",
        "\n",
        "ax = plt.subplot(311)\n",
        "ax.set_autoscaley_on(False)\n",
        "\n",
        "plt.hist(lambda_1_samples, histtype='stepfilled', bins=30, alpha=0.85,\n",
        "         label=\"posterior of $\\lambda_1$\", color=\"#A60628\", density=True)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.title(r\"\"\"Posterior distributions of the variables\n",
        "    $\\lambda_1,\\;\\lambda_2,\\;\\tau$\"\"\")\n",
        "plt.xlim([15, 30])\n",
        "plt.xlabel(\"$\\lambda_1$ value\")\n",
        "\n",
        "ax = plt.subplot(312)\n",
        "ax.set_autoscaley_on(False)\n",
        "plt.hist(lambda_2_samples, histtype='stepfilled', bins=30, alpha=0.85,\n",
        "         label=\"posterior of $\\lambda_2$\", color=\"#7A68A6\", density=True)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlim([15, 30])\n",
        "plt.xlabel(\"$\\lambda_2$ value\")\n",
        "\n",
        "plt.subplot(313)\n",
        "w = 1.0 / tau_samples.shape[0] * np.ones_like(tau_samples)\n",
        "plt.hist(tau_samples, bins=data.size(0), alpha=1,\n",
        "         label=r\"posterior of $\\tau$\",\n",
        "         color=\"#467821\", weights=w, rwidth=2.)\n",
        "plt.xticks(np.arange(data.size(0)))\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.ylim([0, .75])\n",
        "# plt.xlim([35, len(data.size(0))-20])\n",
        "plt.xlabel(r\"$\\tau$ (in days)\")\n",
        "plt.ylabel(\"probability\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtOM99Vkmpd4"
      },
      "source": [
        "### Interpretation\n",
        "Recall that Bayesian methodology returns a distribution. Hence we now have distributions to describe the unknown s and . What have we gained? Immediately, we can see the uncertainty in our estimates: the wider the distribution, the less certain our posterior belief should be. We can also see what the plausible values for the parameters are: \n",
        " is around 18 and \n",
        " is around 23. The posterior distributions of the two s are clearly distinct, indicating that it is indeed likely that there was a change in the user's text-message behaviour.\n",
        "\n",
        "What other observations can you make? If you look at the original data again, do these results seem reasonable?\n",
        "\n",
        "Notice also that the posterior distributions for the s do not look like exponential distributions, even though our priors for these variables were exponential. In fact, the posterior distributions are not really of any form that we recognize from the original model. But that's OK! This is one of the benefits of taking a computational point of view. If we had instead done this analysis using mathematical approaches, we would have been stuck with an analytically intractable (and messy) distribution. Our use of a computational approach makes us indifferent to mathematical tractability.\n",
        "\n",
        "Our analysis also returned a distribution for . Its posterior distribution looks a little different from the other two because it is a discrete random variable, so it doesn't assign probabilities to intervals. We can see that near day 45, there was a 50% chance that the user's behaviour changed. Had no change occurred, or had the change been gradual over time, the posterior distribution of  would have been more spread out, reflecting that many days were plausible candidates for . By contrast, in the actual results we see that only three or four days make any sense as potential transition points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvrL_UQto9Bm"
      },
      "outputs": [],
      "source": [
        "# The switchpoint analysis done above would be considerably more difficult to do without Bayesian calculations"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w3dU4UT4Ymhk"
      ],
      "name": "pyro-count-regression-texting-data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
