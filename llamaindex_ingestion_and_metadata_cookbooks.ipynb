{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZCJR35quwQTpK68l22f3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c7d8045f03c46ffaa8765c94b4869d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_042b585ff9a14c898e9b2bedd2c4d683",
              "IPY_MODEL_60c4b511fe834196acac931f1ee54f67",
              "IPY_MODEL_0fb964c801f24ea4b498a98c9e6bd76f"
            ],
            "layout": "IPY_MODEL_6f7ac72aac864051975e676b691b9ac9"
          }
        },
        "042b585ff9a14c898e9b2bedd2c4d683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98d48d53a3d845d1b935ebb8053bd2e5",
            "placeholder": "​",
            "style": "IPY_MODEL_385b64f888b6441bb6a99945b2aef2f9",
            "value": "Parsing nodes: 100%"
          }
        },
        "60c4b511fe834196acac931f1ee54f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f87aaefa274a53ace2d5fe5dc350c1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9dcfd07f6a884282a60a5d9efcf9fdd8",
            "value": 1
          }
        },
        "0fb964c801f24ea4b498a98c9e6bd76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4911987bc2834b44bcf785a0c4952887",
            "placeholder": "​",
            "style": "IPY_MODEL_b3974eb774924a1388792b13e7395015",
            "value": " 1/1 [00:00&lt;00:00,  3.17it/s]"
          }
        },
        "6f7ac72aac864051975e676b691b9ac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d48d53a3d845d1b935ebb8053bd2e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "385b64f888b6441bb6a99945b2aef2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4f87aaefa274a53ace2d5fe5dc350c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dcfd07f6a884282a60a5d9efcf9fdd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4911987bc2834b44bcf785a0c4952887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3974eb774924a1388792b13e7395015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanreed1111/colab-demos/blob/master/llamaindex_ingestion_and_metadata_cookbooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEwtuhWo5mc2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# - https://docs.llamaindex.ai/en/stable/examples/cookbooks/oreilly_course_cookbooks/Module-4/Ingestion_Pipeline/\n",
        "# - https://docs.llamaindex.ai/en/stable/examples/cookbooks/oreilly_course_cookbooks/Module-4/Metadata_Extraction/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq llama-index llama-index-vector-stores-qdrant llama-index-readers-file llama-index-readers-web"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU6g5Z5l53zY",
        "outputId": "9dc87ee6-9423-42fb-b88a-33275d44b1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Readers from LlamaHub\n",
        "- https://llamahub.ai/l/readers/llama-index-readers-file?from="
      ],
      "metadata": {
        "id": "P8U40gSn4dWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.readers.file import (\n",
        "    DocxReader,\n",
        "    HWPReader,\n",
        "    PDFReader,\n",
        "    EpubReader,\n",
        "    FlatReader,\n",
        "    HTMLTagReader,\n",
        "    ImageCaptionReader,\n",
        "    ImageReader,\n",
        "    ImageVisionLLMReader,\n",
        "    IPYNBReader,\n",
        "    MarkdownReader,\n",
        "    MboxReader,\n",
        "    PptxReader,\n",
        "    PandasCSVReader,\n",
        "    VideoAudioReader,\n",
        "    UnstructuredReader,\n",
        "    PyMuPDFReader,\n",
        "    ImageTabularChartReader,\n",
        "    XMLReader,\n",
        "    PagedCSVReader,\n",
        "    CSVReader,\n",
        "    RTFReader,\n",
        ")\n",
        "\n",
        "# PDF Reader with `SimpleDirectoryReader`\n",
        "parser = PDFReader()\n",
        "file_extractor = {\".pdf\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# Docx Reader example\n",
        "parser = DocxReader()\n",
        "file_extractor = {\".docx\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# HWP Reader example\n",
        "parser = HWPReader()\n",
        "file_extractor = {\".hwp\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# Epub Reader example\n",
        "parser = EpubReader()\n",
        "file_extractor = {\".epub\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# Flat Reader example\n",
        "parser = FlatReader()\n",
        "file_extractor = {\".txt\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# HTML Tag Reader example\n",
        "parser = HTMLTagReader()\n",
        "file_extractor = {\".html\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# Image Reader example\n",
        "parser = ImageReader()\n",
        "file_extractor = {\n",
        "    \".jpg\": parser,\n",
        "    \".jpeg\": parser,\n",
        "    \".png\": parser,\n",
        "}  # Add other image formats as needed\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# IPYNB Reader example\n",
        "parser = IPYNBReader()\n",
        "file_extractor = {\".ipynb\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# Markdown Reader example\n",
        "parser = MarkdownReader()\n",
        "file_extractor = {\".md\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# Mbox Reader example\n",
        "parser = MboxReader()\n",
        "file_extractor = {\".mbox\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# Pptx Reader example\n",
        "parser = PptxReader()\n",
        "file_extractor = {\".pptx\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "\n",
        "# Pandas CSV Reader example\n",
        "parser = PandasCSVReader()\n",
        "file_extractor = {\".csv\": parser}  # Add other CSV formats as needed\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# PyMuPDF Reader example\n",
        "parser = PyMuPDFReader()\n",
        "file_extractor = {\".pdf\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# XML Reader example\n",
        "parser = XMLReader()\n",
        "file_extractor = {\".xml\": parser}\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# Paged CSV Reader example\n",
        "parser = PagedCSVReader()\n",
        "file_extractor = {\".csv\": parser}  # Add other CSV formats as needed\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# CSV Reader example\n",
        "parser = CSVReader()\n",
        "file_extractor = {\".csv\": parser}  # Add other CSV formats as needed\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\", file_extractor=file_extractor\n",
        ").load_data()"
      ],
      "metadata": {
        "id": "Glu7dSuH4vIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        TokenTextSplitter(chunk_size=1024, chunk_overlap=100),\n",
        "    ]\n",
        ")\n",
        "nodes = pipeline.run(documents=documents);nodes[0].metadata"
      ],
      "metadata": {
        "id": "XpqUwHQi9Z9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Readers from LlamaHub\n",
        "- pip install llama-index-readers-web"
      ],
      "metadata": {
        "id": "lrjIF2wV5MWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.web import AsyncWebPageReader\n",
        "from llama_index.readers.web import BeautifulSoupWebReader\n",
        "from llama_index.readers.web import BrowserbaseWebReader\n",
        "from llama_index.readers.web import FireCrawlWebReader\n",
        "from llama_index.readers.web import KnowledgeBaseWebReader\n",
        "from llama_index.readers.web import MainContentExtractorReader\n",
        "from llama_index.readers.web import NewsArticleReader\n",
        "from llama_index.readers.web import ReadabilityWebPageReader\n",
        "from llama_index.readers.web import RssNewsReader\n",
        "from llama_index.readers.web import RssReader\n",
        "from llama_index.readers.web import ScrapflyReader\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from llama_index.readers.web import SitemapReader\n",
        "# from llama_index.readers.web import SpiderReader #does not work\n",
        "from llama_index.readers.web import TrafilaturaWebReader\n",
        "from llama_index.readers.web import UnstructuredURLLoader\n",
        "from llama_index.readers.web import WholeSiteReader"
      ],
      "metadata": {
        "id": "fAZ8gsB25LwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleWebReader"
      ],
      "metadata": {
        "id": "E9y4P8sH7Jj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader #expects URL to scrape not filepath\n",
        "from IPython.display import Markdown, display\n",
        "import os"
      ],
      "metadata": {
        "id": "iqqR9vpe7NAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3xukOR571jC",
        "outputId": "8457014e-cc53-4489-d322-5839a3324604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__func__',\n",
              " '__ge__',\n",
              " '__get__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__self__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"http://paulgraham.com/worked.html\"]\n",
        ")\n",
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI_nMx8h7JIb",
        "outputId": "364a3f7b-f963-4c3a-e886-3d89f0e58721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='http://paulgraham.com/worked.html', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='![](https://s.turbifycdn.com/aah/paulgraham/essays-5.gif)|\\n![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif)|\\n[![](https://s.turbifycdn.com/aah/paulgraham/essays-6.gif)](index.html)  \\n  \\n| ![What I Worked On](https://s.turbifycdn.com/aah/paulgraham/what-i-worked-\\non-4.gif)  \\n  \\nFebruary 2021  \\n  \\nBefore college the two main things I worked on, outside of school, were\\nwriting and programming. I didn\\'t write essays. I wrote what beginning writers\\nwere supposed to write then, and probably still are: short stories. My stories\\nwere awful. They had hardly any plot, just characters with strong feelings,\\nwhich I imagined made them deep.  \\n  \\nThe first programs I tried writing were on the IBM 1401 that our school\\ndistrict used for what was then called \"data processing.\" This was in 9th\\ngrade, so I was 13 or 14. The school district\\'s 1401 happened to be in the\\nbasement of our junior high school, and my friend Rich Draves and I got\\npermission to use it. It was like a mini Bond villain\\'s lair down there, with\\nall these alien-looking machines \\x97 CPU, disk drives, printer, card reader \\x97\\nsitting up on a raised floor under bright fluorescent lights.  \\n  \\nThe language we used was an early version of Fortran. You had to type programs\\non punch cards, then stack them in the card reader and press a button to load\\nthe program into memory and run it. The result would ordinarily be to print\\nsomething on the spectacularly loud printer.  \\n  \\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in\\nretrospect there\\'s not much I could have done with it. The only form of input\\nto programs was data stored on punched cards, and I didn\\'t have any data\\nstored on punched cards. The only other option was to do things that didn\\'t\\nrely on any input, like calculate approximations of pi, but I didn\\'t know\\nenough math to do anything interesting of that type. So I\\'m not surprised I\\ncan\\'t remember any programs I wrote, because they can\\'t have done much. My\\nclearest memory is of the moment I learned it was possible for programs not to\\nterminate, when one of mine didn\\'t. On a machine without time-sharing, this\\nwas a social as well as a technical error, as the data center manager\\'s\\nexpression made clear.  \\n  \\nWith microcomputers, everything changed. Now you could have a computer sitting\\nright in front of you, on a desk, that could respond to your keystrokes as it\\nwas running instead of just churning through a stack of punch cards and then\\nstopping. [1]  \\n  \\nThe first of my friends to get a microcomputer built it himself. It was sold\\nas a kit by Heathkit. I remember vividly how impressed and envious I felt\\nwatching him sitting in front of it, typing programs right into the computer.  \\n  \\nComputers were expensive in those days and it took me years of nagging before\\nI convinced my father to buy one, a TRS-80, in about 1980\\\\. The gold standard\\nthen was the Apple II, but a TRS-80 was good enough. This was when I really\\nstarted programming. I wrote simple games, a program to predict how high my\\nmodel rockets would fly, and a word processor that my father used to write at\\nleast one book. There was only room in memory for about 2 pages of text, so\\nhe\\'d write 2 pages at a time and then print them out, but it was a lot better\\nthan a typewriter.  \\n  \\nThough I liked programming, I didn\\'t plan to study it in college. In college I\\nwas going to study philosophy, which sounded much more powerful. It seemed, to\\nmy naive high school self, to be the study of the ultimate truths, compared to\\nwhich the things studied in other fields would be mere domain knowledge. What\\nI discovered when I got to college was that the other fields took up so much\\nof the space of ideas that there wasn\\'t much left for these supposed ultimate\\ntruths. All that seemed left for philosophy were edge cases that people in\\nother fields felt could safely be ignored.  \\n  \\nI couldn\\'t have put this into words when I was 18. All I knew at the time was\\nthat I kept taking philosophy courses and they kept being boring. So I decided\\nto switch to AI.  \\n  \\nAI was in the air in the mid 1980s, but there were two things especially that\\nmade me want to work on it: a novel by Heinlein called _The Moon is a Harsh\\nMistress_ , which featured an intelligent computer called Mike, and a PBS\\ndocumentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading\\n_The Moon is a Harsh Mistress_ , so I don\\'t know how well it has aged, but\\nwhen I read it I was drawn entirely into its world. It seemed only a matter of\\ntime before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed\\nlike that time would be a few years at most. All you had to do was teach\\nSHRDLU more words.  \\n  \\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so\\nI started trying to teach myself. Which meant learning Lisp, since in those\\ndays Lisp was regarded as the language of AI. The commonly used programming\\nlanguages then were pretty primitive, and programmers\\' ideas correspondingly\\nso. The default language at Cornell was a Pascal-like language called PL/I,\\nand the situation was similar elsewhere. Learning Lisp expanded my concept of\\na program so fast that it was years before I started to have a sense of where\\nthe new limits were. This was more like it; this was what I had expected\\ncollege to do. It wasn\\'t happening in a class, like it was supposed to, but\\nthat was ok. For the next couple years I was on a roll. I knew what I was\\ngoing to do.  \\n  \\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love\\nworking on that program. It was a pleasing bit of code, but what made it even\\nmore exciting was my belief \\x97 hard to imagine now, but not unique in 1985 \\x97\\nthat it was already climbing the lower slopes of intelligence.  \\n  \\nI had gotten into a program at Cornell that didn\\'t make you choose a major.\\nYou could take whatever classes you liked, and choose whatever you liked to\\nput on your degree. I of course chose \"Artificial Intelligence.\" When I got\\nthe actual physical diploma, I was dismayed to find that the quotes had been\\nincluded, which made them read as scare-quotes. At the time this bothered me,\\nbut now it seems amusingly accurate, for reasons I was about to discover.  \\n  \\nI applied to 3 grad schools: MIT and Yale, which were renowned for AI at the\\ntime, and Harvard, which I\\'d visited because Rich Draves went there, and was\\nalso home to Bill Woods, who\\'d invented the type of parser I used in my SHRDLU\\nclone. Only Harvard accepted me, so that was where I went.  \\n  \\nI don\\'t remember the moment it happened, or if there even was a specific\\nmoment, but during the first year of grad school I realized that AI, as\\npracticed at the time, was a hoax. By which I mean the sort of AI in which a\\nprogram that\\'s told \"the dog is sitting on the chair\" translates this into\\nsome formal representation and adds it to the list of things it knows.  \\n  \\nWhat these programs really showed was that there\\'s a subset of natural\\nlanguage that\\'s a formal language. But a very proper subset. It was clear that\\nthere was an unbridgeable gap between what they could do and actually\\nunderstanding natural language. It was not, in fact, simply a matter of\\nteaching SHRDLU more words. That whole way of doing AI, with explicit data\\nstructures representing concepts, was not going to work. Its brokenness did,\\nas so often happens, generate a lot of opportunities to write papers about\\nvarious band-aids that could be applied to it, but it was never going to get\\nus Mike.  \\n  \\nSo I looked around to see what I could salvage from the wreckage of my plans,\\nand there was Lisp. I knew from experience that Lisp was interesting for its\\nown sake and not just for its association with AI, even though that was the\\nmain reason people cared about it at the time. So I decided to focus on Lisp.\\nIn fact, I decided to write a book about Lisp hacking. It\\'s scary to think how\\nlittle I knew about Lisp hacking when I started writing that book. But there\\'s\\nnothing like writing a book about something to help you learn it. The book,\\n_On Lisp_ , wasn\\'t published till 1993, but I wrote much of it in grad school.  \\n  \\nComputer Science is an uneasy alliance between two halves, theory and systems.\\nThe theory people prove things, and the systems people build things. I wanted\\nto build things. I had plenty of respect for theory \\x97 indeed, a sneaking\\nsuspicion that it was the more admirable of the two halves \\x97 but building\\nthings seemed so much more exciting.  \\n  \\nThe problem with systems work, though, was that it didn\\'t last. Any program\\nyou wrote today, no matter how good, would be obsolete in a couple decades at\\nbest. People might mention your software in footnotes, but no one would\\nactually use it. And indeed, it would seem very feeble work. Only people with\\na sense of the history of the field would even realize that, in its time, it\\nhad been good.  \\n  \\nThere were some surplus Xerox Dandelions floating around the computer lab at\\none point. Anyone who wanted one to play around with could have one. I was\\nbriefly tempted, but they were so slow by present standards; what was the\\npoint? No one else wanted one either, so off they went. That was what happened\\nto systems work.  \\n  \\nI wanted not just to build things, but to build things that would last.  \\n  \\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where\\nhe was in grad school. One day I went to visit the Carnegie Institute, where\\nI\\'d spent a lot of time as a kid. While looking at a painting there I realized\\nsomething that might seem obvious, but was a big surprise to me. There, right\\non the wall, was something you could make that would last. Paintings didn\\'t\\nbecome obsolete. Some of the best ones were hundreds of years old.  \\n  \\nAnd moreover this was something you could make a living doing. Not as easily\\nas you could by writing software, of course, but I thought if you were really\\nindustrious and lived really cheaply, it had to be possible to make enough to\\nsurvive. And as an artist you could be truly independent. You wouldn\\'t have a\\nboss, or even need to get research funding.  \\n  \\nI had always liked looking at paintings. Could I make them? I had no idea. I\\'d\\nnever imagined it was even possible. I knew intellectually that people made\\nart \\x97 that it didn\\'t just appear spontaneously \\x97 but it was as if the people\\nwho made it were a different species. They either lived long ago or were\\nmysterious geniuses doing strange things in profiles in _Life_ magazine. The\\nidea of actually being able to make art, to put that verb before that noun,\\nseemed almost miraculous.  \\n  \\nThat fall I started taking art classes at Harvard. Grad students could take\\nclasses in any department, and my advisor, Tom Cheatham, was very easy going.\\nIf he even knew about the strange classes I was taking, he never said\\nanything.  \\n  \\nSo now I was in a PhD program in computer science, yet planning to be an\\nartist, yet also genuinely in love with Lisp hacking and working away at _On\\nLisp_. In other words, like many a grad student, I was working energetically\\non multiple projects that were not my thesis.  \\n  \\nI didn\\'t see a way out of this situation. I didn\\'t want to drop out of grad\\nschool, but how else was I going to get out? I remember when my friend Robert\\nMorris got kicked out of Cornell for writing the internet worm of 1988, I was\\nenvious that he\\'d found such a spectacular way to get out of grad school.  \\n  \\nThen one day in April 1990 a crack appeared in the wall. I ran into professor\\nCheatham and he asked if I was far enough along to graduate that June. I\\ndidn\\'t have a word of my dissertation written, but in what must have been the\\nquickest bit of thinking in my life, I decided to take a shot at writing one\\nin the 5 weeks or so that remained before the deadline, reusing parts of _On\\nLisp_ where I could, and I was able to respond, with no perceptible delay\\n\"Yes, I think so. I\\'ll give you something to read in a few days.\"  \\n  \\nI picked applications of continuations as the topic. In retrospect I should\\nhave written about macros and embedded languages. There\\'s a whole world there\\nthat\\'s barely been explored. But all I wanted was to get out of grad school,\\nand my rapidly written dissertation sufficed, just barely.  \\n  \\nMeanwhile I was applying to art schools. I applied to two: RISD in the US, and\\nthe Accademia di Belli Arti in Florence, which, because it was the oldest art\\nschool, I imagined would be good. RISD accepted me, and I never heard back\\nfrom the Accademia, so off to Providence I went.  \\n  \\nI\\'d applied for the BFA program at RISD, which meant in effect that I had to\\ngo to college again. This was not as strange as it sounds, because I was only\\n25, and art schools are full of people of different ages. RISD counted me as a\\ntransfer sophomore and said I had to do the foundation that summer. The\\nfoundation means the classes that everyone has to take in fundamental subjects\\nlike drawing, color, and design.  \\n  \\nToward the end of the summer I got a big surprise: a letter from the\\nAccademia, which had been delayed because they\\'d sent it to Cambridge England\\ninstead of Cambridge Massachusetts, inviting me to take the entrance exam in\\nFlorence that fall. This was now only weeks away. My nice landlady let me\\nleave my stuff in her attic. I had some money saved from consulting work I\\'d\\ndone in grad school; there was probably enough to last a year if I lived\\ncheaply. Now all I had to do was learn Italian.  \\n  \\nOnly _stranieri_ (foreigners) had to take this entrance exam. In retrospect it\\nmay well have been a way of excluding them, because there were so many\\n_stranieri_ attracted by the idea of studying art in Florence that the Italian\\nstudents would otherwise have been outnumbered. I was in decent shape at\\npainting and drawing from the RISD foundation that summer, but I still don\\'t\\nknow how I managed to pass the written exam. I remember that I answered the\\nessay question by writing about Cezanne, and that I cranked up the\\nintellectual level as high as I could to make the most of my limited\\nvocabulary. [2]  \\n  \\nI\\'m only up to age 25 and already there are such conspicuous patterns. Here I\\nwas, yet again about to attend some august institution in the hopes of\\nlearning about some prestigious subject, and yet again about to be\\ndisappointed. The students and faculty in the painting department at the\\nAccademia were the nicest people you could imagine, but they had long since\\narrived at an arrangement whereby the students wouldn\\'t require the faculty to\\nteach anything, and in return the faculty wouldn\\'t require the students to\\nlearn anything. And at the same time all involved would adhere outwardly to\\nthe conventions of a 19th century atelier. We actually had one of those little\\nstoves, fed with kindling, that you see in 19th century studio paintings, and\\na nude model sitting as close to it as possible without getting burned. Except\\nhardly anyone else painted her besides me. The rest of the students spent\\ntheir time chatting or occasionally trying to imitate things they\\'d seen in\\nAmerican art magazines.  \\n  \\nOur model turned out to live just down the street from me. She made a living\\nfrom a combination of modelling and making fakes for a local antique dealer.\\nShe\\'d copy an obscure old painting out of a book, and then he\\'d take the copy\\nand maltreat it to make it look old. [3]  \\n  \\nWhile I was a student at the Accademia I started painting still lives in my\\nbedroom at night. These paintings were tiny, because the room was, and because\\nI painted them on leftover scraps of canvas, which was all I could afford at\\nthe time. Painting still lives is different from painting people, because the\\nsubject, as its name suggests, can\\'t move. People can\\'t sit for more than\\nabout 15 minutes at a time, and when they do they don\\'t sit very still. So the\\ntraditional m.o. for painting people is to know how to paint a generic person,\\nwhich you then modify to match the specific person you\\'re painting. Whereas a\\nstill life you can, if you want, copy pixel by pixel from what you\\'re seeing.\\nYou don\\'t want to stop there, of course, or you get merely photographic\\naccuracy, and what makes a still life interesting is that it\\'s been through a\\nhead. You want to emphasize the visual cues that tell you, for example, that\\nthe reason the color changes suddenly at a certain point is that it\\'s the edge\\nof an object. By subtly emphasizing such things you can make paintings that\\nare more realistic than photographs not just in some metaphorical sense, but\\nin the strict information-theoretic sense. [4]  \\n  \\nI liked painting still lives because I was curious about what I was seeing. In\\neveryday life, we aren\\'t consciously aware of much we\\'re seeing. Most visual\\nperception is handled by low-level processes that merely tell your brain\\n\"that\\'s a water droplet\" without telling you details like where the lightest\\nand darkest points are, or \"that\\'s a bush\" without telling you the shape and\\nposition of every leaf. This is a feature of brains, not a bug. In everyday\\nlife it would be distracting to notice every leaf on every bush. But when you\\nhave to paint something, you have to look more closely, and when you do\\nthere\\'s a lot to see. You can still be noticing new things after days of\\ntrying to paint something people usually take for granted, just as you can\\nafter days of trying to write an essay about something people usually take for\\ngranted.  \\n  \\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to\\npaint. But it seemed a good enough bet to be worth trying.  \\n  \\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and\\ngave me a good grade, which he wrote down in a sort of passport each student\\nhad. But the Accademia wasn\\'t teaching me anything except Italian, and my\\nmoney was running out, so at the end of the first year I went back to the US.  \\n  \\nI wanted to go back to RISD, but I was now broke and RISD was very expensive,\\nso I decided to get a job for a year and then return to RISD the next fall. I\\ngot one at a company called Interleaf, which made software for creating\\ndocuments. You mean like Microsoft Word? Exactly. That was how I learned that\\nlow end software tends to eat high end software. But Interleaf still had a few\\nyears to live yet. [5]  \\n  \\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a\\nscripting language, and even made the scripting language a dialect of Lisp.\\nNow they wanted a Lisp hacker to write things in it. This was the closest\\nthing I\\'ve had to a normal job, and I hereby apologize to my boss and\\ncoworkers, because I was a bad employee. Their Lisp was the thinnest icing on\\na giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never\\nunderstood most of the software. Plus I was terribly irresponsible. This was\\nback when a programming job meant showing up every day during certain working\\nhours. That seemed unnatural to me, and on this point the rest of the world is\\ncoming around to my way of thinking, but at the time it caused a lot of\\nfriction. Toward the end of the year I spent much of my time surreptitiously\\nworking on _On Lisp_ , which I had by this time gotten a contract to publish.  \\n  \\nThe good part was that I got paid huge amounts of money, especially by art\\nstudent standards. In Florence, after paying my part of the rent, my budget\\nfor everything else had been $7 a day. Now I was getting paid more than 4\\ntimes that every hour, even when I was just sitting in a meeting. By living\\ncheaply I not only managed to save enough to go back to RISD, but also paid\\noff my college loans.  \\n  \\nI learned some useful things at Interleaf, though they were mostly about what\\nnot to do. I learned that it\\'s better for technology companies to be run by\\nproduct people than sales people (though sales is a real skill and people who\\nare good at it are really good at it), that it leads to bugs when code is\\nedited by too many people, that cheap office space is no bargain if it\\'s\\ndepressing, that planned meetings are inferior to corridor conversations, that\\nbig, bureaucratic customers are a dangerous source of money, and that there\\'s\\nnot much overlap between conventional office hours and the optimal time for\\nhacking, or conventional offices and the optimal place for it.  \\n  \\nBut the most important thing I learned, and which I used in both Viaweb and Y\\nCombinator, is that the low end eats the high end: that it\\'s good to be the\\n\"entry level\" option, even though that will be less prestigious, because if\\nyou\\'re not, someone else will be, and will squash you against the ceiling.\\nWhich in turn means that prestige is a danger sign.  \\n  \\nWhen I left to go back to RISD the next fall, I arranged to do freelance work\\nfor the group that did projects for customers, and this was how I survived for\\nthe next several years. When I came back to visit for a project later on,\\nsomeone told me about a new thing called HTML, which was, as he described it,\\na derivative of SGML. Markup language enthusiasts were an occupational hazard\\nat Interleaf and I ignored him, but this HTML thing later became a big part of\\nmy life.  \\n  \\nIn the fall of 1992 I moved back to Providence to continue at RISD. The\\nfoundation had merely been intro stuff, and the Accademia had been a (very\\ncivilized) joke. Now I was going to see what real art school was like. But\\nalas it was more like the Accademia than not. Better organized, certainly, and\\na lot more expensive, but it was now becoming clear that art school did not\\nbear the same relationship to art that medical school bore to medicine. At\\nleast not the painting department. The textile department, which my next door\\nneighbor belonged to, seemed to be pretty rigorous. No doubt illustration and\\narchitecture were too. But painting was post-rigorous. Painting students were\\nsupposed to express themselves, which to the more worldly ones meant to try to\\ncook up some sort of distinctive signature style.  \\n  \\nA signature style is the visual equivalent of what in show business is known\\nas a \"schtick\": something that immediately identifies the work as yours and no\\none else\\'s. For example, when you see a painting that looks like a certain\\nkind of cartoon, you know it\\'s by Roy Lichtenstein. So if you see a big\\npainting of this type hanging in the apartment of a hedge fund manager, you\\nknow he paid millions of dollars for it. That\\'s not always why artists have a\\nsignature style, but it\\'s usually why buyers pay a lot for such work. [6]  \\n  \\nThere were plenty of earnest students too: kids who \"could draw\" in high\\nschool, and now had come to what was supposed to be the best art school in the\\ncountry, to learn to draw even better. They tended to be confused and\\ndemoralized by what they found at RISD, but they kept going, because painting\\nwas what they did. I was not one of the kids who could draw in high school,\\nbut at RISD I was definitely closer to their tribe than the tribe of signature\\nstyle seekers.  \\n  \\nI learned a lot in the color class I took at RISD, but otherwise I was\\nbasically teaching myself to paint, and I could do that for free. So in 1993 I\\ndropped out. I hung around Providence for a bit, and then my college friend\\nNancy Parmet did me a big favor. A rent-controlled apartment in a building her\\nmother owned in New York was becoming vacant. Did I want it? It wasn\\'t much\\nmore than my current place, and New York was supposed to be where the artists\\nwere. So yes, I wanted it! [7]  \\n  \\nAsterix comics begin by zooming in on a tiny corner of Roman Gaul that turns\\nout not to be controlled by the Romans. You can do something similar on a map\\nof New York City: if you zoom in on the Upper East Side, there\\'s a tiny corner\\nthat\\'s not rich, or at least wasn\\'t in 1993. It\\'s called Yorkville, and that\\nwas my new home. Now I was a New York artist \\x97 in the strictly technical sense\\nof making paintings and living in New York.  \\n  \\nI was nervous about money, because I could sense that Interleaf was on the way\\ndown. Freelance Lisp hacking work was very rare, and I didn\\'t want to have to\\nprogram in another language, which in those days would have meant C++ if I was\\nlucky. So with my unerring nose for financial opportunity, I decided to write\\nanother book on Lisp. This would be a popular book, the sort of book that\\ncould be used as a textbook. I imagined myself living frugally off the\\nroyalties and spending all my time painting. (The painting on the cover of\\nthis book, _ANSI Common Lisp_ , is one that I painted around this time.)  \\n  \\nThe best thing about New York for me was the presence of Idelle and Julian\\nWeber. Idelle Weber was a painter, one of the early photorealists, and I\\'d\\ntaken her painting class at Harvard. I\\'ve never known a teacher more beloved\\nby her students. Large numbers of former students kept in touch with her,\\nincluding me. After I moved to New York I became her de facto studio\\nassistant.  \\n  \\nShe liked to paint on big, square canvases, 4 to 5 feet on a side. One day in\\nlate 1994 as I was stretching one of these monsters there was something on the\\nradio about a famous fund manager. He wasn\\'t that much older than me, and was\\nsuper rich. The thought suddenly occurred to me: why don\\'t I become rich? Then\\nI\\'ll be able to work on whatever I want.  \\n  \\nMeanwhile I\\'d been hearing more and more about this new thing called the World\\nWide Web. Robert Morris showed it to me when I visited him in Cambridge, where\\nhe was now in grad school at Harvard. It seemed to me that the web would be a\\nbig deal. I\\'d seen what graphical user interfaces had done for the popularity\\nof microcomputers. It seemed like the web would do the same for the internet.  \\n  \\nIf I wanted to get rich, here was the next train leaving the station. I was\\nright about that part. What I got wrong was the idea. I decided we should\\nstart a company to put art galleries online. I can\\'t honestly say, after\\nreading so many Y Combinator applications, that this was the worst startup\\nidea ever, but it was up there. Art galleries didn\\'t want to be online, and\\nstill don\\'t, not the fancy ones. That\\'s not how they sell. I wrote some\\nsoftware to generate web sites for galleries, and Robert wrote some to resize\\nimages and set up an http server to serve the pages. Then we tried to sign up\\ngalleries. To call this a difficult sale would be an understatement. It was\\ndifficult to give away. A few galleries let us make sites for them for free,\\nbut none paid us.  \\n  \\nThen some online stores started to appear, and I realized that except for the\\norder buttons they were identical to the sites we\\'d been generating for\\ngalleries. This impressive-sounding thing called an \"internet storefront\" was\\nsomething we already knew how to build.  \\n  \\nSo in the summer of 1995, after I submitted the camera-ready copy of _ANSI\\nCommon Lisp_ to the publishers, we started trying to write software to build\\nonline stores. At first this was going to be normal desktop software, which in\\nthose days meant Windows software. That was an alarming prospect, because\\nneither of us knew how to write Windows software or wanted to learn. We lived\\nin the Unix world. But we decided we\\'d at least try writing a prototype store\\nbuilder on Unix. Robert wrote a shopping cart, and I wrote a new site\\ngenerator for stores \\x97 in Lisp, of course.  \\n  \\nWe were working out of Robert\\'s apartment in Cambridge. His roommate was away\\nfor big chunks of time, during which I got to sleep in his room. For some\\nreason there was no bed frame or sheets, just a mattress on the floor. One\\nmorning as I was lying on this mattress I had an idea that made me sit up like\\na capital L. What if we ran the software on the server, and let users control\\nit by clicking on links? Then we\\'d never have to write anything to run on\\nusers\\' computers. We could generate the sites on the same server we\\'d serve\\nthem from. Users wouldn\\'t need anything more than a browser.  \\n  \\nThis kind of software, known as a web app, is common now, but at the time it\\nwasn\\'t clear that it was even possible. To find out, we decided to try making\\na version of our store builder that you could control through the browser. A\\ncouple days later, on August 12, we had one that worked. The UI was horrible,\\nbut it proved you could build a whole store through the browser, without any\\nclient software or typing anything into the command line on the server.  \\n  \\nNow we felt like we were really onto something. I had visions of a whole new\\ngeneration of software working this way. You wouldn\\'t need versions, or ports,\\nor any of that crap. At Interleaf there had been a whole group called Release\\nEngineering that seemed to be at least as big as the group that actually wrote\\nthe software. Now you could just update the software right on the server.  \\n  \\nWe started a new company we called Viaweb, after the fact that our software\\nworked via the web, and we got $10,000 in seed funding from Idelle\\'s husband\\nJulian. In return for that and doing the initial legal work and giving us\\nbusiness advice, we gave him 10% of the company. Ten years later this deal\\nbecame the model for Y Combinator\\'s. We knew founders needed something like\\nthis, because we\\'d needed it ourselves.  \\n  \\nAt this stage I had a negative net worth, because the thousand dollars or so I\\nhad in the bank was more than counterbalanced by what I owed the government in\\ntaxes. (Had I diligently set aside the proper proportion of the money I\\'d made\\nconsulting for Interleaf? No, I had not.) So although Robert had his graduate\\nstudent stipend, I needed that seed funding to live on.  \\n  \\nWe originally hoped to launch in September, but we got more ambitious about\\nthe software as we worked on it. Eventually we managed to build a WYSIWYG site\\nbuilder, in the sense that as you were creating pages, they looked exactly\\nlike the static ones that would be generated later, except that instead of\\nleading to static pages, the links all referred to closures stored in a hash\\ntable on the server.  \\n  \\nIt helped to have studied art, because the main goal of an online store\\nbuilder is to make users look legit, and the key to looking legit is high\\nproduction values. If you get page layouts and fonts and colors right, you can\\nmake a guy running a store out of his bedroom look more legit than a big\\ncompany.  \\n  \\n(If you\\'re curious why my site looks so old-fashioned, it\\'s because it\\'s still\\nmade with this software. It may look clunky today, but in 1996 it was the last\\nword in slick.)  \\n  \\nIn September, Robert rebelled. \"We\\'ve been working on this for a month,\" he\\nsaid, \"and it\\'s still not done.\" This is funny in retrospect, because he would\\nstill be working on it almost 3 years later. But I decided it might be prudent\\nto recruit more programmers, and I asked Robert who else in grad school with\\nhim was really good. He recommended Trevor Blackwell, which surprised me at\\nfirst, because at that point I knew Trevor mainly for his plan to reduce\\neverything in his life to a stack of notecards, which he carried around with\\nhim. But Rtm was right, as usual. Trevor turned out to be a frighteningly\\neffective hacker.  \\n  \\nIt was a lot of fun working with Robert and Trevor. They\\'re the two most\\n[_independent-minded_](think.html) people I know, and in completely different\\nways. If you could see inside Rtm\\'s brain it would look like a colonial New\\nEngland church, and if you could see inside Trevor\\'s it would look like the\\nworst excesses of Austrian Rococo.  \\n  \\nWe opened for business, with 6 stores, in January 1996. It was just as well we\\nwaited a few months, because although we worried we were late, we were\\nactually almost fatally early. There was a lot of talk in the press then about\\necommerce, but not many people actually wanted online stores. [8]  \\n  \\nThere were three main parts to the software: the editor, which people used to\\nbuild sites and which I wrote, the shopping cart, which Robert wrote, and the\\nmanager, which kept track of orders and statistics, and which Trevor wrote. In\\nits time, the editor was one of the best general-purpose site builders. I kept\\nthe code tight and didn\\'t have to integrate with any other software except\\nRobert\\'s and Trevor\\'s, so it was quite fun to work on. If all I\\'d had to do\\nwas work on this software, the next 3 years would have been the easiest of my\\nlife. Unfortunately I had to do a lot more, all of it stuff I was worse at\\nthan programming, and the next 3 years were instead the most stressful.  \\n  \\nThere were a lot of startups making ecommerce software in the second half of\\nthe 90s. We were determined to be the Microsoft Word, not the Interleaf. Which\\nmeant being easy to use and inexpensive. It was lucky for us that we were\\npoor, because that caused us to make Viaweb even more inexpensive than we\\nrealized. We charged $100 a month for a small store and $300 a month for a big\\none. This low price was a big attraction, and a constant thorn in the sides of\\ncompetitors, but it wasn\\'t because of some clever insight that we set the\\nprice low. We had no idea what businesses paid for things. $300 a month seemed\\nlike a lot of money to us.  \\n  \\nWe did a lot of things right by accident like that. For example, we did what\\'s\\nnow called \"doing things that [_don\\'t scale_](ds.html),\" although at the time\\nwe would have described it as \"being so lame that we\\'re driven to the most\\ndesperate measures to get users.\" The most common of which was building stores\\nfor them. This seemed particularly humiliating, since the whole raison d\\'etre\\nof our software was that people could use it to make their own stores. But\\nanything to get users.  \\n  \\nWe learned a lot more about retail than we wanted to know. For example, that\\nif you could only have a small image of a man\\'s shirt (and all images were\\nsmall then by present standards), it was better to have a closeup of the\\ncollar than a picture of the whole shirt. The reason I remember learning this\\nwas that it meant I had to rescan about 30 images of men\\'s shirts. My first\\nset of scans were so beautiful too.  \\n  \\nThough this felt wrong, it was exactly the right thing to be doing. Building\\nstores for users taught us about retail, and about how it felt to use our\\nsoftware. I was initially both mystified and repelled by \"business\" and\\nthought we needed a \"business person\" to be in charge of it, but once we\\nstarted to get users, I was converted, in much the same way I was converted to\\n[_fatherhood_](kids.html) once I had kids. Whatever users wanted, I was all\\ntheirs. Maybe one day we\\'d have so many users that I couldn\\'t scan their\\nimages for them, but in the meantime there was nothing more important to do.  \\n  \\nAnother thing I didn\\'t get at the time is that [_growth rate_](growth.html) is\\nthe ultimate test of a startup. Our growth rate was fine. We had about 70\\nstores at the end of 1996 and about 500 at the end of 1997. I mistakenly\\nthought the thing that mattered was the absolute number of users. And that is\\nthe thing that matters in the sense that that\\'s how much money you\\'re making,\\nand if you\\'re not making enough, you might go out of business. But in the long\\nterm the growth rate takes care of the absolute number. If we\\'d been a startup\\nI was advising at Y Combinator, I would have said: Stop being so stressed out,\\nbecause you\\'re doing fine. You\\'re growing 7x a year. Just don\\'t hire too many\\nmore people and you\\'ll soon be profitable, and then you\\'ll control your own\\ndestiny.  \\n  \\nAlas I hired lots more people, partly because our investors wanted me to, and\\npartly because that\\'s what startups did during the Internet Bubble. A company\\nwith just a handful of employees would have seemed amateurish. So we didn\\'t\\nreach breakeven until about when Yahoo bought us in the summer of 1998. Which\\nin turn meant we were at the mercy of investors for the entire life of the\\ncompany. And since both we and our investors were noobs at startups, the\\nresult was a mess even by startup standards.  \\n  \\nIt was a huge relief when Yahoo bought us. In principle our Viaweb stock was\\nvaluable. It was a share in a business that was profitable and growing\\nrapidly. But it didn\\'t feel very valuable to me; I had no idea how to value a\\nbusiness, but I was all too keenly aware of the near-death experiences we\\nseemed to have every few months. Nor had I changed my grad student lifestyle\\nsignificantly since we started. So when Yahoo bought us it felt like going\\nfrom rags to riches. Since we were going to California, I bought a car, a\\nyellow 1998 VW GTI. I remember thinking that its leather seats alone were by\\nfar the most luxurious thing I owned.  \\n  \\nThe next year, from the summer of 1998 to the summer of 1999, must have been\\nthe least productive of my life. I didn\\'t realize it at the time, but I was\\nworn out from the effort and stress of running Viaweb. For a while after I got\\nto California I tried to continue my usual m.o. of programming till 3 in the\\nmorning, but fatigue combined with Yahoo\\'s prematurely aged\\n[_culture_](yahoo.html) and grim cube farm in Santa Clara gradually dragged me\\ndown. After a few months it felt disconcertingly like working at Interleaf.  \\n  \\nYahoo had given us a lot of options when they bought us. At the time I thought\\nYahoo was so overvalued that they\\'d never be worth anything, but to my\\nastonishment the stock went up 5x in the next year. I hung on till the first\\nchunk of options vested, then in the summer of 1999 I left. It had been so\\nlong since I\\'d painted anything that I\\'d half forgotten why I was doing this.\\nMy brain had been entirely full of software and men\\'s shirts for 4 years. But\\nI had done this to get rich so I could paint, I reminded myself, and now I was\\nrich, so I should go paint.  \\n  \\nWhen I said I was leaving, my boss at Yahoo had a long conversation with me\\nabout my plans. I told him all about the kinds of pictures I wanted to paint.\\nAt the time I was touched that he took such an interest in me. Now I realize\\nit was because he thought I was lying. My options at that point were worth\\nabout $2 million a month. If I was leaving that kind of money on the table, it\\ncould only be to go and start some new startup, and if I did, I might take\\npeople with me. This was the height of the Internet Bubble, and Yahoo was\\nground zero of it. My boss was at that moment a billionaire. Leaving then to\\nstart a new startup must have seemed to him an insanely, and yet also\\nplausibly, ambitious plan.  \\n  \\nBut I really was quitting to paint, and I started immediately. There was no\\ntime to lose. I\\'d already burned 4 years getting rich. Now when I talk to\\nfounders who are leaving after selling their companies, my advice is always\\nthe same: take a vacation. That\\'s what I should have done, just gone off\\nsomewhere and done nothing for a month or two, but the idea never occurred to\\nme.  \\n  \\nSo I tried to paint, but I just didn\\'t seem to have any energy or ambition.\\nPart of the problem was that I didn\\'t know many people in California. I\\'d\\ncompounded this problem by buying a house up in the Santa Cruz Mountains, with\\na beautiful view but miles from anywhere. I stuck it out for a few more\\nmonths, then in desperation I went back to New York, where unless you\\nunderstand about rent control you\\'ll be surprised to hear I still had my\\napartment, sealed up like a tomb of my old life. Idelle was in New York at\\nleast, and there were other people trying to paint there, even though I didn\\'t\\nknow any of them.  \\n  \\nWhen I got back to New York I resumed my old life, except now I was rich. It\\nwas as weird as it sounds. I resumed all my old patterns, except now there\\nwere doors where there hadn\\'t been. Now when I was tired of walking, all I had\\nto do was raise my hand, and (unless it was raining) a taxi would stop to pick\\nme up. Now when I walked past charming little restaurants I could go in and\\norder lunch. It was exciting for a while. Painting started to go better. I\\nexperimented with a new kind of still life where I\\'d paint one painting in the\\nold way, then photograph it and print it, blown up, on canvas, and then use\\nthat as the underpainting for a second still life, painted from the same\\nobjects (which hopefully hadn\\'t rotted yet).  \\n  \\nMeanwhile I looked for an apartment to buy. Now I could actually choose what\\nneighborhood to live in. Where, I asked myself and various real estate agents,\\nis the Cambridge of New York? Aided by occasional visits to actual Cambridge,\\nI gradually realized there wasn\\'t one. Huh.  \\n  \\nAround this time, in the spring of 2000, I had an idea. It was clear from our\\nexperience with Viaweb that web apps were the future. Why not build a web app\\nfor making web apps? Why not let people edit code on our server through the\\nbrowser, and then host the resulting applications for them? [9] You could run\\nall sorts of services on the servers that these applications could use just by\\nmaking an API call: making and receiving phone calls, manipulating images,\\ntaking credit card payments, etc.  \\n  \\nI got so excited about this idea that I couldn\\'t think about anything else. It\\nseemed obvious that this was the future. I didn\\'t particularly want to start\\nanother company, but it was clear that this idea would have to be embodied as\\none, so I decided to move to Cambridge and start it. I hoped to lure Robert\\ninto working on it with me, but there I ran into a hitch. Robert was now a\\npostdoc at MIT, and though he\\'d made a lot of money the last time I\\'d lured\\nhim into working on one of my schemes, it had also been a huge time sink. So\\nwhile he agreed that it sounded like a plausible idea, he firmly refused to\\nwork on it.  \\n  \\nHmph. Well, I\\'d do it myself then. I recruited Dan Giffin, who had worked for\\nViaweb, and two undergrads who wanted summer jobs, and we got to work trying\\nto build what it\\'s now clear is about twenty companies and several open source\\nprojects worth of software. The language for defining applications would of\\ncourse be a dialect of Lisp. But I wasn\\'t so naive as to assume I could spring\\nan overt Lisp on a general audience; we\\'d hide the parentheses, like Dylan\\ndid.  \\n  \\nBy then there was a name for the kind of company Viaweb was, an \"application\\nservice provider,\" or ASP. This name didn\\'t last long before it was replaced\\nby \"software as a service,\" but it was current for long enough that I named\\nthis new company after it: it was going to be called Aspra.  \\n  \\nI started working on the application builder, Dan worked on network\\ninfrastructure, and the two undergrads worked on the first two services\\n(images and phone calls). But about halfway through the summer I realized I\\nreally didn\\'t want to run a company \\x97 especially not a big one, which it was\\nlooking like this would have to be. I\\'d only started Viaweb because I needed\\nthe money. Now that I didn\\'t need money anymore, why was I doing this? If this\\nvision had to be realized as a company, then screw the vision. I\\'d build a\\nsubset that could be done as an open source project.  \\n  \\nMuch to my surprise, the time I spent working on this stuff was not wasted\\nafter all. After we started Y Combinator, I would often encounter startups\\nworking on parts of this new architecture, and it was very useful to have\\nspent so much time thinking about it and even trying to write some of it.  \\n  \\nThe subset I would build as an open source project was the new Lisp, whose\\nparentheses I now wouldn\\'t even have to hide. A lot of Lisp hackers dream of\\nbuilding a new Lisp, partly because one of the distinctive features of the\\nlanguage is that it has dialects, and partly, I think, because we have in our\\nminds a Platonic form of Lisp that all existing dialects fall short of. I\\ncertainly did. So at the end of the summer Dan and I switched to working on\\nthis new dialect of Lisp, which I called Arc, in a house I bought in\\nCambridge.  \\n  \\nThe following spring, lightning struck. I was invited to give a talk at a Lisp\\nconference, so I gave one about how we\\'d used Lisp at Viaweb. Afterward I put\\na postscript file of this talk online, on paulgraham.com, which I\\'d created\\nyears before using Viaweb but had never used for anything. In one day it got\\n30,000 page views. What on earth had happened? The referring urls showed that\\nsomeone had posted it on Slashdot. [10]  \\n  \\nWow, I thought, there\\'s an audience. If I write something and put it on the\\nweb, anyone can read it. That may seem obvious now, but it was surprising\\nthen. In the print era there was a narrow channel to readers, guarded by\\nfierce monsters known as editors. The only way to get an audience for anything\\nyou wrote was to get it published as a book, or in a newspaper or magazine.\\nNow anyone could publish anything.  \\n  \\nThis had been possible in principle since 1993, but not many people had\\nrealized it yet. I had been intimately involved with building the\\ninfrastructure of the web for most of that time, and a writer as well, and it\\nhad taken me 8 years to realize it. Even then it took me several years to\\nunderstand the implications. It meant there would be a whole new generation of\\n[_essays_](essay.html). [11]  \\n  \\nIn the print era, the channel for publishing essays had been vanishingly\\nsmall. Except for a few officially anointed thinkers who went to the right\\nparties in New York, the only people allowed to publish essays were\\nspecialists writing about their specialties. There were so many essays that\\nhad never been written, because there had been no way to publish them. Now\\nthey could be, and I was going to write them. [12]  \\n  \\nI\\'ve worked on several different things, but to the extent there was a turning\\npoint where I figured out what to work on, it was when I started publishing\\nessays online. From then on I knew that whatever else I did, I\\'d always write\\nessays too.  \\n  \\nI knew that online essays would be a [_marginal_](marginal.html) medium at\\nfirst. Socially they\\'d seem more like rants posted by nutjobs on their\\nGeoCities sites than the genteel and beautifully typeset compositions\\npublished in _The New Yorker_. But by this point I knew enough to find that\\nencouraging instead of discouraging.  \\n  \\nOne of the most conspicuous patterns I\\'ve noticed in my life is how well it\\nhas worked, for me at least, to work on things that weren\\'t prestigious. Still\\nlife has always been the least prestigious form of painting. Viaweb and Y\\nCombinator both seemed lame when we started them. I still get the glassy eye\\nfrom strangers when they ask what I\\'m writing, and I explain that it\\'s an\\nessay I\\'m going to publish on my web site. Even Lisp, though prestigious\\nintellectually in something like the way Latin is, also seems about as hip.  \\n  \\nIt\\'s not that unprestigious types of work are good per se. But when you find\\nyourself drawn to some kind of work despite its current lack of prestige, it\\'s\\na sign both that there\\'s something real to be discovered there, and that you\\nhave the right kind of motives. Impure motives are a big danger for the\\nambitious. If anything is going to lead you astray, it will be the desire to\\nimpress people. So while working on things that aren\\'t prestigious doesn\\'t\\nguarantee you\\'re on the right track, it at least guarantees you\\'re not on the\\nmost common type of wrong one.  \\n  \\nOver the next several years I wrote lots of essays about all kinds of\\ndifferent topics. O\\'Reilly reprinted a collection of them as a book, called\\n_Hackers & Painters_ after one of the essays in it. I also worked on spam\\nfilters, and did some more painting. I used to have dinners for a group of\\nfriends every thursday night, which taught me how to cook for groups. And I\\nbought another building in Cambridge, a former candy factory (and later, twas\\nsaid, porn studio), to use as an office.  \\n  \\nOne night in October 2003 there was a big party at my house. It was a clever\\nidea of my friend Maria Daniels, who was one of the thursday diners. Three\\nseparate hosts would all invite their friends to one party. So for every\\nguest, two thirds of the other guests would be people they didn\\'t know but\\nwould probably like. One of the guests was someone I didn\\'t know but would\\nturn out to like a lot: a woman called Jessica Livingston. A couple days later\\nI asked her out.  \\n  \\nJessica was in charge of marketing at a Boston investment bank. This bank\\nthought it understood startups, but over the next year, as she met friends of\\nmine from the startup world, she was surprised how different reality was. And\\nhow colorful their stories were. So she decided to compile a book of\\n[_interviews_](https://www.amazon.com/Founders-Work-Stories-Startups-\\nEarly/dp/1430210788) with startup founders.  \\n  \\nWhen the bank had financial problems and she had to fire half her staff, she\\nstarted looking for a new job. In early 2005 she interviewed for a marketing\\njob at a Boston VC firm. It took them weeks to make up their minds, and during\\nthis time I started telling her about all the things that needed to be fixed\\nabout venture capital. They should make a larger number of smaller investments\\ninstead of a handful of giant ones, they should be funding younger, more\\ntechnical founders instead of MBAs, they should let the founders remain as\\nCEO, and so on.  \\n  \\nOne of my tricks for writing essays had always been to give talks. The\\nprospect of having to stand up in front of a group of people and tell them\\nsomething that won\\'t waste their time is a great spur to the imagination. When\\nthe Harvard Computer Society, the undergrad computer club, asked me to give a\\ntalk, I decided I would tell them how to start a startup. Maybe they\\'d be able\\nto avoid the worst of the mistakes we\\'d made.  \\n  \\nSo I gave this talk, in the course of which I told them that the best sources\\nof seed funding were successful startup founders, because then they\\'d be\\nsources of advice too. Whereupon it seemed they were all looking expectantly\\nat me. Horrified at the prospect of having my inbox flooded by business plans\\n(if I\\'d only known), I blurted out \"But not me!\" and went on with the talk.\\nBut afterward it occurred to me that I should really stop procrastinating\\nabout angel investing. I\\'d been meaning to since Yahoo bought us, and now it\\nwas 7 years later and I still hadn\\'t done one angel investment.  \\n  \\nMeanwhile I had been scheming with Robert and Trevor about projects we could\\nwork on together. I missed working with them, and it seemed like there had to\\nbe something we could collaborate on.  \\n  \\nAs Jessica and I were walking home from dinner on March 11, at the corner of\\nGarden and Walker streets, these three threads converged. Screw the VCs who\\nwere taking so long to make up their minds. We\\'d start our own investment firm\\nand actually implement the ideas we\\'d been talking about. I\\'d fund it, and\\nJessica could quit her job and work for it, and we\\'d get Robert and Trevor as\\npartners too. [13]  \\n  \\nOnce again, ignorance worked in our favor. We had no idea how to be angel\\ninvestors, and in Boston in 2005 there were no Ron Conways to learn from. So\\nwe just made what seemed like the obvious choices, and some of the things we\\ndid turned out to be novel.  \\n  \\nThere are multiple components to Y Combinator, and we didn\\'t figure them all\\nout at once. The part we got first was to be an angel firm. In those days,\\nthose two words didn\\'t go together. There were VC firms, which were organized\\ncompanies with people whose job it was to make investments, but they only did\\nbig, million dollar investments. And there were angels, who did smaller\\ninvestments, but these were individuals who were usually focused on other\\nthings and made investments on the side. And neither of them helped founders\\nenough in the beginning. We knew how helpless founders were in some respects,\\nbecause we remembered how helpless we\\'d been. For example, one thing Julian\\nhad done for us that seemed to us like magic was to get us set up as a\\ncompany. We were fine writing fairly difficult software, but actually getting\\nincorporated, with bylaws and stock and all that stuff, how on earth did you\\ndo that? Our plan was not only to make seed investments, but to do for\\nstartups everything Julian had done for us.  \\n  \\nYC was not organized as a fund. It was cheap enough to run that we funded it\\nwith our own money. That went right by 99% of readers, but professional\\ninvestors are thinking \"Wow, that means they got all the returns.\" But once\\nagain, this was not due to any particular insight on our part. We didn\\'t know\\nhow VC firms were organized. It never occurred to us to try to raise a fund,\\nand if it had, we wouldn\\'t have known where to start. [14]  \\n  \\nThe most distinctive thing about YC is the batch model: to fund a bunch of\\nstartups all at once, twice a year, and then to spend three months focusing\\nintensively on trying to help them. That part we discovered by accident, not\\nmerely implicitly but explicitly due to our ignorance about investing. We\\nneeded to get experience as investors. What better way, we thought, than to\\nfund a whole bunch of startups at once? We knew undergrads got temporary jobs\\nat tech companies during the summer. Why not organize a summer program where\\nthey\\'d start startups instead? We wouldn\\'t feel guilty for being in a sense\\nfake investors, because they would in a similar sense be fake founders. So\\nwhile we probably wouldn\\'t make much money out of it, we\\'d at least get to\\npractice being investors on them, and they for their part would probably have\\na more interesting summer than they would working at Microsoft.  \\n  \\nWe\\'d use the building I owned in Cambridge as our headquarters. We\\'d all have\\ndinner there once a week \\x97 on tuesdays, since I was already cooking for the\\nthursday diners on thursdays \\x97 and after dinner we\\'d bring in experts on\\nstartups to give talks.  \\n  \\nWe knew undergrads were deciding then about summer jobs, so in a matter of\\ndays we cooked up something we called the Summer Founders Program, and I\\nposted an [_announcement_](summerfounder.html) on my site, inviting undergrads\\nto apply. I had never imagined that writing essays would be a way to get \"deal\\nflow,\" as investors call it, but it turned out to be the perfect source. [15]\\nWe got 225 applications for the Summer Founders Program, and we were surprised\\nto find that a lot of them were from people who\\'d already graduated, or were\\nabout to that spring. Already this SFP thing was starting to feel more serious\\nthan we\\'d intended.  \\n  \\nWe invited about 20 of the 225 groups to interview in person, and from those\\nwe picked 8 to fund. They were an impressive group. That first batch included\\nreddit, Justin Kan and Emmett Shear, who went on to found Twitch, Aaron\\nSwartz, who had already helped write the RSS spec and would a few years later\\nbecome a martyr for open access, and Sam Altman, who would later become the\\nsecond president of YC. I don\\'t think it was entirely luck that the first\\nbatch was so good. You had to be pretty bold to sign up for a weird thing like\\nthe Summer Founders Program instead of a summer job at a legit place like\\nMicrosoft or Goldman Sachs.  \\n  \\nThe deal for startups was based on a combination of the deal we did with\\nJulian ($10k for 10%) and what Robert said MIT grad students got for the\\nsummer ($6k). We invested $6k per founder, which in the typical two-founder\\ncase was $12k, in return for 6%. That had to be fair, because it was twice as\\ngood as the deal we ourselves had taken. Plus that first summer, which was\\nreally hot, Jessica brought the founders free air conditioners. [16]  \\n  \\nFairly quickly I realized that we had stumbled upon the way to scale startup\\nfunding. Funding startups in batches was more convenient for us, because it\\nmeant we could do things for a lot of startups at once, but being part of a\\nbatch was better for the startups too. It solved one of the biggest problems\\nfaced by founders: the isolation. Now you not only had colleagues, but\\ncolleagues who understood the problems you were facing and could tell you how\\nthey were solving them.  \\n  \\nAs YC grew, we started to notice other advantages of scale. The alumni became\\na tight community, dedicated to helping one another, and especially the\\ncurrent batch, whose shoes they remembered being in. We also noticed that the\\nstartups were becoming one another\\'s customers. We used to refer jokingly to\\nthe \"YC GDP,\" but as YC grows this becomes less and less of a joke. Now lots\\nof startups get their initial set of customers almost entirely from among\\ntheir batchmates.  \\n  \\nI had not originally intended YC to be a full-time job. I was going to do\\nthree things: hack, write essays, and work on YC. As YC grew, and I grew more\\nexcited about it, it started to take up a lot more than a third of my\\nattention. But for the first few years I was still able to work on other\\nthings.  \\n  \\nIn the summer of 2006, Robert and I started working on a new version of Arc.\\nThis one was reasonably fast, because it was compiled into Scheme. To test\\nthis new Arc, I wrote Hacker News in it. It was originally meant to be a news\\naggregator for startup founders and was called Startup News, but after a few\\nmonths I got tired of reading about nothing but startups. Plus it wasn\\'t\\nstartup founders we wanted to reach. It was future startup founders. So I\\nchanged the name to Hacker News and the topic to whatever engaged one\\'s\\nintellectual curiosity.  \\n  \\nHN was no doubt good for YC, but it was also by far the biggest source of\\nstress for me. If all I\\'d had to do was select and help founders, life would\\nhave been so easy. And that implies that HN was a mistake. Surely the biggest\\nsource of stress in one\\'s work should at least be something close to the core\\nof the work. Whereas I was like someone who was in pain while running a\\nmarathon not from the exertion of running, but because I had a blister from an\\nill-fitting shoe. When I was dealing with some urgent problem during YC, there\\nwas about a 60% chance it had to do with HN, and a 40% chance it had do with\\neverything else combined. [17]  \\n  \\nAs well as HN, I wrote all of YC\\'s internal software in Arc. But while I\\ncontinued to work a good deal _in_ Arc, I gradually stopped working _on_ Arc,\\npartly because I didn\\'t have time to, and partly because it was a lot less\\nattractive to mess around with the language now that we had all this\\ninfrastructure depending on it. So now my three projects were reduced to two:\\nwriting essays and working on YC.  \\n  \\nYC was different from other kinds of work I\\'ve done. Instead of deciding for\\nmyself what to work on, the problems came to me. Every 6 months there was a\\nnew batch of startups, and their problems, whatever they were, became our\\nproblems. It was very engaging work, because their problems were quite varied,\\nand the good founders were very effective. If you were trying to learn the\\nmost you could about startups in the shortest possible time, you couldn\\'t have\\npicked a better way to do it.  \\n  \\nThere were parts of the job I didn\\'t like. Disputes between cofounders,\\nfiguring out when people were lying to us, fighting with people who maltreated\\nthe startups, and so on. But I worked hard even at the parts I didn\\'t like. I\\nwas haunted by something Kevin Hale once said about companies: \"No one works\\nharder than the boss.\" He meant it both descriptively and prescriptively, and\\nit was the second part that scared me. I wanted YC to be good, so if how hard\\nI worked set the upper bound on how hard everyone else worked, I\\'d better work\\nvery hard.  \\n  \\nOne day in 2010, when he was visiting California for interviews, Robert Morris\\ndid something astonishing: he offered me unsolicited advice. I can only\\nremember him doing that once before. One day at Viaweb, when I was bent over\\ndouble from a kidney stone, he suggested that it would be a good idea for him\\nto take me to the hospital. That was what it took for Rtm to offer unsolicited\\nadvice. So I remember his exact words very clearly. \"You know,\" he said, \"you\\nshould make sure Y Combinator isn\\'t the last cool thing you do.\"  \\n  \\nAt the time I didn\\'t understand what he meant, but gradually it dawned on me\\nthat he was saying I should quit. This seemed strange advice, because YC was\\ndoing great. But if there was one thing rarer than Rtm offering advice, it was\\nRtm being wrong. So this set me thinking. It was true that on my current\\ntrajectory, YC would be the last thing I did, because it was only taking up\\nmore of my attention. It had already eaten Arc, and was in the process of\\neating essays too. Either YC was my life\\'s work or I\\'d have to leave\\neventually. And it wasn\\'t, so I would.  \\n  \\nIn the summer of 2012 my mother had a stroke, and the cause turned out to be a\\nblood clot caused by colon cancer. The stroke destroyed her balance, and she\\nwas put in a nursing home, but she really wanted to get out of it and back to\\nher house, and my sister and I were determined to help her do it. I used to\\nfly up to Oregon to visit her regularly, and I had a lot of time to think on\\nthose flights. On one of them I realized I was ready to hand YC over to\\nsomeone else.  \\n  \\nI asked Jessica if she wanted to be president, but she didn\\'t, so we decided\\nwe\\'d try to recruit Sam Altman. We talked to Robert and Trevor and we agreed\\nto make it a complete changing of the guard. Up till that point YC had been\\ncontrolled by the original LLC we four had started. But we wanted YC to last\\nfor a long time, and to do that it couldn\\'t be controlled by the founders. So\\nif Sam said yes, we\\'d let him reorganize YC. Robert and I would retire, and\\nJessica and Trevor would become ordinary partners.  \\n  \\nWhen we asked Sam if he wanted to be president of YC, initially he said no. He\\nwanted to start a startup to make nuclear reactors. But I kept at it, and in\\nOctober 2013 he finally agreed. We decided he\\'d take over starting with the\\nwinter 2014 batch. For the rest of 2013 I left running YC more and more to\\nSam, partly so he could learn the job, and partly because I was focused on my\\nmother, whose cancer had returned.  \\n  \\nShe died on January 15, 2014. We knew this was coming, but it was still hard\\nwhen it did.  \\n  \\nI kept working on YC till March, to help get that batch of startups through\\nDemo Day, then I checked out pretty completely. (I still talk to alumni and to\\nnew startups working on things I\\'m interested in, but that only takes a few\\nhours a week.)  \\n  \\nWhat should I do next? Rtm\\'s advice hadn\\'t included anything about that. I\\nwanted to do something completely different, so I decided I\\'d paint. I wanted\\nto see how good I could get if I really focused on it. So the day after I\\nstopped working on YC, I started painting. I was rusty and it took a while to\\nget back into shape, but it was at least completely engaging. [18]  \\n  \\nI spent most of the rest of 2014 painting. I\\'d never been able to work so\\nuninterruptedly before, and I got to be better than I had been. Not good\\nenough, but better. Then in November, right in the middle of a painting, I ran\\nout of steam. Up till that point I\\'d always been curious to see how the\\npainting I was working on would turn out, but suddenly finishing this one\\nseemed like a chore. So I stopped working on it and cleaned my brushes and\\nhaven\\'t painted since. So far anyway.  \\n  \\nI realize that sounds rather wimpy. But attention is a zero sum game. If you\\ncan choose what to work on, and you choose a project that\\'s not the best one\\n(or at least a good one) for you, then it\\'s getting in the way of another\\nproject that is. And at 50 there was some opportunity cost to screwing around.  \\n  \\nI started writing essays again, and wrote a bunch of new ones over the next\\nfew months. I even wrote a couple that [_weren\\'t_](know.html) about startups.\\nThen in March 2015 I started working on Lisp again.  \\n  \\nThe distinctive thing about Lisp is that its core is a language defined by\\nwriting an interpreter in itself. It wasn\\'t originally intended as a\\nprogramming language in the ordinary sense. It was meant to be a formal model\\nof computation, an alternative to the Turing machine. If you want to write an\\ninterpreter for a language in itself, what\\'s the minimum set of predefined\\noperators you need? The Lisp that John McCarthy invented, or more accurately\\ndiscovered, is an answer to that question. [19]  \\n  \\nMcCarthy didn\\'t realize this Lisp could even be used to program computers till\\nhis grad student Steve Russell suggested it. Russell translated McCarthy\\'s\\ninterpreter into IBM 704 machine language, and from that point Lisp started\\nalso to be a programming language in the ordinary sense. But its origins as a\\nmodel of computation gave it a power and elegance that other languages\\ncouldn\\'t match. It was this that attracted me in college, though I didn\\'t\\nunderstand why at the time.  \\n  \\nMcCarthy\\'s 1960 Lisp did nothing more than interpret Lisp expressions. It was\\nmissing a lot of things you\\'d want in a programming language. So these had to\\nbe added, and when they were, they weren\\'t defined using McCarthy\\'s original\\naxiomatic approach. That wouldn\\'t have been feasible at the time. McCarthy\\ntested his interpreter by hand-simulating the execution of programs. But it\\nwas already getting close to the limit of interpreters you could test that way\\n\\x97 indeed, there was a bug in it that McCarthy had overlooked. To test a more\\ncomplicated interpreter, you\\'d have had to run it, and computers then weren\\'t\\npowerful enough.  \\n  \\nNow they are, though. Now you could continue using McCarthy\\'s axiomatic\\napproach till you\\'d defined a complete programming language. And as long as\\nevery change you made to McCarthy\\'s Lisp was a discoveredness-preserving\\ntransformation, you could, in principle, end up with a complete language that\\nhad this quality. Harder to do than to talk about, of course, but if it was\\npossible in principle, why not try? So I decided to take a shot at it. It took\\n4 years, from March 26, 2015 to October 12, 2019. It was fortunate that I had\\na precisely defined goal, or it would have been hard to keep at it for so\\nlong.  \\n  \\nI wrote this new Lisp, called [_Bel_](bel.html), in itself in Arc. That may\\nsound like a contradiction, but it\\'s an indication of the sort of trickery I\\nhad to engage in to make this work. By means of an egregious collection of\\nhacks I managed to make something close enough to an interpreter written in\\nitself that could actually run. Not fast, but fast enough to test.  \\n  \\nI had to ban myself from writing essays during most of this time, or I\\'d never\\nhave finished. In late 2015 I spent 3 months writing essays, and when I went\\nback to working on Bel I could barely understand the code. Not so much because\\nit was badly written as because the problem is so convoluted. When you\\'re\\nworking on an interpreter written in itself, it\\'s hard to keep track of what\\'s\\nhappening at what level, and errors can be practically encrypted by the time\\nyou get them.  \\n  \\nSo I said no more essays till Bel was done. But I told few people about Bel\\nwhile I was working on it. So for years it must have seemed that I was doing\\nnothing, when in fact I was working harder than I\\'d ever worked on anything.\\nOccasionally after wrestling for hours with some gruesome bug I\\'d check\\nTwitter or HN and see someone asking \"Does Paul Graham still code?\"  \\n  \\nWorking on Bel was hard but satisfying. I worked on it so intensively that at\\nany given time I had a decent chunk of the code in my head and could write\\nmore there. I remember taking the boys to the coast on a sunny day in 2015 and\\nfiguring out how to deal with some problem involving continuations while I\\nwatched them play in the tide pools. It felt like I was doing life right. I\\nremember that because I was slightly dismayed at how novel it felt. The good\\nnews is that I had more moments like this over the next few years.  \\n  \\nIn the summer of 2016 we moved to England. We wanted our kids to see what it\\nwas like living in another country, and since I was a British citizen by\\nbirth, that seemed the obvious choice. We only meant to stay for a year, but\\nwe liked it so much that we still live there. So most of Bel was written in\\nEngland.  \\n  \\nIn the fall of 2019, Bel was finally finished. Like McCarthy\\'s original Lisp,\\nit\\'s a spec rather than an implementation, although like McCarthy\\'s Lisp it\\'s\\na spec expressed as code.  \\n  \\nNow that I could write essays again, I wrote a bunch about topics I\\'d had\\nstacked up. I kept writing essays through 2020, but I also started to think\\nabout other things I could work on. How should I choose what to do? Well, how\\nhad I chosen what to work on in the past? I wrote an essay for myself to\\nanswer that question, and I was surprised how long and messy the answer turned\\nout to be. If this surprised me, who\\'d lived it, then I thought perhaps it\\nwould be interesting to other people, and encouraging to those with similarly\\nmessy lives. So I wrote a more detailed version for others to read, and this\\nis the last sentence of it.  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n**Notes**  \\n  \\n[1] My experience skipped a step in the evolution of computers: time-sharing\\nmachines with interactive OSes. I went straight from batch processing to\\nmicrocomputers, which made microcomputers seem all the more exciting.  \\n  \\n[2] Italian words for abstract concepts can nearly always be predicted from\\ntheir English cognates (except for occasional traps like _polluzione_). It\\'s\\nthe everyday words that differ. So if you string together a lot of abstract\\nconcepts with a few simple verbs, you can make a little Italian go a long way.  \\n  \\n[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight\\ndown the spine of old Florence: past the Pitti, across the bridge, past\\nOrsanmichele, between the Duomo and the Baptistery, and then up Via Ricasoli\\nto Piazza San Marco. I saw Florence at street level in every possible\\ncondition, from empty dark winter evenings to sweltering summer days when the\\nstreets were packed with tourists.  \\n  \\n[4] You can of course paint people like still lives if you want to, and\\nthey\\'re willing. That sort of portrait is arguably the apex of still life\\npainting, though the long sitting does tend to produce pained expressions in\\nthe sitters.  \\n  \\n[5] Interleaf was one of many companies that had smart people and built\\nimpressive technology, and yet got crushed by Moore\\'s Law. In the 1990s the\\nexponential growth in the power of commodity (i.e. Intel) processors rolled up\\nhigh-end, special-purpose hardware and software companies like a bulldozer.  \\n  \\n[6] The signature style seekers at RISD weren\\'t specifically mercenary. In the\\nart world, money and coolness are tightly coupled. Anything expensive comes to\\nbe seen as cool, and anything seen as cool will soon become equally expensive.  \\n  \\n[7] Technically the apartment wasn\\'t rent-controlled but rent-stabilized, but\\nthis is a refinement only New Yorkers would know or care about. The point is\\nthat it was really cheap, less than half market price.  \\n  \\n[8] Most software you can launch as soon as it\\'s done. But when the software\\nis an online store builder and you\\'re hosting the stores, if you don\\'t have\\nany users yet, that fact will be painfully obvious. So before we could launch\\npublicly we had to launch privately, in the sense of recruiting an initial set\\nof users and making sure they had decent-looking stores.  \\n  \\n[9] We\\'d had a code editor in Viaweb for users to define their own page\\nstyles. They didn\\'t know it, but they were editing Lisp expressions\\nunderneath. But this wasn\\'t an app editor, because the code ran when the\\nmerchants\\' sites were generated, not when shoppers visited them.  \\n  \\n[10] This was the first instance of what is now a familiar experience, and so\\nwas what happened next, when I read the comments and found they were full of\\nangry people. How could I claim that Lisp was better than other languages?\\nWeren\\'t they all Turing complete? People who see the responses to essays I\\nwrite sometimes tell me how sorry they feel for me, but I\\'m not exaggerating\\nwhen I reply that it has always been like this, since the very beginning. It\\ncomes with the territory. An essay must tell readers things they [_don\\'t\\nalready know_](useful.html), and some people dislike being told such things.  \\n  \\n[11] People put plenty of stuff on the internet in the 90s of course, but\\nputting something online is not the same as publishing it online. Publishing\\nonline means you treat the online version as the (or at least a) primary\\nversion.  \\n  \\n[12] There is a general lesson here that our experience with Y Combinator also\\nteaches: Customs continue to constrain you long after the restrictions that\\ncaused them have disappeared. Customary VC practice had once, like the customs\\nabout publishing essays, been based on real constraints. Startups had once\\nbeen much more expensive to start, and proportionally rare. Now they could be\\ncheap and common, but the VCs\\' customs still reflected the old world, just as\\ncustoms about writing essays still reflected the constraints of the print era.  \\n  \\nWhich in turn implies that people who are independent-minded (i.e. less\\ninfluenced by custom) will have an advantage in fields affected by rapid\\nchange (where customs are more likely to be obsolete).  \\n  \\nHere\\'s an interesting point, though: you can\\'t always predict which fields\\nwill be affected by rapid change. Obviously software and venture capital will\\nbe, but who would have predicted that essay writing would be?  \\n  \\n[13] Y Combinator was not the original name. At first we were called Cambridge\\nSeed. But we didn\\'t want a regional name, in case someone copied us in Silicon\\nValley, so we renamed ourselves after one of the coolest tricks in the lambda\\ncalculus, the Y combinator.  \\n  \\nI picked orange as our color partly because it\\'s the warmest, and partly\\nbecause no VC used it. In 2005 all the VCs used staid colors like maroon, navy\\nblue, and forest green, because they were trying to appeal to LPs, not\\nfounders. The YC logo itself is an inside joke: the Viaweb logo had been a\\nwhite V on a red circle, so I made the YC logo a white Y on an orange square.  \\n  \\n[14] YC did become a fund for a couple years starting in 2009, because it was\\ngetting so big I could no longer afford to fund it personally. But after\\nHeroku got bought we had enough money to go back to being self-funded.  \\n  \\n[15] I\\'ve never liked the term \"deal flow,\" because it implies that the number\\nof new startups at any given time is fixed. This is not only false, but it\\'s\\nthe purpose of YC to falsify it, by causing startups to be founded that would\\nnot otherwise have existed.  \\n  \\n[16] She reports that they were all different shapes and sizes, because there\\nwas a run on air conditioners and she had to get whatever she could, but that\\nthey were all heavier than she could carry now.  \\n  \\n[17] Another problem with HN was a bizarre edge case that occurs when you both\\nwrite essays and run a forum. When you run a forum, you\\'re assumed to see if\\nnot every conversation, at least every conversation involving you. And when\\nyou write essays, people post highly imaginative misinterpretations of them on\\nforums. Individually these two phenomena are tedious but bearable, but the\\ncombination is disastrous. You actually have to respond to the\\nmisinterpretations, because the assumption that you\\'re present in the\\nconversation means that not responding to any sufficiently upvoted\\nmisinterpretation reads as a tacit admission that it\\'s correct. But that in\\nturn encourages more; anyone who wants to pick a fight with you senses that\\nnow is their chance.  \\n  \\n[18] The worst thing about leaving YC was not working with Jessica anymore.\\nWe\\'d been working on YC almost the whole time we\\'d known each other, and we\\'d\\nneither tried nor wanted to separate it from our personal lives, so leaving\\nwas like pulling up a deeply rooted tree.  \\n  \\n[19] One way to get more precise about the concept of invented vs discovered\\nis to talk about space aliens. Any sufficiently advanced alien civilization\\nwould certainly know about the Pythagorean theorem, for example. I believe,\\nthough with less certainty, that they would also know about the Lisp in\\nMcCarthy\\'s 1960 paper.  \\n  \\nBut if so there\\'s no reason to suppose that this is the limit of the language\\nthat might be known to them. Presumably aliens need numbers and errors and I/O\\ntoo. So it seems likely there exists at least one path out of McCarthy\\'s Lisp\\nalong which discoveredness is preserved.  \\n  \\n  \\n  \\n**Thanks** to Trevor Blackwell, John Collison, Patrick Collison, Daniel\\nGackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj Taggar for\\nreading drafts of this.  \\n  \\n  \\n---  \\n  \\n  \\n\\n* * *  \\n  \\n---\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTML Tag Reader"
      ],
      "metadata": {
        "id": "MquS3Gow7FkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## HTML Tag Reader\n",
        "\n",
        "%%bash\n",
        "wget -e robots=off --no-clobber --page-requisites \\\n",
        "  --html-extension --convert-links --restrict-file-names=windows \\\n",
        "  --domains docs.ray.io --no-parent --accept=html \\\n",
        "  -P data/ https://docs.ray.io/en/master/ray-overview/installation.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pssv5QMo6F2r",
        "outputId": "e857747a-b9d2-45f3-cb7b-1a5384717c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both --no-clobber and --convert-links were specified, only --convert-links will be used.\n",
            "--2024-10-03 19:43:29--  https://docs.ray.io/en/master/ray-overview/installation.html\n",
            "Resolving docs.ray.io (docs.ray.io)... 104.18.0.163, 104.18.1.163, 2606:4700::6812:1a3, ...\n",
            "Connecting to docs.ray.io (docs.ray.io)|104.18.0.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘data/docs.ray.io/en/master/ray-overview/installation.html’\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 14.3M\n",
            "    50K .......... .......... .......... .......... .......... 1.33M\n",
            "   100K .......... .......... .......... .......... ..........  113M\n",
            "   150K .......... .......... .......... .......... .......... 14.7M\n",
            "   200K .......... .......... .......... .......... ..........  169M\n",
            "   250K                                                         261G=0.04s\n",
            "\n",
            "2024-10-03 19:43:31 (5.54 MB/s) - ‘data/docs.ray.io/en/master/ray-overview/installation.html’ saved [256140]\n",
            "\n",
            "FINISHED --2024-10-03 19:43:31--\n",
            "Total wall clock time: 1.3s\n",
            "Downloaded: 1 files, 250K in 0.04s (5.54 MB/s)\n",
            "Converting links in data/docs.ray.io/en/master/ray-overview/installation.html... 898.\n",
            "36-862\n",
            "Converted links in 1 files in 0.006 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.file import HTMLTagReader\n",
        "\n",
        "reader = HTMLTagReader(tag=\"section\", ignore_no_id=True)\n",
        "docs = reader.load_data(\n",
        "    \"data/docs.ray.io/en/master/ray-overview/installation.html\"\n",
        ")\n",
        "for doc in docs:\n",
        "    print(doc.metadata,'\\n', doc,'\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CK0nX2PF6MtQ",
        "outputId": "e7fcfcd8-88d2-40ac-9297-a51057d6a23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tag': 'section', 'tag_id': 'installing-ray', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: f44232cc-67d2-42df-bbea-16bcae984e2c\n",
            "Text: Installing Ray#  Ray currently officially supports x86_64,\n",
            "aarch64 (ARM) for Linux, and Apple silicon (M1) hardware. Ray on\n",
            "Windows is currently in beta. \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'official-releases', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: fcb4eeac-2190-4e10-998b-4294517b792c\n",
            "Text: Official Releases# \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'from-wheels', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: 7d8e3083-5dac-4735-a7cf-d3d922df67d3\n",
            "Text: From Wheels# You can install the latest official version of Ray\n",
            "from PyPI on Linux, Windows, and macOS by choosing the option that\n",
            "best matches your use case. Recommended For machine learning\n",
            "applications pip install -U \"ray[data,train,tune,serve]\"  # For\n",
            "reinforcement learning support, install RLlib instead. # pip install\n",
            "-U \"ray[rllib]\"   For ... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'daily-releases-nightlies', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: a038d93c-d9ec-4c27-9473-af7105e6a8eb\n",
            "Text: Daily Releases (Nightlies)# You can install the nightly Ray\n",
            "wheels via the following links. These daily releases are tested via\n",
            "automated tests but do not go through the full release process. To\n",
            "install these wheels, use the following pip command and wheels: #\n",
            "Clean removal of previous install pip uninstall -y ray # Install Ray\n",
            "with support for ... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'installing-from-a-specific-commit', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: d956e905-73b7-4f37-bfb1-bd941cdff7a6\n",
            "Text: Installing from a specific commit# You can install the Ray\n",
            "wheels of any particular commit on master with the following template.\n",
            "You need to specify the commit hash, Ray version, Operating System,\n",
            "and Python version: pip install https://s3-us-\n",
            "west-2.amazonaws.com/ray-wheels/master/{COMMIT_HASH}/ray-\n",
            "{RAY_VERSION}-{PYTHON_VERSION}-{PYTHON_VERSION... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'm1-mac-apple-silicon-support', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: c9b90248-c4af-4ee0-b2a7-a9cb90e54373\n",
            "Text: M1 Mac (Apple Silicon) Support# Ray supports machines running\n",
            "Apple Silicon (such as M1 macs). Multi-node clusters are untested. To\n",
            "get started with local Ray development: Install miniforge.  wget\n",
            "https://github.com/conda-\n",
            "forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\n",
            "bash Miniforge3-MacOSX-arm64.sh rm Miniforge3-MacOSX-arm6... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'windows-support', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: 0f6c6c73-1c24-46bd-988d-4b79bcfc7617\n",
            "Text: Windows Support# Windows support is in Beta. Ray supports\n",
            "running on Windows with the following caveats (only the first is Ray-\n",
            "specific, the rest are true anywhere Windows is used): Multi-node Ray\n",
            "clusters are untested. Filenames are tricky on Windows and there still\n",
            "may be a few places where Ray assumes UNIX filenames rather than\n",
            "Windows ones. ... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'installing-ray-on-arch-linux', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: 730828a8-9f34-4495-90cc-afd1fe20a90a\n",
            "Text: Installing Ray on Arch Linux# Note: Installing Ray on Arch Linux\n",
            "is not tested by the Project Ray developers. Ray is available on Arch\n",
            "Linux via the Arch User Repository (AUR) as python-ray. You can\n",
            "manually install the package by following the instructions on the Arch\n",
            "Wiki or use an AUR helper like yay (recommended for ease of install)\n",
            "as follo... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'installing-from-conda-forge', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: cb85755e-bfd9-48b3-be67-8d4aa5df93c8\n",
            "Text: Installing From conda-forge# Ray can also be installed as a\n",
            "conda package on Linux and Windows. # also works with mamba conda\n",
            "create -c conda-forge python=3.9 -n ray conda activate ray  # Install\n",
            "Ray with support for the dashboard + cluster launcher conda install -c\n",
            "conda-forge \"ray-default\"  # Install Ray with minimal dependencies #\n",
            "conda insta... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'building-ray-from-source', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: 893cb694-4ac5-41db-ba89-f8d4be2f9c5c\n",
            "Text: Building Ray from Source# Installing from pip should be\n",
            "sufficient for most Ray users. However, should you need to build from\n",
            "source, follow these instructions for building Ray. \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'docker-source-images', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: fe21aa13-394d-442b-ad3b-79e62b9419e5\n",
            "Text: Docker Source Images# Users can pull a Docker image from the\n",
            "rayproject/ray Docker Hub repository. The images include Ray and all\n",
            "required dependencies. It comes with anaconda and various versions of\n",
            "Python. Images are tagged with the format {Ray version}[-{Python\n",
            "version}][-{Platform}]. Ray version tag can be one of the following:\n",
            "Ray version t... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'launch-ray-in-docker', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: 791bcba3-7e5f-4d01-a602-841c9fd856ba\n",
            "Text: Launch Ray in Docker# Start out by launching the deployment\n",
            "container. docker run --shm-size=<shm-size> -t -i rayproject/ray\n",
            "Replace <shm-size> with a limit appropriate for your system, for\n",
            "example 512M or 2G. A good estimate for this is to use roughly 30% of\n",
            "your available memory (this is what Ray uses internally for its Object\n",
            "Store). The -t a... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'test-if-the-installation-succeeded', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: 050f1584-394d-4435-af10-ae87295ea844\n",
            "Text: Test if the installation succeeded# To test if the installation\n",
            "was successful, try running some tests. This assumes that you’ve\n",
            "cloned the git repository. python -m pytest -v\n",
            "python/ray/tests/test_mini.py \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'installed-python-dependencies', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: 500d438d-de98-4b30-ba27-b38fc123c2d0\n",
            "Text: Installed Python dependencies# Our docker images are shipped\n",
            "with pre-installed Python dependencies required for Ray and its\n",
            "libraries. We publish the dependencies that are installed in our ray\n",
            "Docker images for Python 3.9. ray (Python 3.9) Ray version: nightly\n",
            "(d2982b7) adal==1.2.7 aiohttp==3.9.5 aiohttp-cors==0.7.0\n",
            "aiosignal==1.3.1 anaconda-an... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'install-ray-java-with-maven', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: 6ae81fcf-5780-4174-b17f-3b9898b404d7\n",
            "Text: Install Ray Java with Maven# Note All Ray Java APIs are\n",
            "experimental and only supported by the community. Before installing\n",
            "Ray Java with Maven, you should install Ray Python with pip install -U\n",
            "ray . Note that the versions of Ray Java and Ray Python must match.\n",
            "Note that nightly Ray python wheels are also required if you want to\n",
            "install Ray Jav... \n",
            "\n",
            "\n",
            "{'tag': 'section', 'tag_id': 'install-ray-c', 'file_path': 'data/docs.ray.io/en/master/ray-overview/installation.html'} \n",
            " Doc ID: d86f5172-63fa-4d6d-803f-6e0b977fe4ac\n",
            "Text: Install Ray C++# Note All Ray C++ APIs are experimental and only\n",
            "supported by the community. You can install and use Ray C++ API as\n",
            "follows. pip install -U ray[cpp]  # Create a Ray C++ project template\n",
            "to start with. ray cpp --generate-bazel-project-template-to ray-\n",
            "template Note If you build Ray from source, remove the build option\n",
            "build --cxxop... \n",
            "\n",
            "\n",
            "Doc ID: f44232cc-67d2-42df-bbea-16bcae984e2c\n",
            "Text: Installing Ray#  Ray currently officially supports x86_64,\n",
            "aarch64 (ARM) for Linux, and Apple silicon (M1) hardware. Ray on\n",
            "Windows is currently in beta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u576XiLB65dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "import os\n",
        "from llama_index.core import Document\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.extractors import TitleExtractor\n",
        "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "SDFLyYOg6VId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwcnpxkV6kBb",
        "outputId": "0c7637b7-a8e1-4cc4-dcdc-5007ae07badc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-03 15:07:27--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-10-03 15:07:27 (5.34 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        TokenTextSplitter(chunk_size=1024, chunk_overlap=100),\n",
        "    ]\n",
        ")\n",
        "nodes = pipeline.run(documents=documents);nodes[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvaesBtD68xz",
        "outputId": "cec1da2a-20a5-4f82-c74b-8edb9b190299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file_path': '/content/data/paul_graham/paul_graham_essay.txt',\n",
              " 'file_name': 'paul_graham_essay.txt',\n",
              " 'file_type': 'text/plain',\n",
              " 'file_size': 75042,\n",
              " 'creation_date': '2024-10-03',\n",
              " 'last_modified_date': '2024-10-03'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## let's add title extractor to the pipeline"
      ],
      "metadata": {
        "id": "Vp2TUPTG7S1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        TokenTextSplitter(chunk_size=1024, chunk_overlap=100),\n",
        "        TitleExtractor(),\n",
        "    ]\n",
        ")\n",
        "nodes = pipeline.run(documents=documents);nodes[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyhsKpyC7JKo",
        "outputId": "b13757af-b415-4071-d6f8-d3aca915a2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  3.80it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file_path': '/content/data/paul_graham/paul_graham_essay.txt',\n",
              " 'file_name': 'paul_graham_essay.txt',\n",
              " 'file_type': 'text/plain',\n",
              " 'file_size': 75042,\n",
              " 'creation_date': '2024-10-03',\n",
              " 'last_modified_date': '2024-10-03',\n",
              " 'document_title': 'The Intersection of Technology, Art, and Philosophy: A Journey through Writing, Programming, and Artificial Intelligence'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        TokenTextSplitter(chunk_size=1024, chunk_overlap=100),\n",
        "        TitleExtractor(),\n",
        "        OpenAIEmbedding(), #creates nodes[0].embedding\n",
        "    ]\n",
        ")\n",
        "nodes = pipeline.run(documents=documents);nodes[0].metadata#, nodes[0].embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aOC2HTf7YOE",
        "outputId": "7fc322cf-1135-4123-b3a3-1e5a9379ba88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file_path': '/content/data/paul_graham/paul_graham_essay.txt',\n",
              " 'file_name': 'paul_graham_essay.txt',\n",
              " 'file_type': 'text/plain',\n",
              " 'file_size': 75042,\n",
              " 'creation_date': '2024-10-03',\n",
              " 'last_modified_date': '2024-10-03',\n",
              " 'document_title': 'The Intersection of Art, Technology, and Programming: A Journey from Short Stories to AI and Fine Arts'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save and load to cache\n",
        "pipeline.cache.persist(\"./llama_cache.json\")\n",
        "new_cache = IngestionCache.from_persist_path(\"./llama_cache.json\")"
      ],
      "metadata": {
        "id": "vG411keT-PVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#uses the premade cache\n",
        "new_pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        TokenTextSplitter(chunk_size=1024, chunk_overlap=100),\n",
        "        TitleExtractor(),\n",
        "        OpenAIEmbedding(), #creates nodes[0].embedding\n",
        "    ],\n",
        "    cache=new_cache,\n",
        ")\n",
        "nodes = pipeline.run(documents=documents)"
      ],
      "metadata": {
        "id": "t6qIK04T-p0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = pipeline.run(documents=documents);nodes[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "23mLb1NcKn0C",
        "outputId": "1e06810a-8fc6-4af6-9bfd-3024e464da04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\\n\\nThough I liked programming, I didn\\'t plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn\\'t much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\\n\\nI couldn\\'t have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\\n\\nAI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading The Moon is a Harsh Mistress, so I don\\'t know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.\\n\\nThere weren\\'t any classes in AI at Cornell then, not even graduate classes, so I started trying to teach'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lpy7xJ74Llnn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MFUaXI9LYFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG using Ingestion Pipeline"
      ],
      "metadata": {
        "id": "GSBSfjjULmJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import qdrant_client\n",
        "\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "\n",
        "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client, collection_name=\"llama_index_vector_store\"\n",
        ")\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        TokenTextSplitter(chunk_size=1024, chunk_overlap=100),\n",
        "        TitleExtractor(),\n",
        "        OpenAIEmbedding(),\n",
        "    ],\n",
        "    cache=new_cache,\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "# Ingest directly into a vector db\n",
        "nodes = pipeline.run(documents=documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLI0g0-FLpG9",
        "outputId": "07f80867-36a8-44d5-a48f-fa94d448e977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(vector_store)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What did paul graham do growing up?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5KKPEpaLxZ0",
        "outputId": "cac3a4c9-6890-4776-9134-21ef67da4246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham skipped a step in the evolution of computers and went straight from batch processing to microcomputers, which made microcomputers seem all the more exciting to him.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4b8AVz7L5bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qfhcYoydMPEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Transformations\n",
        "\n",
        "## Implementing custom transformations is pretty easy.\n",
        "\n",
        "Let's include a transformation that removes special characters from the text before generating embeddings.\n",
        "\n",
        "The primary requirement for transformations is that they should take a list of nodes as input and return a modified list of nodes.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rkq5uju5MO2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NkocMkyHMbUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.schema import TransformComponent\n",
        "import re\n",
        "\n",
        "#can make a CVE extractor that searches the text and adds  CVE URL to the metadata\n",
        "class TextCleaner(TransformComponent):\n",
        "    def __call__(self, nodes, **kwargs):\n",
        "        for node in nodes:\n",
        "            node.text = re.sub(r\"[^0-9A-Za-z ]\", \"\", node.text)\n",
        "        return nodes\n",
        "\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        TokenTextSplitter(chunk_size=1024, chunk_overlap=100),\n",
        "        TextCleaner(),\n",
        "        OpenAIEmbedding(),\n",
        "    ],\n",
        "    cache=new_cache,\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(documents=documents);nodes[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "miINJuC7MRaL",
        "outputId": "04572b82-a34f-4975-f8c3-4227ac1b581b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What I Worked OnFebruary 2021Before college the two main things I worked on outside of school were writing and programming I didnt write essays I wrote what beginning writers were supposed to write then and probably still are short stories My stories were awful They had hardly any plot just characters with strong feelings which I imagined made them deepThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called data processing This was in 9th grade so I was 13 or 14 The school districts 1401 happened to be in the basement of our junior high school and my friend Rich Draves and I got permission to use it It was like a mini Bond villains lair down there with all these alienlooking machines  CPU disk drives printer card reader  sitting up on a raised floor under bright fluorescent lightsThe language we used was an early version of Fortran You had to type programs on punch cards then stack them in the card reader and press a button to load the program into memory and run it The result would ordinarily be to print something on the spectacularly loud printerI was puzzled by the 1401 I couldnt figure out what to do with it And in retrospect theres not much I could have done with it The only form of input to programs was data stored on punched cards and I didnt have any data stored on punched cards The only other option was to do things that didnt rely on any input like calculate approximations of pi but I didnt know enough math to do anything interesting of that type So Im not surprised I cant remember any programs I wrote because they cant have done much My clearest memory is of the moment I learned it was possible for programs not to terminate when one of mine didnt On a machine without timesharing this was a social as well as a technical error as the data center managers expression made clearWith microcomputers everything changed Now you could have a computer sitting right in front of you on a desk that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping 1The first of my friends to get a microcomputer built it himself It was sold as a kit by Heathkit I remember vividly how impressed and envious I felt watching him sitting in front of it typing programs right into the computerComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one a TRS80 in about 1980 The gold standard then was the Apple II but a TRS80 was good enough This was when I really started programming I wrote simple games a program to predict how high my model rockets would fly and a word processor that my father used to write at least one book There was only room in memory for about 2 pages of text so hed write 2 pages at a time and then print them out but it was a lot better than a typewriterThough I liked programming I didnt plan to study it in college In college I was going to study philosophy which sounded much more powerful It seemed to my naive high school self to be the study of the ultimate truths compared to which the things studied in other fields would be mere domain knowledge What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasnt much left for these supposed ultimate truths All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignoredI couldnt have put this into words when I was 18 All I knew at the time was that I kept taking philosophy courses and they kept being boring So I decided to switch to AIAI was in the air in the mid 1980s but there were two things especially that made me want to work on it a novel by Heinlein called The Moon is a Harsh Mistress which featured an intelligent computer called Mike and a PBS documentary that showed Terry Winograd using SHRDLU I havent tried rereading The Moon is a Harsh Mistress so I dont know how well it has aged but when I read it I was drawn entirely into its world It seemed only a matter of time before wed have Mike and when I saw Winograd using SHRDLU it seemed like that time would be a few years at most All you had to do was teach SHRDLU more wordsThere werent any classes in AI at Cornell then not even graduate classes so I started trying to teach'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## END OF https://docs.llamaindex.ai/en/stable/examples/cookbooks/oreilly_course_cookbooks/Module-4/Ingestion_Pipeline/"
      ],
      "metadata": {
        "id": "jjqm4novMzOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oEFiguafM55f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METADATA EXTRACTION\n",
        "#### https://docs.llamaindex.ai/en/stable/examples/cookbooks/oreilly_course_cookbooks/Module-4/Metadata_Extraction/"
      ],
      "metadata": {
        "id": "K_ADL9omM90c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq llama-index\n",
        "!pip install -qqq llama_index-readers-web"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlGLGM3_MvA9",
        "outputId": "cd1653f5-d007-4a02-f438-60966ee25068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.schema import MetadataMode\n",
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "1kZaj2BoNSNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\", max_tokens=512)\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "id": "mnYClBtZOFaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uses QuestionsAnsweredExtractor"
      ],
      "metadata": {
        "id": "eAn0QmaGOa9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.extractors import QuestionsAnsweredExtractor\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "node_parser = TokenTextSplitter(\n",
        "    separator=\" \", chunk_size=256, chunk_overlap=128\n",
        ")\n",
        "\n",
        "question_extractor = QuestionsAnsweredExtractor(\n",
        "    questions=3, llm=llm, metadata_mode=MetadataMode.EMBED\n",
        ")"
      ],
      "metadata": {
        "id": "79wFJ7iGOSr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.readers.web import SimpleWebPageReader\n",
        "reader = SimpleWebPageReader(html_to_text=True)\n",
        "docs = reader.load_data(urls=[\"https://eugeneyan.com/writing/llm-patterns/\"])"
      ],
      "metadata": {
        "id": "k6_rX_pZOq-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bb2NINvJOYag",
        "outputId": "38522f08-2e26-47bf-f053-e84d569cd246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# [eugeneyan](/)\n",
            "\n",
            "  * [Start Here](/start-here/ \"Start Here\")\n",
            "  * [Writing](/writing/ \"Writing\")\n",
            "  * [Speaking](/speaking/ \"Speaking\")\n",
            "  * [Prototyping](/prototyping/ \"Prototyping\")\n",
            "  * [About](/about/ \"About\")\n",
            "\n",
            "# Patterns for Building LLM-based Systems & Products\n",
            "\n",
            "[ [llm](/tag/llm/) [engineering](/tag/engineering/)\n",
            "[production](/tag/production/) [🔥](/tag/🔥/) ]  · 66 min read\n",
            "\n",
            "> Discussions on [HackerNews](https://news.ycombinator.com/item?id=36965993),\n",
            "> [Twitter](https://twitter.com/eugeneyan/status/1686531758701899776), and\n",
            "> [LinkedIn](https://www.linkedin.com/posts/eugeneyan_patterns-for-building-\n",
            "> llm-based-systems-activity-7092300473981927424-_wVo)\n",
            "\n",
            "“There is a large class of problems that are easy to imagine and build demos\n",
            "for, but extremely hard to make products out of. For example, self-driving:\n",
            "It’s easy to demo a car self-driving around a block, but making it into a\n",
            "product takes a decade.” -\n",
            "[Karpathy](https://twitter.com/eugeneyan/status/1672692174704766976)\n",
            "\n",
            "This write-up is about practical patterns for integrating large language\n",
            "models (LLMs) into systems & products. We’ll build on academic research,\n",
            "industry resources, and practitioner know-how, and distill them into key ideas\n",
            "and practices.\n",
            "\n",
            "There are seven key patterns. They’re also organized along the spectrum of\n",
            "improving performance vs. reducing cost/risk, and closer to the data vs.\n",
            "closer to the user.\n",
            "\n",
            "  * Evals: To measure performance\n",
            "  * RAG: To add recent, external knowledge\n",
            "  * Fine-tuning: To get better at specific tasks\n",
            "  * Caching: To reduce latency & cost\n",
            "  * Guardrails: To ensure output quality\n",
            "  * Defensive UX: To anticipate & manage errors gracefully\n",
            "  * Collect user feedback: To build our data flywheel\n",
            "\n",
            "(Also see this addendum on [how to match these LLM patterns to potential\n",
            "problems](/writing/llm-problems/).)\n",
            "\n",
            "![Image](/assets/llm-patterns-og.png)\n",
            "\n",
            "LLM patterns: From data to user, from defensive to offensive (see connections\n",
            "between patterns)\n",
            "\n",
            "## Evals: To measure performance\n",
            "\n",
            "Evaluations are a set of measurements used to assess a model’s performance on\n",
            "a task. They include benchmark data and metrics. From a [HackerNews\n",
            "comment](https://news.ycombinator.com/item?id=36789901):\n",
            "\n",
            "> How important evals are to the team is a major differentiator between folks\n",
            "> rushing out hot garbage and those seriously building products in the space.\n",
            "\n",
            "### Why evals?\n",
            "\n",
            "Evals enable us to measure how well our system or product is doing and detect\n",
            "any regressions. (A system or product can be made up of multiple components\n",
            "such as LLMs, prompt templates, retrieved context, and parameters like\n",
            "temperature.) A representative set of evals takes us a step towards measuring\n",
            "system changes at scale. Without evals, we would be flying blind, or would\n",
            "have to visually inspect LLM outputs with each change.\n",
            "\n",
            "### More about evals\n",
            "\n",
            "**There are many benchmarks in the field of language modeling**. Some notable\n",
            "ones are:\n",
            "\n",
            "  * **[MMLU](https://arxiv.org/abs/2009.03300)** : A set of 57 tasks that span elementary math, US history, computer science, law, and more. To perform well, models must possess extensive world knowledge and problem-solving ability.\n",
            "  * **[EleutherAI Eval](https://github.com/EleutherAI/lm-evaluation-harness)** : Unified framework to test models via zero/few-shot settings on 200 tasks. Incorporates a large number of evals including BigBench, MMLU, etc.\n",
            "  * **[HELM](https://arxiv.org/abs/2211.09110)** : Instead of specific tasks and metrics, HELM offers a comprehensive assessment of LLMs by evaluating them across domains. Metrics include accuracy, calibration, robustness, fairness, bias, toxicity, etc. Tasks include Q&A, information retrieval, summarization, text classification, etc.\n",
            "  * **[AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)** : Automated evaluation framework which measures how often a strong LLM (e.g., GPT-4) prefers the output of one model over a reference model. Metrics include win rate, bias, latency, price, variance, etc. Validated to have high agreement with 20k human annotations.\n",
            "\n",
            "We can group metrics into two categories: context-dependent or context-free.\n",
            "\n",
            "  * **Context-dependent** : These take context into account. They’re often proposed for a specific task; repurposing them for other tasks will require some adjustment.\n",
            "  * **Context-free** : These aren’t tied to the context when evaluating generated output; they only compare the output with the provided gold references. As they’re task agnostic, they’re easier to apply to a wide variety of tasks.\n",
            "\n",
            "To get a better sense of these metrics (and their potential shortfalls), we’ll\n",
            "explore a few of the commonly used metrics such as BLEU, ROUGE, BERTScore, and\n",
            "MoverScore.\n",
            "\n",
            "**[BLEU](https://dl.acm.org/doi/10.3115/1073083.1073135) (Bilingual Evaluation\n",
            "Understudy)** is a precision-based metric: It counts the number of n-grams in\n",
            "the generated output that also show up in the reference, and then divides it\n",
            "by the total number of words in the output. It’s predominantly used in machine\n",
            "translation and remains a popular metric due to its cost-effectiveness.\n",
            "\n",
            "First, precision for various values of \\\\(n\\\\) is computed:\n",
            "\n",
            "\\\\[\\text{precision}_n = \\frac{\\sum_{p \\in \\text{output}} \\sum_{\\text{n-gram}\n",
            "\\in p} \\text{Count}_{\\text{clip}} (\\text{n-gram})}{\\sum_{p \\in \\text{output}}\n",
            "\\sum_{\\text{n-gram} \\in p} \\text{Count}(\\text{n-gram})}\\\\]\n",
            "\n",
            "\\\\(Count_{clip}(\\text{n-gram})\\\\) is clipped by the maximum number of times an\n",
            "n-gram appears in any corresponding reference sentence.\n",
            "\n",
            "\\\\[\\text{Count}_{\\text{clip}}(n\\text{-gram}) = \\min \\left(\\text{matched }\n",
            "n\\text{-gram count}, \\max_{r \\in R} \\left(n\\text{-gram count in }\n",
            "r\\right)\\right)\\\\]\n",
            "\n",
            "Once we’ve computed precision at various \\\\(n\\\\), a final BLEU-N score is\n",
            "computed as the geometric mean of all the \\\\(precision_n\\\\) scores.\n",
            "\n",
            "However, since precision relies solely on n-grams and doesn’t consider the\n",
            "length of the generated output, an output containing just one unigram of a\n",
            "common word (like a stop word) would achieve perfect precision. This can be\n",
            "misleading and encourage outputs that contain fewer words to increase BLEU\n",
            "scores. To counter this, a brevity penalty is added to penalize excessively\n",
            "short sentences.\n",
            "\n",
            "\\\\[BP = \\begin{cases} 1 & \\text{if } |p| > |r| \\\\\\ e^{1-\\frac{|r|}{|p|}} &\n",
            "\\text{otherwise} \\end{cases}\\\\]\n",
            "\n",
            "Thus, the final formula is:\n",
            "\n",
            "\\\\[\\text{BLEU-N} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} W_n\n",
            "\\log(\\text{precision}_n)\\right)\\\\]\n",
            "\n",
            "**[ROUGE](https://aclanthology.org/W04-1013/) (Recall-Oriented Understudy for\n",
            "Gisting Evaluation)**: In contrast to BLEU, ROUGE is recall-oriented. It\n",
            "counts the number of words in the reference that also occur in the output.\n",
            "It’s typically used to assess automatic summarization tasks.\n",
            "\n",
            "There are several ROUGE variants. ROUGE-N is most similar to BLEU in that it\n",
            "also counts the number of matching n-grams between the output and the\n",
            "reference.\n",
            "\n",
            "\\\\[\\text{ROUGE-N} = \\frac{\\sum_{s_r \\in \\text{references}} \\sum_{n\\text{-gram}\n",
            "\\in s_r} \\text{Count}_{\\text{match}} (n\\text{-gram})}{\\sum_{s_r \\in\n",
            "\\text{references}} \\sum_{n\\text{-gram} \\in s_r} \\text{Count}\n",
            "(n\\text{-gram})}\\\\]\n",
            "\n",
            "Other variants include:\n",
            "\n",
            "  * ROUGE-L: This measures the longest common subsequence (LCS) between the output and the reference. It considers sentence-level structure similarity and zeros in on the longest series of co-occurring in-sequence n-grams.\n",
            "  * ROUGE-S: This measures the skip-bigram between the output and reference. Skip-bigrams are pairs of words that maintain their sentence order regardless of the words that might be sandwiched between them.\n",
            "\n",
            "**[BERTScore](https://arxiv.org/abs/1904.09675)** is an embedding-based metric\n",
            "that uses cosine similarity to compare each token or n-gram in the generated\n",
            "output with the reference sentence. There are three components to BERTScore:\n",
            "\n",
            "  * Recall: Average cosine similarity between each token in the reference and its closest match in the generated output.\n",
            "  * Precision: Average cosine similarity between each token in the generated output and its nearest match in the reference.\n",
            "  * F1: Harmonic mean of recall and precision\n",
            "\n",
            "\\\\[Recall_{\\text{BERT}} = \\frac{1}{|r|} \\sum_{i \\in r} \\max_{j \\in p}\n",
            "\\vec{i}^T \\vec{j}, \\quad Precision_{\\text{BERT}} = \\frac{1}{|p|} \\sum_{j \\in\n",
            "p} \\max_{i \\in r} \\vec{i}^T \\vec{j}\\\\] \\\\[\\text{BERTscore} = F_{\\text{BERT}} =\n",
            "\\frac{2 \\cdot P_{\\text{BERT}} \\cdot R_{\\text{BERT}}}{P_{\\text{BERT}} +\n",
            "R_{\\text{BERT}}}\\\\]\n",
            "\n",
            "BERTScore is useful because it can account for synonyms and paraphrasing.\n",
            "Simpler metrics like BLEU and ROUGE can’t do this due to their reliance on\n",
            "exact matches. BERTScore has been shown to have better correlation for tasks\n",
            "such as image captioning and machine translation.\n",
            "\n",
            "**[MoverScore](https://arxiv.org/abs/1909.02622)** also uses contextualized\n",
            "embeddings to compute the distance between tokens in the generated output and\n",
            "reference. But unlike BERTScore, which is based on one-to-one matching (or\n",
            "“hard alignment”) of tokens, MoverScore allows for many-to-one matching (or\n",
            "“soft alignment”).\n",
            "\n",
            "![BERTScore \\(left\\) vs. MoverScore \\(right\\)](/assets/mover-score.jpg)\n",
            "\n",
            "BERTScore (left) vs. MoverScore (right;\n",
            "[source](https://arxiv.org/abs/1909.02622))\n",
            "\n",
            "MoverScore enables the mapping of semantically related words in one sequence\n",
            "to their counterparts in another sequence. It does this by solving a\n",
            "constrained optimization problem that finds the minimum effort to transform\n",
            "one text into another. The idea is to measure the distance that words would\n",
            "have to move to convert one sequence to another.\n",
            "\n",
            "However, there are several pitfalls to using these conventional benchmarks and\n",
            "metrics.\n",
            "\n",
            "First, there’s **poor correlation between these metrics and human judgments.**\n",
            "BLEU, ROUGE, and others have had [negative correlation with how humans\n",
            "evaluate fluency](https://arxiv.org/abs/2008.12009). They also showed moderate\n",
            "to less correlation with human adequacy scores. In particular, BLEU and ROUGE\n",
            "have [low correlation with tasks that require creativity and\n",
            "diversity](https://arxiv.org/abs/2303.16634).\n",
            "\n",
            "Second, these metrics often have **poor adaptability to a wider variety of\n",
            "tasks**. Adopting a metric proposed for one task to another is not always\n",
            "prudent. For example, exact match metrics such as BLEU and ROUGE are a poor\n",
            "fit for tasks like abstractive summarization or dialogue. Since they’re based\n",
            "on n-gram overlap between output and reference, they don’t make sense for a\n",
            "dialogue task where a wide variety of responses are possible. An output can\n",
            "have zero n-gram overlap with the reference but yet be a good response.\n",
            "\n",
            "Third, these metrics have **poor reproducibility**. Even for the same metric,\n",
            "[high variance is reported across different\n",
            "studies](https://arxiv.org/abs/2008.12009), possibly due to variations in\n",
            "human judgment collection or metric parameter settings. Another study of\n",
            "[ROUGE scores](https://aclanthology.org/2023.acl-long.107/) across 2,000\n",
            "studies found that scores were hard to reproduce, difficult to compare, and\n",
            "often incorrect because evals were often conducted with untested, incorrect\n",
            "ROUGE implementations.\n",
            "\n",
            "![Dimensions of model evaluations with ROUGE](/assets/rogue-scores.jpg)\n",
            "\n",
            "Dimensions of model evaluations with ROUGE\n",
            "([source](https://aclanthology.org/2023.acl-long.107/))\n",
            "\n",
            "And even with recent benchmarks such as MMLU, **the same model can get\n",
            "significantly different scores based on the eval implementation**.\n",
            "[Huggingface compared the original MMLU\n",
            "implementation](https://huggingface.co/blog/evaluating-mmlu-leaderboard) with\n",
            "the HELM and EleutherAI implementations and found that the same example could\n",
            "have different prompts across various providers.\n",
            "\n",
            "![Different prompts for the same question across MMLU\n",
            "implementations](/assets/mmlu-prompt.jpg)\n",
            "\n",
            "Different prompts for the same question across MMLU implementations\n",
            "([source](https://huggingface.co/blog/evaluating-mmlu-leaderboard))\n",
            "\n",
            "Furthermore, the evaluation approach differed across all three benchmarks:\n",
            "\n",
            "  * Original MMLU: Compares predicted probabilities on the answers only (A, B, C, D)\n",
            "  * HELM: Uses the next token probabilities from the model and picks the token with the highest probability, even if it’s _not_ one of the options.\n",
            "  * EleutherAI: Computes probability of the full answer sequence (i.e., a letter followed by the answer text) for each answer. Then, pick answer with highest probability.\n",
            "\n",
            "![Different eval for the same question across MMLU\n",
            "implementations](/assets/mmlu-eval.jpg)\n",
            "\n",
            "Different eval for the same question across MMLU implementations\n",
            "([source](https://huggingface.co/blog/evaluating-mmlu-leaderboard))\n",
            "\n",
            "As a result, even for the same eval, both absolute scores and model ranking\n",
            "can fluctuate widely depending on eval implementation. This means that model\n",
            "metrics aren’t truly comparable—even for the same eval—unless the eval’s\n",
            "implementation is identical down to minute details like prompts and\n",
            "tokenization. Similarly, the author of QLoRA found MMLU overly sensitive and\n",
            "concluded: “[do not work with/report or trust MMLU\n",
            "scores](https://twitter.com/Tim_Dettmers/status/1673446047266504704)”.\n",
            "\n",
            "Beyond conventional evals such as those mentioned above, **an emerging trend\n",
            "is to use a strong LLM as a reference-free metric** to evaluate generations\n",
            "from other LLMs. This means we may not need human judgments or gold references\n",
            "for evaluation.\n",
            "\n",
            "**[G-Eval](https://arxiv.org/abs/2303.16634) is a framework that applies\n",
            "LLMs** with Chain-of-Though (CoT) and a form-filling paradigm to **evaluate\n",
            "LLM outputs**. First, they provide a task introduction and evaluation criteria\n",
            "to an LLM and ask it to generate a CoT of evaluation steps. Then, to evaluate\n",
            "coherence in news summarization, they concatenate the prompt, CoT, news\n",
            "article, and summary and ask the LLM to output a score between 1 to 5.\n",
            "Finally, they use the probabilities of the output tokens from the LLM to\n",
            "normalize the score and take their weighted summation as the final result.\n",
            "\n",
            "![Overview of G-Eval](/assets/geval.jpg)\n",
            "\n",
            "Overview of G-Eval ([source](https://arxiv.org/abs/2303.16634))\n",
            "\n",
            "They found that GPT-4 as an evaluator had a high Spearman correlation with\n",
            "human judgments (0.514), outperforming all previous methods. It also\n",
            "outperformed traditional metrics on aspects such as coherence, consistency,\n",
            "fluency, and relevance. On topical chat, it did better than traditional\n",
            "metrics such as ROUGE-L, BLEU-4, and BERTScore across several criteria such as\n",
            "naturalness, coherence, engagingness, and groundedness.\n",
            "\n",
            "**The[Vicuna](https://arxiv.org/abs/2306.05685) paper adopted a similar\n",
            "approach.** They start by defining eight categories (writing, roleplay,\n",
            "extraction, reasoning, math, coding, STEM, and humanities/social science)\n",
            "before developing 10 questions for each category. Next, they generated answers\n",
            "from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. Finally, they\n",
            "asked GPT-4 to rate the quality of the answers based on helpfulness,\n",
            "relevance, accuracy, and detail.\n",
            "\n",
            "Overall, they found that GPT-4 not only provided consistent scores but could\n",
            "also give detailed explanations for those scores. Under the single answer\n",
            "grading paradigm, GPT-4 had higher agreement with humans (85%) than the humans\n",
            "had amongst themselves (81%). This suggests that GPT-4’s judgment aligns\n",
            "closely with the human evaluators.\n",
            "\n",
            "**[QLoRA](https://arxiv.org/abs/2305.14314) also used an LLM to evaluate\n",
            "another LLM’s output.** They asked GPT-4 to rate the performance of various\n",
            "models against gpt-3.5-turbo on the Vicuna benchmark. Given the responses from\n",
            "gpt-3.5-turbo and another model, GPT-4 was prompted to score both out of 10\n",
            "and explain its ratings. They also measured performance via direct comparisons\n",
            "between models, simplifying the task to a three-class rating scheme that\n",
            "included ties.\n",
            "\n",
            "To validate the automated evaluation, they collected human judgments on the\n",
            "Vicuna benchmark. Using Mechanical Turk, they enlisted two annotators for\n",
            "comparisons to gpt-3.5-turbo, and three annotators for pairwise comparisons.\n",
            "They found that human and GPT-4 ranking of models were largely in agreement,\n",
            "with a Spearman rank correlation of 0.55 at the model level. This provides an\n",
            "additional data point suggesting that LLM-based automated evals could be a\n",
            "cost-effective and reasonable alternative to human evals.\n",
            "\n",
            "### How to apply evals?\n",
            "\n",
            "**Building solid evals should be the starting point** for any LLM-based system\n",
            "or product (as well as conventional machine learning systems).\n",
            "\n",
            "Unfortunately, classical metrics such as BLEU and ROUGE don’t make sense for\n",
            "more complex tasks such as abstractive summarization or dialogue. Furthermore,\n",
            "we’ve seen that benchmarks like MMLU (and metrics like ROUGE) are sensitive to\n",
            "how they’re implemented and measured. And to be candid, unless your LLM system\n",
            "is studying for a school exam, using MMLU as an eval [doesn’t quite make\n",
            "sense](https://twitter.com/Tim_Dettmers/status/1680782418335367169).\n",
            "\n",
            "Thus, instead of using off-the-shelf benchmarks, we can **start by collecting\n",
            "a set of task-specific evals** (i.e., prompt, context, expected outputs as\n",
            "references). These evals will then guide prompt engineering, model selection,\n",
            "fine-tuning, and so on. And as we update our systems, we can run these evals\n",
            "to quickly measure improvements or regressions. Think of it as Eval Driven\n",
            "Development (EDD).\n",
            "\n",
            "In addition to the evaluation dataset, we **also need useful metrics**. They\n",
            "help us distill performance changes into a single number that’s comparable\n",
            "across eval runs. And if we can simplify the problem, we can choose metrics\n",
            "that are easier to compute and interpret.\n",
            "\n",
            "The simplest task is probably classification: If we’re using an LLM for\n",
            "classification-like tasks (e.g., toxicity detection, document categorization)\n",
            "or extractive QA without dialogue, we can rely on standard classification\n",
            "metrics such as recall, precision, PRAUC, etc. If our task has no correct\n",
            "answer but we have references (e.g., machine translation, extractive\n",
            "summarization), we can rely on reference metrics based on matching (BLEU,\n",
            "ROUGE) or semantic similarity (BERTScore, MoverScore).\n",
            "\n",
            "However, these metrics may not work for more open-ended tasks such as\n",
            "abstractive summarization, dialogue, and others. But collecting human\n",
            "judgments can be slow and expensive. Thus, we may opt to lean on **automated\n",
            "evaluations via a strong LLM**.\n",
            "\n",
            "Relative to human judgments which are typically noisy (due to differing biases\n",
            "among annotators), LLM judgments tend to be less noisy (as the bias is more\n",
            "systematic) but more biased. Nonetheless, since we’re aware of these biases,\n",
            "we can mitigate them accordingly:\n",
            "\n",
            "  * Position bias: LLMs tend to favor the response in the first position. To mitigate this, we can evaluate the same pair of responses twice while swapping their order. If the same response is preferred in both orders, we mark it as a win; else, it’s a tie.\n",
            "  * Verbosity bias: LLMs tend to favor longer, wordier responses over more concise ones, even if the latter is clearer and of higher quality. A possible solution is to ensure that comparison responses are similar in length.\n",
            "  * Self-enhancement bias: LLMs have a slight bias towards their own answers. [GPT-4 favors itself with a 10% higher win rate while Claude-v1 favors itself with a 25% higher win rate.](https://arxiv.org/abs/2306.05685) To counter this, don’t use the same LLM for evaluation tasks.\n",
            "\n",
            "Another tip: Rather than asking an LLM for a direct evaluation (via giving a\n",
            "score), try giving it a reference and asking for a comparison. This helps with\n",
            "reducing noise.\n",
            "\n",
            "Finally, sometimes the best eval is human eval aka vibe check. (Not to be\n",
            "confused with the poorly named code evaluation benchmark\n",
            "[HumanEval](https://arxiv.org/abs/2107.03374).) As mentioned in the [Latent\n",
            "Space podcast with MosaicML](https://www.latent.space/p/mosaic-mpt-7b#details)\n",
            "(34th minute):\n",
            "\n",
            "> The vibe-based eval cannot be underrated. … One of our evals was just having\n",
            "> a bunch of prompts and watching the answers as the models trained and see if\n",
            "> they change. Honestly, I don’t really believe that any of these eval metrics\n",
            "> capture what we care about. One of our prompts was “suggest games for a\n",
            "> 3-year-old and a 7-year-old to play” and that was a lot more valuable to see\n",
            "> how the answer changed during the course of training. — Jonathan Frankle\n",
            "\n",
            "Also see this [deep dive into evals](/writing/abstractive/) for abstractive\n",
            "summarization. It covers reference, context, and preference-based metrics, and\n",
            "also discusses hallucination detection.\n",
            "\n",
            "## Retrieval-Augmented Generation: To add knowledge\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) fetches relevant data from outside the\n",
            "foundation model and enhances the input with this data, providing richer\n",
            "context to improve output.\n",
            "\n",
            "### Why RAG?\n",
            "\n",
            "RAG helps reduce hallucination by grounding the model on the retrieved\n",
            "context, thus increasing factuality. In addition, it’s cheaper to keep\n",
            "retrieval indices up-to-date than to continuously pre-train an LLM. This cost\n",
            "efficiency makes it easier to provide LLMs with access to recent data via RAG.\n",
            "Finally, if we need to update or remove data such as biased or toxic\n",
            "documents, it’s more straightforward to update the retrieval index (compared\n",
            "to fine-tuning or prompting an LLM not to generate toxic outputs).\n",
            "\n",
            "In short, RAG applies mature and simpler ideas from the field of information\n",
            "retrieval to support LLM generation. In a [recent Sequoia\n",
            "survey](https://www.sequoiacap.com/article/llm-stack-perspective/), 88% of\n",
            "respondents believe that retrieval will be a key component of their stack.\n",
            "\n",
            "### More about RAG\n",
            "\n",
            "Before diving into RAG, it helps to have a basic understanding of text\n",
            "embeddings. (Feel free to skip this section if you’re familiar with the\n",
            "subject.)\n",
            "\n",
            "A text embedding is a **compressed, abstract representation of text data**\n",
            "where text of arbitrary length can be represented as a fixed-size vector of\n",
            "numbers. It’s usually learned from a corpus of text such as Wikipedia. Think\n",
            "of them as a universal encoding for text, where **similar items are close to\n",
            "each other while dissimilar items are farther apart**.\n",
            "\n",
            "A good embedding is one that does well on a downstream task, such as\n",
            "retrieving similar items. Huggingface’s [Massive Text Embedding Benchmark\n",
            "(MTEB)](https://huggingface.co/spaces/mteb/leaderboard) scores various models\n",
            "on diverse tasks such as classification, clustering, retrieval, summarization,\n",
            "etc.\n",
            "\n",
            "Quick note: While we mainly discuss text embeddings here, embeddings can take\n",
            "many modalities. For example, [CLIP](https://arxiv.org/abs/2103.00020) is\n",
            "multimodal and embeds images and text in the same space, allowing us to find\n",
            "images most similar to an input text. We can also [embed products based on\n",
            "user behavior](/writing/search-query-matching/#supervised-techniques-improves-\n",
            "modeling-of-our-desired-event) (e.g., clicks, purchases) or [graph\n",
            "relationships](/writing/search-query-matching/#self-supervised-techniques-no-\n",
            "need-for-labels).\n",
            "\n",
            "**RAG has its roots in open-domain Q &A.** An early [Meta\n",
            "paper](https://arxiv.org/abs/2005.04611) showed that retrieving relevant\n",
            "documents via TF-IDF and providing them as context to a language model (BERT)\n",
            "improved performance on an open-domain QA task. They converted each task into\n",
            "a cloze statement and queried the language model for the missing token.\n",
            "\n",
            "Following that, **[Dense Passage Retrieval\n",
            "(DPR)](https://arxiv.org/abs/2004.04906)** showed that using dense embeddings\n",
            "(instead of a sparse vector space such as TF-IDF) for document retrieval can\n",
            "outperform strong baselines like Lucene BM25 (65.2% vs. 42.9% for top-5\n",
            "accuracy.) They also showed that higher retrieval precision translates to\n",
            "higher end-to-end QA accuracy, highlighting the importance of upstream\n",
            "retrieval.\n",
            "\n",
            "To learn the DPR embedding, they fine-tuned two independent BERT-based\n",
            "encoders on existing question-answer pairs. The passage encoder (\\\\(E_p\\\\))\n",
            "embeds text passages into vectors while the query encoder (\\\\(E_q\\\\)) embeds\n",
            "questions into vectors. The query embedding is then used to retrieve \\\\(k\\\\)\n",
            "passages that are most similar to the question.\n",
            "\n",
            "They trained the encoders so that the dot-product similarity makes a good\n",
            "ranking function, and optimized the loss function as the negative log-\n",
            "likelihood of the positive passage. The DPR embeddings are optimized for\n",
            "maximum inner product between the question and relevant passage vectors. The\n",
            "goal is to learn a vector space such that pairs of questions and their\n",
            "relevant passages are close together.\n",
            "\n",
            "For inference, they embed all passages (via \\\\(E_p\\\\)) and index them in FAISS\n",
            "offline. Then, given a question at query time, they compute the question\n",
            "embedding (via \\\\(E_q\\\\)), retrieve the top \\\\(k\\\\) passages via approximate\n",
            "nearest neighbors, and provide it to the language model (BERT) that outputs\n",
            "the answer to the question.\n",
            "\n",
            "**[Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2005.11401)** ,\n",
            "from which this pattern gets its name, highlighted the downsides of pre-\n",
            "trained LLMs. These include not being able to expand or revise memory, not\n",
            "providing insights into generated output, and hallucinations.\n",
            "\n",
            "To address these downsides, they introduced RAG (aka semi-parametric models).\n",
            "Dense vector retrieval serves as the non-parametric component while a pre-\n",
            "trained LLM acts as the parametric component. They reused the DPR encoders to\n",
            "initialize the retriever and build the document index. For the LLM, they used\n",
            "BART, a 400M parameter seq2seq model.\n",
            "\n",
            "![Overview of Retrieval Augmented Generation](/assets/rag.jpg)\n",
            "\n",
            "Overview of Retrieval Augmented Generation\n",
            "([source](https://arxiv.org/abs/2005.11401))\n",
            "\n",
            "During inference, they concatenate the input with the retrieved document.\n",
            "Then, the LLM generates \\\\(\\text{token}_i\\\\) based on the original input, the\n",
            "retrieved document, and the previous \\\\(i-1\\\\) tokens. For generation, they\n",
            "proposed two approaches that vary in how the retrieved passages are used to\n",
            "generate output.\n",
            "\n",
            "In the first approach, RAG-Sequence, the model uses the same document to\n",
            "generate the complete sequence. Thus, for \\\\(k\\\\) retrieved documents, the\n",
            "generator produces an output for each document. Then, the probability of each\n",
            "output sequence is marginalized (sum the probability of each output sequence\n",
            "in \\\\(k\\\\) and weigh it by the probability of each document being retrieved).\n",
            "Finally, the output sequence with the highest probability is selected.\n",
            "\n",
            "On the other hand, RAG-Token can generate each token based on a _different_\n",
            "document. Given \\\\(k\\\\) retrieved documents, the generator produces a\n",
            "distribution for the next output token for each document before marginalizing\n",
            "(aggregating all the individual token distributions.). The process is then\n",
            "repeated for the next token. This means that, for each token generation, it\n",
            "can retrieve a different set of \\\\(k\\\\) relevant documents based on the\n",
            "original input _and_ previously generated tokens. Thus, documents can have\n",
            "different retrieval probabilities and contribute differently to the next\n",
            "generated token.\n",
            "\n",
            "[**Fusion-in-Decoder (FiD)**](https://arxiv.org/abs/2007.01282) also uses\n",
            "retrieval with generative models for open-domain QA. It supports two methods\n",
            "for retrieval, BM25 (Lucene with default parameters) and DPR. FiD is named for\n",
            "how it performs fusion on the retrieved documents in the decoder only.\n",
            "\n",
            "![Overview of Fusion-in-Decoder](/assets/fid.jpg)\n",
            "\n",
            "Overview of Fusion-in-Decoder ([source](https://arxiv.org/abs/2007.01282))\n",
            "\n",
            "For each retrieved passage, the title and passage are concatenated with the\n",
            "question. These pairs are processed independently in the encoder. They also\n",
            "add special tokens such as `question:`, `title:`, and `context:` before their\n",
            "corresponding sections. The decoder attends over the concatenation of these\n",
            "retrieved passages.\n",
            "\n",
            "Because it processes passages independently in the encoder, it can scale to a\n",
            "large number of passages as it only needs to do self-attention over one\n",
            "context at a time. Thus, compute grows linearly (instead of quadratically)\n",
            "with the number of retrieved passages, making it more scalable than\n",
            "alternatives such as RAG-Token. Then, during decoding, the decoder processes\n",
            "the encoded passages jointly, allowing it to better aggregate context across\n",
            "multiple retrieved passages.\n",
            "\n",
            "[**Retrieval-Enhanced Transformer (RETRO)**](https://arxiv.org/abs/2112.04426)\n",
            "adopts a similar pattern where it combines a frozen BERT retriever, a\n",
            "differentiable encoder, and chunked cross-attention to generate output. What’s\n",
            "different is that RETRO does retrieval throughout the entire pre-training\n",
            "stage, and not just during inference. Furthermore, they fetch relevant\n",
            "documents based on chunks of the input. This allows for finer-grained,\n",
            "repeated retrieval during generation instead of only retrieving once per\n",
            "query.\n",
            "\n",
            "For each input chunk (\\\\(C_u\\\\)), the \\\\(k\\\\) retrieved chunks \\\\(RET(C_u)\\\\)\n",
            "are fed into an encoder. The output is the encoded neighbors \\\\(E^{j}_{u}\\\\)\n",
            "where \\\\(E^{j}_{u} = \\text{Encoder}(\\text{RET}(C_{u})^{j}, H_{u}) \\in\n",
            "\\mathbb{R}^{r \\times d_{0}}\\\\). Here, each chunk encoding is conditioned on\n",
            "\\\\(H_u\\\\) (the intermediate activations) and the activations of chunk\n",
            "\\\\(C_u\\\\) through cross-attention layers. In short, the encoding of the\n",
            "retrieved chunks depends on the attended activation of the input chunk.\n",
            "\\\\(E^{j}_{u}\\\\) is then used to condition the generation of the next chunk.\n",
            "\n",
            "![Overview of RETRO](/assets/retro.jpg)\n",
            "\n",
            "Overview of RETRO ([source](https://arxiv.org/abs/2112.04426))\n",
            "\n",
            "During retrieval, RETRO splits the input sequence into chunks of 64 tokens.\n",
            "Then, it finds text similar to the _previous_ chunk to provide context to the\n",
            "_current_ chunk. The retrieval index consists of two contiguous chunks of\n",
            "tokens, \\\\(N\\\\) and \\\\(F\\\\). The former is the neighbor chunk (64 tokens)\n",
            "which is used to compute the key while the latter is the continuation chunk\n",
            "(64 tokens) in the original document.\n",
            "\n",
            "Retrieval is based on approximate \\\\(k\\\\)-nearest neighbors via \\\\(L_2\\\\)\n",
            "distance (euclidean) on BERT embeddings. (Interesting departure from the usual\n",
            "cosine or dot product similarity.) The retrieval index, built on SCaNN, can\n",
            "query a 2T token database in 10ms.\n",
            "\n",
            "They also demonstrated how to RETRO-fit existing baseline models. By freezing\n",
            "the pre-trained weights and only training the chunked cross-attention and\n",
            "neighbor encoder parameters (< 10% of weights for a 7B model), they can\n",
            "enhance transformers with retrieval while only requiring 6M training sequences\n",
            "(3% of pre-training sequences). RETRO-fitted models were able to surpass the\n",
            "performance of baseline models and achieve performance close to that of RETRO\n",
            "trained from scratch.\n",
            "\n",
            "![Performance from RETRO-fitting a pre-trained model](/assets/retrofit.jpg)\n",
            "\n",
            "Performance from RETRO-fitting a pre-trained model\n",
            "([source](https://arxiv.org/abs/2112.04426))\n",
            "\n",
            "**[Internet-augmented LMs](https://arxiv.org/abs/2203.05115)** proposes using\n",
            "a humble “off-the-shelf” search engine to augment LLMs. First, they retrieve a\n",
            "set of relevant documents via Google Search. Since these retrieved documents\n",
            "tend to be long (average length 2,056 words), they chunk them into paragraphs\n",
            "of six sentences each. Finally, they embed the question and paragraphs via TF-\n",
            "IDF and applied cosine similarity to rank the most relevant paragraphs for\n",
            "each query.\n",
            "\n",
            "![Overview of internet-augmented LLMs](/assets/internet-llm.jpg)\n",
            "\n",
            "Overview of internet-augmented LLMs\n",
            "([source](https://arxiv.org/abs/2203.05115))\n",
            "\n",
            "The retrieved paragraphs are used to condition the LLM via few-shot prompting.\n",
            "They adopt the conventional \\\\(k\\\\)-shot prompting (\\\\(k=15\\\\)) from closed-\n",
            "book QA (only providing question-answer pairs) and extend it with an evidence\n",
            "paragraph, such that each context is an evidence, question, and answer\n",
            "triplet.\n",
            "\n",
            "For the generator, they used Gopher, a 280B parameter model trained on 300B\n",
            "tokens. For each question, they generated four candidate answers based on each\n",
            "of the 50 retrieved paragraphs. Finally, they select the best answer by\n",
            "estimating the answer probability via several methods including direct\n",
            "inference, RAG, noisy channel inference, and Product-of-Experts (PoE). PoE\n",
            "consistently performed the best.\n",
            "\n",
            "RAG has also been **applied to non-QA tasks such as code generation**. While\n",
            "**[CodeT5+](https://arxiv.org/abs/2305.07922)** can be used as a standalone\n",
            "generator, when combined with RAG, it significantly outperforms similar models\n",
            "in code generation.\n",
            "\n",
            "To assess the impact of RAG on code generation, they evaluate the model in\n",
            "three settings:\n",
            "\n",
            "  * Retrieval-based: Fetch the top-1 code sample as the prediction\n",
            "  * Generative-only: Output code based on the decoder only\n",
            "  * Retrieval-augmented: Append top-1 code sample to encoder input before code generation via the decoder.\n",
            "\n",
            "![>Overview of RAG for CodeT5+](/assets/codet5.jpg)\n",
            "\n",
            "Overview of RAG for CodeT5+ ([source](https://arxiv.org/abs/2305.07922))\n",
            "\n",
            "As a qualitative example, they showed that retrieved code provides crucial\n",
            "context (e.g., use `urllib3` for an HTTP request) and guides the generative\n",
            "process towards more correct predictions. In contrast, the generative-only\n",
            "approach returns incorrect output that only captures the concepts of\n",
            "“download” and “compress”.\n",
            "\n",
            "**What if we don’t have relevance judgments for query-passage pairs?** Without\n",
            "them, we would not be able to train the bi-encoders that embed the queries and\n",
            "documents in the same embedding space where relevance is represented by the\n",
            "inner product. **[Hypothetical document embeddings\n",
            "(HyDE)](https://arxiv.org/abs/2212.10496)** suggests a solution.\n",
            "\n",
            "![Overview of HyDE](/assets/hyde.jpg)\n",
            "\n",
            "Overview of HyDE ([source](https://arxiv.org/abs/2212.10496))\n",
            "\n",
            "Given a query, HyDE first prompts an LLM, such as InstructGPT, to generate a\n",
            "hypothetical document. Then, an unsupervised encoder, such as Contriver,\n",
            "encodes the document into an embedding vector. Finally, the inner product is\n",
            "computed between the _hypothetical_ document and the corpus, and the most\n",
            "similar _real_ documents are retrieved.\n",
            "\n",
            "The expectation is that the encoder’s dense bottleneck serves as a lossy\n",
            "compressor and the extraneous, non-factual details are excluded via the\n",
            "embedding. This reframes the relevance modeling problem from a representation\n",
            "learning task to a generation task.\n",
            "\n",
            "### How to apply RAG\n",
            "\n",
            "From experience with [Obsidian-Copilot](/writing/obsidian-copilot/), I’ve\n",
            "found that hybrid retrieval (traditional search index + embedding-based\n",
            "search) works better than either alone. There, I complemented classical\n",
            "retrieval (BM25 via OpenSearch) with semantic search (`e5-small-v2`).\n",
            "\n",
            "Why not embedding-based search only? While it’s great in many instances, there\n",
            "are situations where it falls short, such as:\n",
            "\n",
            "  * Searching for a person or object’s name (e.g., Eugene, Kaptir 2.0)\n",
            "  * Searching for an acronym or phrase (e.g., RAG, RLHF)\n",
            "  * Searching for an ID (e.g., `gpt-3.5-turbo`, `titan-xlarge-v1.01`)\n",
            "\n",
            "But keyword search has its limitations too. It only models simple word\n",
            "frequencies and doesn’t capture semantic or correlation information. Thus, it\n",
            "doesn’t deal well with synonyms or hypernyms (i.e., words that represent a\n",
            "generalization). This is where combining it with semantic search is\n",
            "complementary.\n",
            "\n",
            "In addition, with a conventional search index, we can use metadata to refine\n",
            "results. For example, we can use date filters to prioritize newer documents or\n",
            "narrow our search to a specific time period. And if the search is related to\n",
            "e-commerce, filters on average rating or categories are helpful. Finally,\n",
            "having metadata is handy for downstream ranking, such as prioritizing\n",
            "documents that are cited more, or boosting products by their sales volume.\n",
            "\n",
            "**With regard to embeddings** , the seemingly popular approach is to use\n",
            "[`text-embedding-ada-002`](https://openai.com/blog/new-and-improved-embedding-\n",
            "model). Its benefits include ease of use via an API and not having to maintain\n",
            "our own embedding infra or self-host embedding models. Nonetheless, personal\n",
            "experience and anecdotes from others suggest there are better alternatives for\n",
            "retrieval.\n",
            "\n",
            "The OG embedding approaches include Word2vec and\n",
            "[fastText](https://fasttext.cc). FastText is an open-source, lightweight\n",
            "library that enables users to leverage pre-trained embeddings or train new\n",
            "embedding models. It comes with pre-trained embeddings for 157 languages and\n",
            "is extremely fast, even without a GPU. It’s my go-to for early-stage proof of\n",
            "concepts.\n",
            "\n",
            "Another good baseline is [sentence-\n",
            "transformers](https://github.com/UKPLab/sentence-transformers). It makes it\n",
            "simple to compute embeddings for sentences, paragraphs, and even images. It’s\n",
            "based on workhorse transformers such as BERT and RoBERTa and is available in\n",
            "more than 100 languages.\n",
            "\n",
            "More recently, instructor models have shown SOTA performance. During training,\n",
            "these models prepend the task description to the text. Then, when embedding\n",
            "new text, we simply have to describe the task to get task-specific embeddings.\n",
            "(Not that different from instruction tuning for embedding models IMHO.)\n",
            "\n",
            "An example is the [E5](https://arxiv.org/abs/2212.03533) family of models. For\n",
            "open QA and information retrieval, we simply prepend documents in the index\n",
            "with `passage:`, and prepend queries with `query:`. If the task is symmetric\n",
            "(e.g., semantic similarity, paraphrase retrieval) or if we want to use\n",
            "embeddings as features (e.g., classification, clustering), we just use the\n",
            "`query:` prefix.\n",
            "\n",
            "The [Instructor](https://arxiv.org/abs/2212.09741) model takes it a step\n",
            "further, allowing users to customize the prepended prompt: “Represent the\n",
            "`domain` `task_type` for the `task_objective`:” For example, “Represent the\n",
            "Wikipedia document for retrieval:”. (The domain and task objective are\n",
            "optional). This brings the concept of prompt tuning into the field of text\n",
            "embedding.\n",
            "\n",
            "Finally, as of Aug 1st, the top embedding model on the [MTEB\n",
            "Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) is the\n",
            "[GTE](https://huggingface.co/thenlper/gte-large) family of models by Alibaba\n",
            "DAMO Academy. The top performing model’s size is half of the next best model\n",
            "`e5-large-v2` (0.67GB vs 1.34GB). In 2nd position is `gte-base` with a model\n",
            "size of only 0.22GB and embedding dimension of 768. (H/T\n",
            "[Nirant](https://twitter.com/NirantK).)\n",
            "\n",
            "To retrieve documents with low latency at scale, we use approximate nearest\n",
            "neighbors (ANN). It optimizes for retrieval speed and returns the approximate\n",
            "(instead of exact) top \\\\(k\\\\) most similar neighbors, trading off a little\n",
            "accuracy loss for a large speed up.\n",
            "\n",
            "ANN embedding indices are data structures that let us do ANN searches\n",
            "efficiently. At a high level, they build partitions over the embedding space\n",
            "so we can quickly zoom in on the specific space where the query vector is.\n",
            "Some popular techniques include:\n",
            "\n",
            "  * [Locality Sensitive Hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) (LSH): The core idea is to create hash functions so that similar items are likely to end up in the same hash bucket. By only needing to check the relevant buckets, we can perform ANN queries efficiently.\n",
            "  * [Facebook AI Similarity Search](https://github.com/facebookresearch/faiss) (FAISS): It uses a combination of quantization and indexing for efficient retrieval, supports both CPU and GPU, and can handle billions of vectors due to its efficient use of memory.\n",
            "  * [Hierarchical Navigable Small Worlds](https://github.com/nmslib/hnswlib) (HNSW): Inspired by “six degrees of separation”, it builds a hierarchical graph structure that embodies the small world phenomenon. Here, most nodes can be reached from any other node via a minimum number of hops. This structure allows HNSW to initiate queries from broader, coarser approximations and progressively narrow the search at lower levels.\n",
            "  * [Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/master/scann) (ScaNN): It has a two-step process. First, coarse quantization reduces the search space. Then, fine-grained search is done within the reduced set. Best recall/latency trade-off I’ve seen.\n",
            "\n",
            "When evaluating an ANN index, some factors to consider include:\n",
            "\n",
            "  * Recall: How does it fare against exact nearest neighbors?\n",
            "  * Latency/throughput: How many queries can it handle per second?\n",
            "  * Memory footprint: How much RAM is required to serve an index?\n",
            "  * Ease of adding new items: Can new items be added without having to reindex all documents (LSH) or does the index need to be rebuilt (ScaNN)?\n",
            "\n",
            "No single framework is better than all others in every aspect. Thus, start by\n",
            "defining your functional and non-functional requirements before benchmarking.\n",
            "Personally, I’ve found ScaNN to be outstanding in the recall-latency trade-off\n",
            "(see benchmark graph [here](/writing/real-time-recommendations/#how-to-design-\n",
            "and-implement-an-mvp)).\n",
            "\n",
            "## Fine-tuning: To get better at specific tasks\n",
            "\n",
            "Fine-tuning is the process of taking a pre-trained model (that has already\n",
            "been trained with a vast amount of data) and further refining it on a specific\n",
            "task. The intent is to harness the knowledge that the model has already\n",
            "acquired during its pre-training and apply it to a specific task, usually\n",
            "involving a smaller, task-specific, dataset.\n",
            "\n",
            "The term “fine-tuning” is used loosely and can refer to several concepts such\n",
            "as:\n",
            "\n",
            "  * **Continued pre-training** : With domain-specific data, apply the same pre-training regime (next token prediction, masked language modeling) on the base model.\n",
            "  * **Instruction fine-tuning** : The pre-trained (base) model is fine-tuned on examples of instruction-output pairs to follow instructions, answer questions, be waifu, etc.\n",
            "  * **Single-task fine-tuning** : The pre-trained model is honed for a narrow and specific task such as toxicity detection or summarization, similar to BERT and T5.\n",
            "  * **Reinforcement learning with human feedback (RLHF)** : This combines instruction fine-tuning with reinforcement learning. It requires collecting human preferences (e.g., pairwise comparisons) which are then used to train a reward model. The reward model is then used to further fine-tune the instructed LLM via RL techniques such as proximal policy optimization (PPO).\n",
            "\n",
            "We’ll mainly focus on single-task and instruction fine-tuning here.\n",
            "\n",
            "### Why fine-tuning?\n",
            "\n",
            "Fine-tuning an open LLM is becoming an increasingly viable alternative to\n",
            "using a 3rd-party, cloud-based LLM for several reasons.\n",
            "\n",
            "**Performance & control:** Fine-tuning can improve the performance of an off-\n",
            "the-shelf base model, and may even surpass a 3rd-party LLM. It also provides\n",
            "greater control over LLM behavior, resulting in a more robust system or\n",
            "product. Overall, fine-tuning enables us to build products that are\n",
            "differentiated from simply using 3rd-party or open LLMs.\n",
            "\n",
            "**Modularization:** Single-task fine-tuning lets us to use an army of smaller\n",
            "models that each specialize on their own tasks. Via this setup, a system can\n",
            "be modularized into individual models for tasks like content moderation,\n",
            "extraction, summarization, etc. Also, given that each model only has to focus\n",
            "on a narrow set of tasks, we can get around the alignment tax, where fine-\n",
            "tuning a model on one task reduces performance on other tasks.\n",
            "\n",
            "**Reduced dependencies:** By fine-tuning and hosting our own models, we can\n",
            "reduce legal concerns about proprietary data (e.g., PII, internal documents\n",
            "and code) being exposed to external APIs. It also gets around constraints that\n",
            "come with 3rd-party LLMs such as rate-limiting, high costs, or overly\n",
            "restrictive safety filters. By fine-tuning and hosting our own LLMs, we can\n",
            "ensure data doesn’t leave our network, and can scale throughput as needed.\n",
            "\n",
            "### More about fine-tuning\n",
            "\n",
            "Why do we need to fine-tune a _base_ model? At the risk of oversimplifying,\n",
            "base models are primarily optimized to predict the next word based on the\n",
            "corpus they’re trained on. Hence, they aren’t naturally adept at following\n",
            "instructions or answering questions. When posed a question, they tend to\n",
            "respond with more questions. Thus, we perform instruction fine-tuning so they\n",
            "learn to respond appropriately.\n",
            "\n",
            "However, fine-tuning isn’t without its challenges. First, we **need a\n",
            "significant volume of demonstration data**. For instance, in the [InstructGPT\n",
            "paper](https://arxiv.org/abs/2203.02155), they used 13k instruction-output\n",
            "samples for supervised fine-tuning, 33k output comparisons for reward\n",
            "modeling, and 31k prompts without human labels as input for RLHF.\n",
            "\n",
            "Furthermore, fine-tuning comes with an alignment tax—the process can lead to\n",
            "**lower performance on certain critical tasks**. (There’s no free lunch after\n",
            "all.) The same InstructGPT paper found that RLHF led to performance\n",
            "regressions (relative to the GPT-3 base model) on public NLP tasks like SQuAD,\n",
            "HellaSwag, and WMT 2015 French to English. (A workaround is to have several\n",
            "smaller, specialized models that excel at narrow tasks.)\n",
            "\n",
            "Fine-tuning is similar to the concept of transfer learning. As defined in\n",
            "Wikipedia: “Transfer learning is a technique in machine learning in which\n",
            "knowledge learned from a task is re-used to boost performance on a related\n",
            "task.” Several years ago, transfer learning made it easy for me to apply\n",
            "ResNet models trained on ImageNet to [classify fashion\n",
            "products](/writing/image-categorization-is-now-live/) and [build image\n",
            "search](/writing/image-search-is-now-live/).\n",
            "\n",
            "**[ULMFit](https://arxiv.org/abs/1801.06146)** is one of the earlier papers to\n",
            "apply transfer learning to text. They established the protocol of self-\n",
            "supervised pre-training (on unlabeled data) followed by fine-tuning (on\n",
            "labeled data). They used AWS-LSTM, an LSTM variant with dropout at various\n",
            "gates.\n",
            "\n",
            "![Overview of ULMFit](/assets/ulmfit.jpg)\n",
            "\n",
            "Overview of ULMFit ([source](https://arxiv.org/abs/1801.06146))\n",
            "\n",
            "During pre-training (next word prediction), the model is trained on\n",
            "wikitext-103 which contains 28.6 Wikipedia articles and 103M words. Then,\n",
            "during target task fine-tuning, the LM is fine-tuned with data from the domain\n",
            "of the specific task. Finally, during classifier fine-tuning, the model is\n",
            "augmented with two additional linear blocks and fine-tuned on the target\n",
            "classification tasks which includes sentiment analysis, question\n",
            "classification, and topic classification.\n",
            "\n",
            "Since then, the pre-training followed by fine-tuning paradigm has driven much\n",
            "progress in language modeling. **[Bidirectional Encoder Representations from\n",
            "Transformers (BERT; encoder only)](https://arxiv.org/abs/1810.04805)** was\n",
            "pre-trained on masked language modeling and next sentence prediction on\n",
            "English Wikipedia and BooksCorpus. It was then fine-tuned on task-specific\n",
            "inputs and labels for single-sentence classification, sentence pair\n",
            "classification, single-sentence tagging, and question & answering.\n",
            "\n",
            "![Overview of BERT](/assets/bert.jpg)\n",
            "\n",
            "Overview of BERT ([source](https://arxiv.org/abs/1810.04805))\n",
            "\n",
            "**[Generative Pre-trained Transformers (GPT; decoder only)](https://s3-us-\n",
            "west-2.amazonaws.com/openai-assets/research-covers/language-\n",
            "unsupervised/language_understanding_paper.pdf)** was first pre-trained on\n",
            "BooksCorpus via next token prediction. This was followed by single-task fine-\n",
            "tuning for tasks such as text classification, textual entailment, similarity,\n",
            "and Q&A. Interestingly, they found that including language modeling as an\n",
            "auxiliary objective helped the model generalize and converge faster during\n",
            "training.\n",
            "\n",
            "![Overview of GPT](/assets/gpt.jpg)\n",
            "\n",
            "Overview of GPT ([source](https://s3-us-west-2.amazonaws.com/openai-\n",
            "assets/research-covers/language-unsupervised/language_understanding_paper.pd))\n",
            "\n",
            "**[Text-to-text Transfer Transformer (T5; encoder-\n",
            "decoder)](https://arxiv.org/abs/1910.10683)** was pre-trained on the Colossal\n",
            "Clean Crawled Corpus (C4), a cleaned version of the Common Crawl from April\n",
            "2019. It employed the same denoising objective as BERT, namely masked language\n",
            "modeling. It was then fine-tuned on tasks such as text classification,\n",
            "abstractive summarization, Q&A, and machine translation.\n",
            "\n",
            "![Overview of T5](/assets/t5.jpg)\n",
            "\n",
            "Overview of T5 ([source](https://arxiv.org/abs/1910.10683))\n",
            "\n",
            "But unlike ULMFIt, BERT, and GPT which used different classifier heads for\n",
            "downstream tasks, T5 represented downstream tasks as text-to-text only. For\n",
            "example, a translation task would have input text starting with `Translation\n",
            "English to German:`, while a summarization task might start with `Summarize:`\n",
            "or `TL;DR:`. The prefix essentially became a hyperparameter (first instance of\n",
            "prompt engineering?) This design choice allowed them to use a single fine-\n",
            "tuned model across a variety of downstream tasks.\n",
            "\n",
            "**[InstructGPT](https://arxiv.org/abs/2203.02155)** expanded this idea of\n",
            "single-task fine-tuning to instruction fine-tuning. The base model was GPT-3,\n",
            "pre-trained on internet data including Common Crawl, WebText, Books, and\n",
            "Wikipedia. It then applied supervised fine-tuning on demonstrations of desired\n",
            "behavior (instruction and output). Next, it trained a reward model on the\n",
            "dataset of comparisons. Finally, it optimized the instructed model against the\n",
            "reward model via PPO, with this last stage focusing more on alignment than\n",
            "specific task performance.\n",
            "\n",
            "![Overview of fine-tuning steps in InstructGPT](/assets/instructgpt.jpg)\n",
            "\n",
            "Overview of fine-tuning steps in InstructGPT\n",
            "([source](https://arxiv.org/abs/2203.02155))\n",
            "\n",
            "Next, let’s move from fine-tuned models to fine-tuning techniques.\n",
            "\n",
            "**[Soft prompt tuning](https://arxiv.org/abs/2104.08691)** prepends a\n",
            "trainable tensor to the model’s input embeddings, essentially creating a soft\n",
            "prompt. Unlike discrete text prompts, soft prompts can be learned via\n",
            "backpropagation, meaning they can be fine-tuned to incorporate signals from\n",
            "any number of labeled examples.\n",
            "\n",
            "Next, there’s **[prefix tuning](https://arxiv.org/abs/2101.00190)**. Instead\n",
            "of adding a soft prompt to the model input, it prepends trainable parameters\n",
            "to the hidden states of all transformer blocks. During fine-tuning, the LM’s\n",
            "original parameters are kept frozen while the prefix parameters are updated.\n",
            "\n",
            "![Overview of prefix-tuning](/assets/prefix.jpg)\n",
            "\n",
            "Overview of prefix-tuning ([source](https://arxiv.org/abs/2101.00190))\n",
            "\n",
            "The paper showed that this achieved performance comparable to full fine-tuning\n",
            "despite requiring updates on just 0.1% of parameters. Moreover, in settings\n",
            "with limited data and involved extrapolation to new topics, it outperformed\n",
            "full fine-tuning. One hypothesis is that training fewer parameters helped\n",
            "reduce overfitting on smaller target datasets.\n",
            "\n",
            "There’s also the **[adapter](https://arxiv.org/abs/1902.00751)** technique.\n",
            "This method adds fully connected network layers twice to each transformer\n",
            "block, after the attention layer and after the feed-forward network layer. On\n",
            "GLUE, it’s able to achieve within 0.4% of the performance of full fine-tuning\n",
            "by just adding 3.6% parameters per task.\n",
            "\n",
            "![Overview of adapters](/assets/adapter.jpg)\n",
            "\n",
            "Overview of adapters ([source](https://arxiv.org/abs/1902.00751))\n",
            "\n",
            "**[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685)** is a\n",
            "technique where adapters are designed to be the product of two low-rank\n",
            "matrices. It was inspired by [Aghajanyan et\n",
            "al.](https://arxiv.org/abs/2012.13255) which showed that, when adapting to a\n",
            "specific task, pre-trained language models have a low intrinsic dimension and\n",
            "can still learn efficiently despite a random projection into a smaller\n",
            "subspace. Thus, LoRA hypothesized that weight updates during adaption also\n",
            "have low intrinsic rank.\n",
            "\n",
            "![Overview of LoRA](/assets/lora.jpg)\n",
            "\n",
            "Overview of LoRA ([source](https://arxiv.org/abs/2106.09685))\n",
            "\n",
            "Similar to prefix tuning, they found that LoRA outperformed several baselines\n",
            "including full fine-tuning. Again, the hypothesis is that LoRA, thanks to its\n",
            "reduced rank, provides implicit regularization. In contrast, full fine-tuning,\n",
            "which updates all weights, could be prone to overfitting.\n",
            "\n",
            "**[QLoRA](https://arxiv.org/abs/2305.14314)** builds on the idea of LoRA. But\n",
            "instead of using the full 16-bit model during fine-tuning, it applies a 4-bit\n",
            "quantized model. It introduced several innovations such as 4-bit NormalFloat\n",
            "(to quantize models), double quantization (for additional memory savings), and\n",
            "paged optimizers (that prevent OOM errors by transferring data to CPU RAM when\n",
            "the GPU runs out of memory).\n",
            "\n",
            "![Overview of QLoRA](/assets/qlora.jpg)\n",
            "\n",
            "Overview of QLoRA ([source](https://arxiv.org/abs/2305.14314))\n",
            "\n",
            "As a result, QLoRA reduces the average memory requirements for fine-tuning a\n",
            "65B model from > 780GB memory to a more manageable 48B without degrading\n",
            "runtime or predictive performance compared to a 16-bit fully fine-tuned\n",
            "baseline.\n",
            "\n",
            "(Fun fact: During a meetup with Tim Dettmers, an author of QLoRA, he quipped\n",
            "that double quantization was “a bit of a silly idea but works perfectly.” Hey,\n",
            "if it works, it works.)\n",
            "\n",
            "### How to apply fine-tuning?\n",
            "\n",
            "The first step is to **collect demonstration data/labels**. These could be for\n",
            "straightforward tasks such as document classification, entity extraction, or\n",
            "summarization, or they could be more complex such as Q&A or dialogue. Some\n",
            "ways to collect this data include:\n",
            "\n",
            "  * **Via experts or crowd-sourced human annotators** : While this is expensive and slow, it usually leads to higher-quality data with [good guidelines](/writing/labeling-guidelines/).\n",
            "  * **Via user feedback** : This can be as simple as asking users to select attributes that describe a product, rating LLM responses with thumbs up or down (e.g., ChatGPT), or logging which images users choose to download (e.g., Midjourney).\n",
            "  * **Query larger open models with permissive licenses** : With prompt engineering, we might be able to elicit reasonable demonstration data from a larger model (Falcon 40B Instruct) that can be used to fine-tune a smaller model.\n",
            "  * **Reuse open-source data** : If your task can be framed as a natural language inference (NLI) task, we could fine-tune a model to perform NLI using [MNLI data](https://cims.nyu.edu/~sbowman/multinli/). Then, we can continue fine-tuning the model on internal data to classify inputs as entailment, neutral, or contradiction.\n",
            "\n",
            "Note: Some LLM terms prevent users from using their output to develop other\n",
            "models.\n",
            "\n",
            "  * [OpenAI Terms of Use](https://openai.com/policies/terms-of-use) (Section 2c, iii): You may not use output from the Services to develop models that compete with OpenAI.\n",
            "  * [LLaMA 2 Community License Agreement](https://ai.meta.com/llama/license/) (Section 1b-v): You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Llama 2 or derivative works thereof).\n",
            "\n",
            "The next step is to **define evaluation metrics**. We’ve discussed this in a\n",
            "previous section.\n",
            "\n",
            "Then, **select a pre-trained model.** There are [several open LLMs with\n",
            "permissive licenses](https://github.com/eugeneyan/open-llms) to choose from.\n",
            "Excluding Llama 2 (since it isn’t fully commercial use), Falcon-40B is known\n",
            "to be the best-performing model. Nonetheless, I’ve found it unwieldy to fine-\n",
            "tune and serve in production given how heavy it is.\n",
            "\n",
            "Instead, I’m inclined to use smaller models like the Falcon-7B. And if we can\n",
            "simplify and frame the task more narrowly, BERT (340M params), RoBERTA (355M\n",
            "params), and BART (406M params) are solid picks for classification and natural\n",
            "language inference tasks. Beyond that, Flan-T5 (770M and 3B variants) is a\n",
            "reliable baseline for translation, abstractive summarization, headline\n",
            "generation, etc.\n",
            "\n",
            "We may also need to **update the model architecture** , such as when the pre-\n",
            "trained model’s architecture doesn’t align with the task. For example, we\n",
            "might need to update the classification heads on BERT or T5 to match our task.\n",
            "Tip: If the task is a simple binary classification task, NLI models can work\n",
            "out of the box. Entailment is mapped to positive, contradiction is mapped to\n",
            "negative, while the neural label can indicate uncertainty.\n",
            "\n",
            "**Then, pick a fine-tuning approach.** LoRA and QLoRA are good places to\n",
            "start. But if your fine-tuning is more intensive, such as continued pre-\n",
            "training on new domain knowledge, you may find full fine-tuning necessary.\n",
            "\n",
            "**Finally, basic hyperparameter tuning.** Generally, most papers focus on\n",
            "learning rate, batch size, and number of epochs (see LoRA, QLoRA). And if\n",
            "we’re using LoRA, we might want to tune the rank parameter (though the QLoRA\n",
            "paper found that different rank and alpha led to similar results). Other\n",
            "hyperparameters include input sequence length, loss type (contrastive loss vs.\n",
            "token match), and data ratios (like the mix of pre-training or demonstration\n",
            "data, or the ratio of positive to negative examples, among others).\n",
            "\n",
            "## Caching: To reduce latency and cost\n",
            "\n",
            "Caching is a technique to store data that has been previously retrieved or\n",
            "computed. This way, future requests for the same data can be served faster. In\n",
            "the space of serving LLM generations, the popularized approach is to cache the\n",
            "LLM response keyed on the embedding of the input request. Then, for each new\n",
            "request, if a semantically similar request is received, we can serve the\n",
            "cached response.\n",
            "\n",
            "For some practitioners, this sounds like “[a disaster waiting to\n",
            "happen.](https://twitter.com/HanchungLee/status/1681146845186363392)” I’m\n",
            "inclined to agree. Thus, I think the key to adopting this pattern is figuring\n",
            "out how to cache safely, instead of solely depending on semantic similarity.\n",
            "\n",
            "### Why caching?\n",
            "\n",
            "Caching can significantly reduce latency for responses that have been served\n",
            "before. In addition, by eliminating the need to compute a response for the\n",
            "same input again and again, we can reduce the number of LLM requests and thus\n",
            "save cost. Also, there are certain use cases that do not support latency on\n",
            "the order of seconds. Thus, pre-computing and caching may be the only way to\n",
            "serve those use cases.\n",
            "\n",
            "### More about caching\n",
            "\n",
            "A cache is a high-speed storage layer that stores a subset of data that’s\n",
            "accessed more frequently. This lets us serve these requests faster via the\n",
            "cache instead of the data’s primary storage (e.g., search index, relational\n",
            "database). Overall, caching enables efficient reuse of previously fetched or\n",
            "computed data. (More about [caching](https://aws.amazon.com/caching/) and\n",
            "[best practices](https://aws.amazon.com/caching/best-practices/).)\n",
            "\n",
            "An example of caching for LLMs is\n",
            "[GPTCache](https://github.com/zilliztech/GPTCache).\n",
            "\n",
            "![Overview of GPTCache](/assets/gptcache.jpg)\n",
            "\n",
            "Overview of GPTCache ([source](https://github.com/zilliztech/GPTCache))\n",
            "\n",
            "When a new request is received:\n",
            "\n",
            "  * Embedding generator: This embeds the request via various models such as OpenAI’s `text-embedding-ada-002`, FastText, Sentence Transformers, and more.\n",
            "  * Similarity evaluator: This computes the similarity of the request via the vector store and then provides a distance metric. The vector store can either be local (FAISS, Hnswlib) or cloud-based. It can also compute similarity via a model.\n",
            "  * Cache storage: If the request is similar, the cached response is fetched and served.\n",
            "  * LLM: If the request isn’t similar enough, it gets passed to the LLM which then generates the result. Finally, the response is served and cached for future use.\n",
            "\n",
            "Redis also shared a [similar\n",
            "example](https://www.youtube.com/live/9VgpXcfJYvw?feature=share&t=1517),\n",
            "mentioning that some teams go as far as precomputing all the queries they\n",
            "anticipate receiving. Then, they set a similarity threshold on which queries\n",
            "are similar enough to warrant a cached response.\n",
            "\n",
            "### How to apply caching?\n",
            "\n",
            "**We should start with having a good understanding of user request patterns**.\n",
            "This allows us to design the cache thoughtfully so it can be applied reliably.\n",
            "\n",
            "First, let’s consider a non-LLM example. Imagine we’re caching product prices\n",
            "for an e-commerce site. During checkout, is it safe to display the (possibly\n",
            "outdated) cached price? Probably not, since the price the customer sees during\n",
            "checkout should be the same as the final amount they’re charged. Caching isn’t\n",
            "appropriate here as we need to ensure consistency for the customer.\n",
            "\n",
            "Now, bringing it back to LLM responses. Imagine we get a request for a summary\n",
            "of “Mission Impossible 2” that’s semantically similar enough to “Mission\n",
            "Impossible 3”. If we’re looking up cache based on semantic similarity, we\n",
            "could serve the wrong response.\n",
            "\n",
            "We also need to **consider if caching is effective for the usage pattern.**\n",
            "One way to quantify this is via the cache hit rate (percentage of requests\n",
            "served directly from the cache). If the usage pattern is uniformly random, the\n",
            "cache would need frequent updates. Thus, the effort to keep the cache up-to-\n",
            "date could negate any benefit a cache has to offer. On the other hand, if the\n",
            "usage follows a power law where a small proportion of unique requests account\n",
            "for the majority of traffic (e.g., search queries, product views), then\n",
            "caching could be an effective strategy.\n",
            "\n",
            "Beyond semantic similarity, we could also explore caching based on:\n",
            "\n",
            "  * **Item IDs:** This applies when we pre-compute [summaries of product reviews](https://www.cnbc.com/2023/06/12/amazon-is-using-generative-ai-to-summarize-product-reviews.html) or generate a summary for an entire movie trilogy.\n",
            "  * **Pairs of Item IDs:** Such as when we generate comparisons between two movies. While this appears to be \\\\(O(N^2)\\\\), in practice, a small number of combinations drive the bulk of traffic, such as comparison between popular movies in a series or genre.\n",
            "  * **Constrained input:** Such as variables like movie genre, director, or lead actor. For example, if a user is looking for movies by a specific director, we could execute a structured query and run it through an LLM to frame the response more eloquently. Another example is [generating code based on drop-down options](https://cheatlayer.com)—if the code has been verified to work, we can cache it for reliable reuse.\n",
            "\n",
            "Also, **caching doesn’t only have to occur on-the-fly.** As Redis shared, we\n",
            "can pre-compute LLM generations offline or asynchronously before serving them.\n",
            "By serving from a cache, we shift the latency from generation (typically\n",
            "seconds) to cache lookup (milliseconds). Pre-computing in batch can also help\n",
            "reduce cost relative to serving in real-time.\n",
            "\n",
            "While the approaches listed here may not be as flexible as semantically\n",
            "caching on natural language inputs, I think it provides a good balance between\n",
            "efficiency and reliability.\n",
            "\n",
            "## Guardrails: To ensure output quality\n",
            "\n",
            "In the context of LLMs, guardrails validate the output of LLMs, ensuring that\n",
            "the output doesn’t just sound good but is also syntactically correct, factual,\n",
            "and free from harmful content. It also includes guarding against adversarial\n",
            "input.\n",
            "\n",
            "### Why guardrails?\n",
            "\n",
            "First, they help ensure that model outputs are reliable and consistent enough\n",
            "to use in production. For example, we may require output to be in a specific\n",
            "JSON schema so that it’s machine-readable, or we need code generated to be\n",
            "executable. Guardrails can help with such syntactic validation.\n",
            "\n",
            "Second, they provide an additional layer of safety and maintain quality\n",
            "control over an LLM’s output. For example, to verify if the content generated\n",
            "is appropriate for serving, we may want to check that the output isn’t\n",
            "harmful, verify it for factual accuracy, or ensure coherence with the context\n",
            "provided.\n",
            "\n",
            "### More about guardrails\n",
            "\n",
            "**One approach is to control the model’s responses via prompts.** For example,\n",
            "Anthropic shared about prompts designed to guide the model toward generating\n",
            "responses that are [helpful, harmless, and\n",
            "honest](https://arxiv.org/abs/2204.05862) (HHH). They found that Python fine-\n",
            "tuning with the HHH prompt led to better performance compared to fine-tuning\n",
            "with RLHF.\n",
            "\n",
            "![Example of HHH prompt](/assets/hhh.jpg)\n",
            "\n",
            "Example of HHH prompt ([source](https://arxiv.org/abs/2204.05862))\n",
            "\n",
            "**A more common approach is to validate the output.** An example is the\n",
            "[Guardrails package](https://github.com/ShreyaR/guardrails). It allows users\n",
            "to add structural, type, and quality requirements on LLM outputs via Pydantic-\n",
            "style validation. And if the check fails, it can trigger corrective action\n",
            "such as filtering on the offending output or regenerating another response.\n",
            "\n",
            "Most of the validation logic is in\n",
            "[`validators.py`](https://github.com/ShreyaR/guardrails/blob/main/guardrails/validators.py).\n",
            "It’s interesting to see how they’re implemented. Broadly speaking, its\n",
            "validators fall into the following categories:\n",
            "\n",
            "  * Single output value validation: This includes ensuring that the output (i) is one of the predefined choices, (ii) has a length within a certain range, (iii) if numeric, falls within an expected range, and (iv) is a complete sentence.\n",
            "  * Syntactic checks: This includes ensuring that generated URLs are valid and reachable, and that Python and SQL code is bug-free.\n",
            "  * Semantic checks: This verifies that the output is aligned with the reference document, or that the extractive summary closely matches the source document. These checks can be done via cosine similarity or fuzzy matching techniques.\n",
            "  * Safety checks: This ensures that the generated output is free of inappropriate language or that the quality of translated text is high.\n",
            "\n",
            "Nvidia’s [NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) follows\n",
            "a similar principle but is designed to guide LLM-based conversational systems.\n",
            "Rather than focusing on syntactic guardrails, it emphasizes semantic ones.\n",
            "This includes ensuring that the assistant steers clear of politically charged\n",
            "topics, provides factually correct information, and can detect jailbreaking\n",
            "attempts.\n",
            "\n",
            "Thus, NeMo’s approach is somewhat different: Instead of using more\n",
            "deterministic checks like verifying if a value exists in a list or inspecting\n",
            "code for syntax errors, NeMo leans heavily on using another LLM to validate\n",
            "outputs (inspired by [SelfCheckGPT](https://arxiv.org/abs/2303.08896)).\n",
            "\n",
            "In their example for fact-checking and preventing hallucination, they ask the\n",
            "LLM itself to check whether the most recent output is consistent with the\n",
            "given context. To fact-check, the LLM is queried if the response is true based\n",
            "on the documents retrieved from the knowledge base. To prevent hallucinations,\n",
            "since there isn’t a knowledge base available, they get the LLM to generate\n",
            "multiple alternative completions which serve as the context. The underlying\n",
            "assumption is that if the LLM produces multiple completions that disagree with\n",
            "one another, the original completion is likely a hallucination.\n",
            "\n",
            "The moderation example follows a similar approach: The response is screened\n",
            "for harmful and unethical content via an LLM. Given the nuance of ethics and\n",
            "harmful content, heuristics and conventional machine learning techniques fall\n",
            "short. Thus, an LLM is required for a deeper understanding of the intent and\n",
            "structure of dialogue.\n",
            "\n",
            "Apart from using guardrails to verify the output of LLMs, we can also\n",
            "**directly steer the output to adhere to a specific grammar.** An example of\n",
            "this is Microsoft’s [Guidance](https://github.com/microsoft/guidance). Unlike\n",
            "Guardrails which [imposes JSON schema via a\n",
            "prompt](https://github.com/ShreyaR/guardrails/blob/main/guardrails/constants.xml#L14),\n",
            "Guidance enforces the schema by injecting tokens that make up the structure.\n",
            "\n",
            "We can think of Guidance as a domain-specific language for LLM interactions\n",
            "and output. It draws inspiration from [Handlebars](https://handlebarsjs.com),\n",
            "a popular templating language used in web applications that empowers users to\n",
            "perform variable interpolation and logical control.\n",
            "\n",
            "However, Guidance sets itself apart from regular templating languages by\n",
            "executing linearly. This means it maintains the order of tokens generated.\n",
            "Thus, by inserting tokens that are part of the structure—instead of relying on\n",
            "the LLM to generate them correctly—Guidance can dictate the specific output\n",
            "format. In their examples, they show how to [generate JSON that’s always\n",
            "valid](https://github.com/microsoft/guidance#guaranteeing-valid-syntax-json-\n",
            "example-notebook), [generate complex output\n",
            "formats](https://github.com/microsoft/guidance#rich-output-structure-example-\n",
            "notebook) with multiple keys, ensure that LLMs [play the right\n",
            "roles](https://github.com/microsoft/guidance#role-based-chat-model-example-\n",
            "notebook), and have [agents interact with each\n",
            "other](https://github.com/microsoft/guidance#agents-notebook).\n",
            "\n",
            "They also introduced a concept called [token\n",
            "healing](https://github.com/microsoft/guidance#token-healing-notebook), a\n",
            "useful feature that helps avoid subtle bugs that occur due to tokenization. In\n",
            "simple terms, it rewinds the generation by one token before the end of the\n",
            "prompt and then restricts the first generated token to have a prefix matching\n",
            "the last token in the prompt. This eliminates the need to fret about token\n",
            "boundaries when crafting prompts.\n",
            "\n",
            "### How to apply guardrails?\n",
            "\n",
            "Though the concept of guardrails for LLMs in industry is still nascent, there\n",
            "are a handful of immediately useful and practical strategies we can consider.\n",
            "\n",
            "**Structural guidance:** Apply guidance whenever possible. It provides direct\n",
            "control over outputs and offers a more precise method to ensure that output\n",
            "conforms to a specific structure or format.\n",
            "\n",
            "**Syntactic guardrails:** These include checking if categorical output is\n",
            "within a set of acceptable choices, or if numeric output is within an expected\n",
            "range. Also, if we generate SQL, these can verify its free from syntax errors\n",
            "and also ensure that all columns in the query match the schema. Ditto for\n",
            "generating code (e.g., Python, JavaScript).\n",
            "\n",
            "**Content safety guardrails:** These verify that the output has no harmful or\n",
            "inappropriate content. It can be as simple as checking against the [List of\n",
            "Dirty, Naughty, Obscene, and Otherwise Bad\n",
            "Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-\n",
            "Bad-Words) or using [profanity detection](https://pypi.org/project/profanity-\n",
            "check/) models. (It’s [common to run moderation classifiers on\n",
            "output](https://twitter.com/goodside/status/1685023251532320768).) More\n",
            "complex and nuanced output can rely on an LLM evaluator.\n",
            "\n",
            "**Semantic/factuality guardrails:** These confirm that the output is\n",
            "semantically relevant to the input. Say we’re generating a two-sentence\n",
            "summary of a movie based on its synopsis. We can validate if the produced\n",
            "summary is semantically similar to the output, or have (another) LLM ascertain\n",
            "if the summary accurately represents the provided synopsis.\n",
            "\n",
            "**Input guardrails:** These limit the types of input the model will respond\n",
            "to, helping to mitigate the risk of the model responding to inappropriate or\n",
            "adversarial prompts which would lead to generating harmful content. For\n",
            "example, you’ll get an error if you ask Midjourney to generate NSFW content.\n",
            "This can be as straightforward as comparing against a list of strings or using\n",
            "a moderation classifier.\n",
            "\n",
            "![An example of an input guardrail on Midjourney](/assets/input-guardrail.jpg)\n",
            "\n",
            "An example of an input guardrail on Midjourney\n",
            "\n",
            "## Defensive UX: To anticipate & handle errors gracefully\n",
            "\n",
            "Defensive UX is a design strategy that acknowledges that bad things, such as\n",
            "inaccuracies or hallucinations, can happen during user interactions with\n",
            "machine learning or LLM-based products. Thus, the intent is to anticipate and\n",
            "manage these in advance, primarily by guiding user behavior, averting misuse,\n",
            "and handling errors gracefully.\n",
            "\n",
            "### Why defensive UX?\n",
            "\n",
            "Machine learning and LLMs aren’t perfect—they can produce inaccurate output.\n",
            "Also, they respond differently to the same input over time, such as search\n",
            "engines displaying varying results due to personalization, or LLMs generating\n",
            "diverse output on more creative, higher temperature, settings. This can\n",
            "violate the principle of consistency which advocates for a consistent UI and\n",
            "predictable behaviors.\n",
            "\n",
            "Defensive UX can help mitigate the above by providing:\n",
            "\n",
            "  * **Increased accessibility** : By helping users understand how ML/LLM features work and their limitations, defensive UX makes it more accessible and user-friendly.\n",
            "  * **Increased trust** : When users see that the feature can handle difficult scenarios gracefully and doesn’t produce harmful output, they’re likely to trust it more.\n",
            "  * **Better UX** : By designing the system and UX to handle ambiguous situations and errors, defensive UX paves the way for a smoother, more enjoyable user experience.\n",
            "\n",
            "### More about defensive UX\n",
            "\n",
            "To learn more about defensive UX, we can look at Human-AI guidelines from\n",
            "Microsoft, Google, and Apple.\n",
            "\n",
            "**Microsoft’s[Guidelines for Human-AI\n",
            "Interaction](https://www.microsoft.com/en-us/research/publication/guidelines-\n",
            "for-human-ai-interaction/)** is based on a survey of 168 potential guidelines.\n",
            "These were collected from internal and external industry sources, academic\n",
            "literature, and public articles. After combining guidelines that were similar,\n",
            "filtering guidelines that were too vague or too specific or not AI-specific,\n",
            "and a round of heuristic evaluation, they narrowed it down to 18 guidelines.\n",
            "\n",
            "![Guidelines for Human-AI interaction across the user journey](/assets/ms-\n",
            "guidelines.jpg)\n",
            "\n",
            "Guidelines for Human-AI interaction across the user journey\n",
            "([source](https://www.microsoft.com/en-us/research/project/guidelines-for-\n",
            "human-ai-interaction/))\n",
            "\n",
            "These guidelines follow a certain style: Each one is a succinct action rule of\n",
            "3 - 10 words, beginning with a verb. Each rule is accompanied by a one-liner\n",
            "that addresses potential ambiguities. They are organized based on their likely\n",
            "application during user interaction:\n",
            "\n",
            "  * Initially: Make clear what the system can do (G1), make clear how well the system can do what it can do (G2)\n",
            "  * During interaction: Time services based on context (G3), mitigate social biases (G6)\n",
            "  * When wrong: Support efficient dismissal (G8), support efficient correction (G9)\n",
            "  * Over time: Learn from user behavior (G13), provide global controls (G17)\n",
            "\n",
            "**Google’s[People + AI Guidebook](https://pair.withgoogle.com/guidebook/)** is\n",
            "rooted in data and insights drawn from Google’s product team and academic\n",
            "research. In contrast to Microsoft’s guidelines which are organized around the\n",
            "user, Google organizes its guidelines into concepts that a developer needs to\n",
            "keep in mind.\n",
            "\n",
            "There are 23 patterns grouped around common questions that come up during the\n",
            "product development process, including:\n",
            "\n",
            "  * How do I get started with human-centered AI: Determine if the AI adds value, invest early in good data practices (e.g., evals)\n",
            "  * How do I onboard users to new AI features: Make it safe to explore, anchor on familiarity, automate in phases\n",
            "  * How do I help users build trust in my product: Set the right expectations, be transparent, automate more when the risk is low.\n",
            "\n",
            "**Apple’s[Human Interface Guidelines for Machine\n",
            "Learning](https://developer.apple.com/design/human-interface-\n",
            "guidelines/machine-learning)** differs from the bottom-up approach of academic\n",
            "literature and user studies. Instead, its primary source is practitioner\n",
            "knowledge and experience. Thus, it doesn’t include many references or data\n",
            "points, but instead focuses on Apple’s longstanding design principles. This\n",
            "results in a unique perspective that distinguishes it from the other two\n",
            "guidelines.\n",
            "\n",
            "The document focuses on how Apple’s design principles can be applied to ML-\n",
            "infused products, emphasizing aspects of UI rather than model functionality.\n",
            "It starts by asking developers to consider the role of ML in their app and\n",
            "work backwards from the user experience. This includes questions such as\n",
            "whether ML is:\n",
            "\n",
            "  * Critical or complementary: For example, Face ID cannot work without ML but the keyboard can still work without QuickType.\n",
            "  * Proactive or reactive: Siri Suggestions are proactive while autocorrect is reactive.\n",
            "  * Dynamic or static: Recommendations are dynamic while object detection in Photos only improves with each iOS release.\n",
            "\n",
            "It then delves into several patterns, split into inputs and outputs of a\n",
            "system. Inputs focus on explicit feedback, implicit feedback, calibration, and\n",
            "corrections. This section guides the design for how AI products request and\n",
            "process user data and interactions. Outputs focus on mistakes, multiple\n",
            "options, confidence, attribution, and limitations. The intent is to ensure the\n",
            "model’s output is presented in a comprehensible and useful manner.\n",
            "\n",
            "The differences between the three guidelines are insightful. Google has more\n",
            "emphasis on considerations for training data and model development, likely due\n",
            "to its engineering-driven culture. Microsoft has more focus on mental models,\n",
            "likely an artifact of the HCI academic study. Lastly, Apple’s approach centers\n",
            "around providing a seamless UX, a focus likely influenced by its cultural\n",
            "values and principles.\n",
            "\n",
            "### How to apply defensive UX?\n",
            "\n",
            "Here are some patterns based on the guidelines above. (Disclaimer: I’m not a\n",
            "designer.)\n",
            "\n",
            "**Set the right expectations.** This principle is consistent across all three\n",
            "guidelines:\n",
            "\n",
            "  * Microsoft: Make clear how well the system can do what it can do (help the user understand how often the AI system may make mistakes)\n",
            "  * Google: Set the right expectations (be transparent with your users about what your AI-powered product can and cannot do)\n",
            "  * Apple: Help people establish realistic expectations (describe the limitation in marketing material or within the feature’s context)\n",
            "\n",
            "This can be as simple as adding a brief disclaimer above AI-generated results,\n",
            "like those of Bard, or highlighting our app’s limitations on its landing page,\n",
            "like how ChatGPT does it.\n",
            "\n",
            "![Example of a disclaimer on Google Bard results \\(Note: The code provided\n",
            "will not work.\\)](/assets/bard-disclaimer.png)\n",
            "\n",
            "Example of a disclaimer on Google Bard results (Note: `nrows` is not a valid\n",
            "argument.)\n",
            "\n",
            "By being transparent about our product’s capabilities and limitations, we help\n",
            "users calibrate their expectations about its functionality and output. While\n",
            "this may cause users to trust it less in the short run, it helps foster trust\n",
            "in the long run—users are less likely to overestimate our product and\n",
            "subsequently face disappointment.\n",
            "\n",
            "**Enable efficient dismissal.** This is explicitly mentioned as Microsoft’s\n",
            "Guideline 8: Support efficient dismissal (make it easy to dismiss or ignore\n",
            "undesired AI system services).\n",
            "\n",
            "For example, if a user is navigating our site and a chatbot pops up asking if\n",
            "they need help, it should be easy for the user to dismiss the chatbot. This\n",
            "ensures the chatbot doesn’t get in the way, especially on devices with smaller\n",
            "screens. Similarly, GitHub Copilot allows users to conveniently ignore its\n",
            "code suggestions by simply continuing to type. While this may reduce usage of\n",
            "the AI feature in the short term, it prevents it from becoming a nuisance and\n",
            "potentially reducing customer satisfaction in the long term.\n",
            "\n",
            "**Provide attribution.** This is listed in all three guidelines:\n",
            "\n",
            "  * Microsoft: Make clear why the system did what it did (enable the user to access an explanation of why the AI system behaved as it did)\n",
            "  * Google: Add context from human sources (help users appraise your recommendations with input from 3rd-party sources)\n",
            "  * Apple: Consider using attributions to help people distinguish among results\n",
            "\n",
            "Citations are becoming an increasingly common design element. Take BingChat\n",
            "for example. When we make a query, it includes citations, usually from\n",
            "reputable sources, in its responses. This not only shows where the information\n",
            "came from, but also allows users to assess the quality of the sources.\n",
            "Similarly, imagine we’re using an LLM to explain why a user might like a\n",
            "product. Alongside the LLM-generated explanation, we could include a quote\n",
            "from an actual review or mention the product rating.\n",
            "\n",
            "Context from experts and the community also enhances user trust. For example,\n",
            "if a user is seeking recommendations for a hiking trail, mentioning that a\n",
            "suggested trail comes highly recommended by the relevant community can go a\n",
            "long way. It not only adds value to the recommendation but also helps users\n",
            "calibrate trust through the human connection.\n",
            "\n",
            "![Example of attribution via social proof](/assets/social-proof.jpg)\n",
            "\n",
            "Example of attribution via social proof\n",
            "([source](https://pair.withgoogle.com/guidebook/patterns))\n",
            "\n",
            "Finally, Apple’s guidelines include popular attributions such as “Because\n",
            "you’ve read non-fiction”, “New books by authors you’ve read”. These\n",
            "descriptors not only personalize the experience but also provide context,\n",
            "enhancing user understanding and trust.\n",
            "\n",
            "**Anchor on familiarity.** When introducing users to a new AI product or\n",
            "feature, it helps to guide them with familiar UX patterns and features. This\n",
            "makes it easier for users to focus on the main task and start to earn customer\n",
            "trust in our new product. Resist the temptation to showcase new and “magical”\n",
            "features via exotic UI elements.\n",
            "\n",
            "Along a similar vein, chat-based features are becoming more common due to\n",
            "ChatGPT’s growing popularity. For example, chat with your docs, chat to query\n",
            "your data, chat to buy groceries. However, I [question whether chat is the\n",
            "right UX](/writing/llm-ux/) for most user experiences—it just takes too much\n",
            "effort relative to the familiar UX of clicking on text and images.\n",
            "\n",
            "Furthermore, increasing user effort leads to higher expectations that are\n",
            "harder to meet. Netflix shared that users have [higher expectations for\n",
            "recommendations](https://slideslive.com/38934788/a-human-perspective-on-\n",
            "algorithmic-similarity?ref=folder-59726) that result from explicit actions\n",
            "such as search. In general, the more effort a user puts in (e.g., chat,\n",
            "search), the higher the expectations they have. Contrast this with lower-\n",
            "effort interactions such as scrolling over recommendations slates or clicking\n",
            "on a product.\n",
            "\n",
            "Thus, while chat offers more flexibility, it also demands more user effort.\n",
            "Moreover, using a chat box is less intuitive as it lacks signifiers on how\n",
            "users can adjust the output. Overall, I think that sticking with a familiar\n",
            "and constrained UI makes it easier for users to navigate our product; chat\n",
            "should only be considered as a secondary or tertiary option.\n",
            "\n",
            "## Collect user feedback: To build our data flywheel\n",
            "\n",
            "Gathering user feedback allows us to learn their preferences. Specific to LLM\n",
            "products, user feedback contributes to building evals, fine-tuning, and\n",
            "guardrails. If we think about it, data—such as corpus for pre-training,\n",
            "expert-crafted demonstrations, human preferences for reward modeling—is one of\n",
            "the few moats for LLM products. Thus, we want to be deliberately thinking\n",
            "about collecting user feedback when designing our UX.\n",
            "\n",
            "Feedback can be explicit or implicit. Explicit feedback is information users\n",
            "provide in response to a request by our product; implicit feedback is\n",
            "information we learn from user interactions without needing users to\n",
            "deliberately provide feedback.\n",
            "\n",
            "### Why collect user feedback\n",
            "\n",
            "User feedback **helps our models improve**. By learning what users like,\n",
            "dislike, or complain about, we can improve our models to better meet their\n",
            "needs. It also allows us to **adapt to individual preferences**.\n",
            "Recommendation systems are a prime example. As users interact with items, we\n",
            "learn what they like and dislike and better cater to their tastes over time.\n",
            "\n",
            "In addition, the feedback loop helps us **evaluate our system’s overall\n",
            "performance**. While evals can help us measure model/system performance, user\n",
            "feedback offers a concrete measure of user satisfaction and product\n",
            "effectiveness.\n",
            "\n",
            "### How to collect user feedback\n",
            "\n",
            "**Make it easy for users to provide feedback.** This is echoed across all\n",
            "three guidelines:\n",
            "\n",
            "  * Microsoft: Encourage granular feedback (enable the user to provide feedback indicating their preferences during regular interaction with the AI system)\n",
            "  * Google: Let users give feedback (give users the opportunity for real-time teaching, feedback, and error correction)\n",
            "  * Apple: Provide actionable information your app can use to improve the content and experience it presents to people\n",
            "\n",
            "ChatGPT is one such example. Users can indicate thumbs up/down on responses,\n",
            "or choose to regenerate a response if it’s really bad or unhelpful. This is\n",
            "useful feedback on human preferences which can then be used to fine-tune LLMs.\n",
            "\n",
            "Midjourney is another good example. After images are generated, users can\n",
            "generate a new set of images (negative feedback), tweak an image by asking for\n",
            "a variation (positive feedback), or upscale and download the image (strong\n",
            "positive feedback). This enables Midjourney to gather rich comparison data on\n",
            "the outputs generated.\n",
            "\n",
            "![>Example of collecting user feedback as part of the\n",
            "UX](/assets/midjourney.jpg)\n",
            "\n",
            "Example of collecting user feedback as part of the UX\n",
            "\n",
            "**Consider implicit feedback too.** Implicit feedback is information that\n",
            "arises as users interact with our product. Unlike the specific responses we\n",
            "get from explicit feedback, implicit feedback can provide a wide range of data\n",
            "on user behavior and preferences.\n",
            "\n",
            "Copilot-like assistants are a prime example. Users indicate whether a\n",
            "suggestion was helpful by either wholly accepting it (strong positive\n",
            "feedback), accepting and making minor tweaks (positive feedback), or ignoring\n",
            "it (neutral/negative feedback). Alternatively, they may update the comment\n",
            "that led to the generated code, suggesting that the initial code generation\n",
            "didn’t meet their needs.\n",
            "\n",
            "Chatbots, such as ChatGPT and BingChat, are another example. How has daily\n",
            "usage changed over time? If the product is sticky, it suggests that users like\n",
            "it. Also, how long is the average conversation? This can be tricky to\n",
            "interpret: Is a longer conversation better because the conversation was\n",
            "engaging and fruitful? Or is it worse because it took the user longer to get\n",
            "what they needed?\n",
            "\n",
            "## Other patterns common in machine learning\n",
            "\n",
            "Apart from the seven patterns above, there are other patterns in machine\n",
            "learning that are also relevant to LLM systems and products. They include:\n",
            "\n",
            "  * [Data flywheel](/writing/more-patterns/#data-flywheel-to-continuously-improve--build-a-moat): Continuous data collection improves the model and leads to a better user experience. This, in turn, promotes more usage which provides more data to further evaluate and fine-tune models, creating a virtuous cycle.\n",
            "  * [Cascade](/writing/more-patterns/#cascade-to-split-a-problem-into-smaller-problems): Rather than assigning a single, complex task to the LLM, we can simplify and break it down so it only has to handle tasks it excels at, such as reasoning or communicating eloquently. RAG is an example of this. Instead of relying on the LLM to retrieve and rank items based on its internal knowledge, we can augment LLMs with external knowledge and focus on applying the LLM’s reasoning abilities.\n",
            "  * [Monitoring](/writing/practical-guide-to-maintaining-machine-learning/#monitor-models-for-misbehaviour-when-retraining): This helps demonstrate the value added by the AI system, or the lack of it. Someone shared an anecdote of running an LLM-based customer support solution in prod for two weeks before discontinuing it—an A/B test showed that losses were 12x more when using an LLM as a substitute for their support team!\n",
            "\n",
            "(Read more about design patterns for [machine learning code](/writing/design-\n",
            "patterns/) and [systems](/writing/more-patterns/).)\n",
            "\n",
            "Also, here’s what others said:\n",
            "\n",
            "> Separation of concerns/task decomposition- having distinct prompts for\n",
            "> distinct subtasks and chaining them together helps w attention and\n",
            "> reliability (hurts latency). We were having trouble specifying a rigid\n",
            "> output structure AND variable response content so we split up the tasks —\n",
            "> [Erick Enriquez](https://twitter.com/generick_ez/status/1681153738822516736)\n",
            "\n",
            "> A few others that will be needed: role based access control: who can access\n",
            "> what; security: if I’m using a DB with an LLM, how do I ensure that I have\n",
            "> the right security guards —\n",
            "> [Krishna](https://twitter.com/ntkris/status/16812092400299991050)\n",
            "\n",
            "> Consistent output format: setting outputs to a standardized format such as\n",
            "> JSON; Tool augmentation: offload tasks to more specialised, proven, reliable\n",
            "> models — [Paul Tune](https://twitter.com/ptuls/status/1681284873741561857)\n",
            "\n",
            "> Security: mitigate cache poisoning, input validation, mitigate prompt\n",
            "> injection, training data provenance, output with non-vulnerable code,\n",
            "> mitigate malicious input aimed at influencing requests used by tools (AI\n",
            "> Agent), mitigate denial of service (stress test llm), to name a few :) —\n",
            "> [Anderson\n",
            "> Darario](https://www.linkedin.com/feed/update/urn:li:activity:7087089908229558272?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7087089908229558272%2C7087224131292684288%29)\n",
            "\n",
            "> Another ux/ui related: incentivize users to provide feedback on generated\n",
            "> answers (implicit or explicit). Implicit could be sth like copilot’s ghost\n",
            "> text style, if accepted with TAB, meaning positive feedback etc. — [Wen\n",
            "> Yang](https://www.linkedin.com/feed/update/urn:li:activity:7087089908229558272?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7087089908229558272%2C7087149792660750336%29)\n",
            "\n",
            "> Great list. I would add consistency checks like self-consistency sampling,\n",
            "> chaining and decomposition of tasks, and the emsembling of multiple model\n",
            "> outputs. Applying each of these almost daily. [Dan\n",
            "> White](https://www.threads.net/@dwhitena/post/Cu3BBaJtoyj/?igshid=OGQ5ZDc2ODk2ZA==)\n",
            "\n",
            "> Guardrails is super relevant for building analytics tools where llm is a\n",
            "> translator from natural to programming language —\n",
            "> [m_voitko](https://www.threads.net/@m_voitko/post/Cu1b4liNwCS/?igshid=OGQ5ZDc2ODk2ZA==)\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "This is the longest post I’ve written by far. If you’re still with me, thank\n",
            "you! I hope you found reading about these patterns helpful, and that the 2x2\n",
            "below makes sense.\n",
            "\n",
            "![LLM patterns across the axis of data to user, and defensive to\n",
            "offensive.](/assets/llm-patterns.png)\n",
            "\n",
            "LLM patterns across the axis of data to user, and defensive to offensive.\n",
            "\n",
            "We’re still so early on the journey towards building LLM-based systems and\n",
            "products. Are there any other key patterns or resources? What have you found\n",
            "useful or not useful? I’d love to hear your experience. **Please[reach\n",
            "out!](https://twitter.com/eugeneyan)**\n",
            "\n",
            "## References\n",
            "\n",
            "Hendrycks, Dan, et al. [“Measuring massive multitask language\n",
            "understanding.”](https://arxiv.org/abs/2009.03300) arXiv preprint\n",
            "arXiv:2009.03300 (2020).\n",
            "\n",
            "Gao, Leo, et al. [“A Framework for Few-Shot Language Model\n",
            "Evaluation.”](https://github.com/EleutherAI/lm-evaluation-harness) v0.0.1,\n",
            "Zenodo, (2021), doi:10.5281/zenodo.5371628.\n",
            "\n",
            "Liang, Percy, et al. [“Holistic evaluation of language\n",
            "models.”](https://arxiv.org/abs/2211.09110) arXiv preprint arXiv:2211.09110\n",
            "(2022).\n",
            "\n",
            "Dubois, Yann, et al. [“AlpacaFarm: A Simulation Framework for Methods That\n",
            "Learn from Human Feedback.”](https://github.com/tatsu-lab/alpaca_eval) (2023)\n",
            "\n",
            "Papineni, Kishore, et al. [“Bleu: a method for automatic evaluation of machine\n",
            "translation.”](https://dl.acm.org/doi/10.3115/1073083.1073135) Proceedings of\n",
            "the 40th annual meeting of the Association for Computational Linguistics.\n",
            "2002.\n",
            "\n",
            "Lin, Chin-Yew. [“Rouge: A package for automatic evaluation of\n",
            "summaries.”](https://aclanthology.org/W04-1013/) Text summarization branches\n",
            "out. 2004.\n",
            "\n",
            "Zhang, Tianyi, et al. [“Bertscore: Evaluating text generation with\n",
            "bert.”](https://arxiv.org/abs/1904.09675) arXiv preprint arXiv:1904.09675\n",
            "(2019).\n",
            "\n",
            "Zhao, Wei, et al. [“MoverScore: Text generation evaluating with contextualized\n",
            "embeddings and earth mover distance.”](https://arxiv.org/abs/1909.02622) arXiv\n",
            "preprint arXiv:1909.02622 (2019).\n",
            "\n",
            "Sai, Ananya B., Akash Kumar Mohankumar, and Mitesh M. Khapra. [“A survey of\n",
            "evaluation metrics used for NLG systems.”](https://arxiv.org/abs/2008.12009)\n",
            "ACM Computing Surveys (CSUR) 55.2 (2022): 1-39.\n",
            "\n",
            "Grusky, Max. [“Rogue Scores.”](https://aclanthology.org/2023.acl-long.107/)\n",
            "Proceedings of the 61st Annual Meeting of the Association for Computational\n",
            "Linguistics (Volume 1: Long Papers). 2023.\n",
            "\n",
            "Liu, Yang, et al. [“Gpteval: Nlg evaluation using gpt-4 with better human\n",
            "alignment.”](https://arxiv.org/abs/2303.16634) arXiv preprint arXiv:2303.16634\n",
            "(2023).\n",
            "\n",
            "Fourrier, Clémentine, et al. [“What’s going on with the Open LLM\n",
            "Leaderboard?”](https://huggingface.co/blog/evaluating-mmlu-leaderboard#whats-\n",
            "going-on-with-the-open-llm-leaderboard) (2023).\n",
            "\n",
            "Zheng, Lianmin, et al. [“Judging LLM-as-a-judge with MT-Bench and Chatbot\n",
            "Arena.”](https://arxiv.org/abs/2306.05685) arXiv preprint arXiv:2306.05685\n",
            "(2023).\n",
            "\n",
            "Dettmers, Tim, et al. [“Qlora: Efficient finetuning of quantized\n",
            "llms.”](https://arxiv.org/abs/2305.14314) arXiv preprint arXiv:2305.14314\n",
            "(2023).\n",
            "\n",
            "Swyx et al. [MPT-7B and The Beginning of\n",
            "Context=Infinity](https://www.latent.space/p/mosaic-mpt-7b#details) (2023).\n",
            "\n",
            "Fradin, Michelle, Reeder, Lauren [“The New Language Model\n",
            "Stack”](https://www.sequoiacap.com/article/llm-stack-perspective/) (2023).\n",
            "\n",
            "Radford, Alec, et al. [“Learning transferable visual models from natural\n",
            "language supervision.”](https://arxiv.org/abs/2103.00020) International\n",
            "conference on machine learning. PMLR, 2021.\n",
            "\n",
            "Yan, Ziyou. [“Search: Query Matching via Lexical, Graph, and Embedding\n",
            "Methods.”](https://eugeneyan.com/writing/search-query-matching/)\n",
            "eugeneyan.com, (2021).\n",
            "\n",
            "Petroni, Fabio, et al. [“How context affects language models’ factual\n",
            "predictions.”](https://arxiv.org/abs/2005.04611) arXiv preprint\n",
            "arXiv:2005.04611 (2020).\n",
            "\n",
            "Karpukhin, Vladimir, et al. [“Dense passage retrieval for open-domain question\n",
            "answering.”](https://arxiv.org/abs/2004.04906) arXiv preprint arXiv:2004.04906\n",
            "(2020).\n",
            "\n",
            "Lewis, Patrick, et al. [“Retrieval-augmented generation for knowledge-\n",
            "intensive nlp tasks.”](https://arxiv.org/abs/2005.11401) Advances in Neural\n",
            "Information Processing Systems 33 (2020): 9459-9474.\n",
            "\n",
            "Izacard, Gautier, and Edouard Grave. [“Leveraging passage retrieval with\n",
            "generative models for open domain question\n",
            "answering.”](https://arxiv.org/abs/2007.01282) arXiv preprint arXiv:2007.01282\n",
            "(2020).\n",
            "\n",
            "Borgeaud, Sebastian, et al. [“Improving language models by retrieving from\n",
            "trillions of tokens.”](https://arxiv.org/abs/2112.04426) International\n",
            "conference on machine learning. PMLR, (2022).\n",
            "\n",
            "Lazaridou, Angeliki, et al. [“Internet-augmented language models through few-\n",
            "shot prompting for open-domain question\n",
            "answering.”](https://arxiv.org/abs/2203.05115) arXiv preprint arXiv:2203.05115\n",
            "(2022).\n",
            "\n",
            "Wang, Yue, et al. [“Codet5+: Open code large language models for code\n",
            "understanding and generation.”](https://arxiv.org/abs/2305.07922) arXiv\n",
            "preprint arXiv:2305.07922 (2023).\n",
            "\n",
            "Gao, Luyu, et al. [“Precise zero-shot dense retrieval without relevance\n",
            "labels.”](https://arxiv.org/abs/2212.10496) arXiv preprint arXiv:2212.10496\n",
            "(2022).\n",
            "\n",
            "Yan, Ziyou. [“Obsidian-Copilot: An Assistant for Writing &\n",
            "Reflecting.”](https://eugeneyan.com/writing/obsidian-copilot/) eugeneyan.com,\n",
            "(2023).\n",
            "\n",
            "Bojanowski, Piotr, et al. [“Enriching word vectors with subword\n",
            "information.”](https://arxiv.org/abs/1607.04606) Transactions of the\n",
            "association for computational linguistics 5 (2017): 135-146.\n",
            "\n",
            "Reimers, Nils, and Iryna Gurevych. [“Making Monolingual Sentence Embeddings\n",
            "Multilingual Using Knowledge Distillation.”](https://arxiv.org/abs/2004.09813)\n",
            "Proceedings of the 2020 Conference on Empirical Methods in Natural Language\n",
            "Processing, Association for Computational Linguistics, (2020).\n",
            "\n",
            "Wang, Liang, et al. [“Text embeddings by weakly-supervised contrastive pre-\n",
            "training.”](https://arxiv.org/abs/2212.03533) arXiv preprint arXiv:2212.03533\n",
            "(2022).\n",
            "\n",
            "Su, Hongjin, et al. [“One embedder, any task: Instruction-finetuned text\n",
            "embeddings.”](https://arxiv.org/abs/2212.09741) arXiv preprint\n",
            "arXiv:2212.09741 (2022).\n",
            "\n",
            "Johnson, Jeff, et al. [“Billion-Scale Similarity Search with\n",
            "GPUs.”](https://arxiv.org/abs/1702.08734) IEEE Transactions on Big Data, vol.\n",
            "7, no. 3, IEEE, 2019, pp. 535–47.\n",
            "\n",
            "Malkov, Yu A., and Dmitry A. Yashunin. [“Efficient and Robust Approximate\n",
            "Nearest Neighbor Search Using Hierarchical Navigable Small World\n",
            "Graphs.”](https://arxiv.org/abs/1603.09320) IEEE Transactions on Pattern\n",
            "Analysis and Machine Intelligence, vol. 42, no. 4, IEEE, 2018, pp. 824–36.\n",
            "\n",
            "Guo, Ruiqi, et al. [“Accelerating Large-Scale Inference with Anisotropic\n",
            "Vector Quantization.”](https://arxiv.org/abs/1908.10396.) International\n",
            "Conference on Machine Learning, (2020)\n",
            "\n",
            "Ouyang, Long, et al. [“Training language models to follow instructions with\n",
            "human feedback.”](https://arxiv.org/abs/2203.02155) Advances in Neural\n",
            "Information Processing Systems 35 (2022): 27730-27744.\n",
            "\n",
            "Howard, Jeremy, and Sebastian Ruder. [“Universal language model fine-tuning\n",
            "for text classification.”](https://arxiv.org/abs/1801.06146) arXiv preprint\n",
            "arXiv:1801.06146 (2018).\n",
            "\n",
            "Devlin, Jacob, et al. [“Bert: Pre-training of deep bidirectional transformers\n",
            "for language understanding.”](https://arxiv.org/abs/1810.04805) arXiv preprint\n",
            "arXiv:1810.04805 (2018).\n",
            "\n",
            "Radford, Alec, et al. [“Improving language understanding with unsupervised\n",
            "learning.”](https://openai.com/research/language-unsupervised) (2018).\n",
            "\n",
            "Raffel, Colin, et al. [“Exploring the limits of transfer learning with a\n",
            "unified text-to-text transformer.”](https://arxiv.org/abs/1910.10683) The\n",
            "Journal of Machine Learning Research 21.1 (2020): 5485-5551.\n",
            "\n",
            "Lester, Brian, Rami Al-Rfou, and Noah Constant. [“The power of scale for\n",
            "parameter-efficient prompt tuning.”](https://arxiv.org/abs/2104.08691) arXiv\n",
            "preprint arXiv:2104.08691 (2021).\n",
            "\n",
            "Li, Xiang Lisa, and Percy Liang. [“Prefix-tuning: Optimizing continuous\n",
            "prompts for generation.”](https://arxiv.org/abs/2101.00190) arXiv preprint\n",
            "arXiv:2101.00190 (2021).\n",
            "\n",
            "Houlsby, Neil, et al. [“Parameter-efficient transfer learning for\n",
            "NLP.”](https://arxiv.org/abs/1902.00751) International Conference on Machine\n",
            "Learning. PMLR, 2019.\n",
            "\n",
            "Hu, Edward J., et al. [“Lora: Low-rank adaptation of large language\n",
            "models.”](https://arxiv.org/abs/2106.09685) arXiv preprint arXiv:2106.09685\n",
            "(2021).\n",
            "\n",
            "Dettmers, Tim, et al. [“Qlora: Efficient finetuning of quantized\n",
            "llms.”](https://arxiv.org/abs/2305.14314) arXiv preprint arXiv:2305.14314\n",
            "(2023).\n",
            "\n",
            "Williams, Adina, et al. [“A Broad-Coverage Challenge Corpus for Sentence\n",
            "Understanding through Inference.”](https://cims.nyu.edu/~sbowman/multinli/)\n",
            "Proceedings of the 2018 Conference of the North American Chapter of the\n",
            "Association for Computational Linguistics: Human Language Technologies, Volume\n",
            "1 (Long Papers), Association for Computational Linguistics, (2018).\n",
            "\n",
            "[GPTCache](https://github.com/zilliztech/GPTCache) (2023).\n",
            "\n",
            "Bai, Yuntao, et al. [“Training a helpful and harmless assistant with\n",
            "reinforcement learning from human\n",
            "feedback.”](https://arxiv.org/abs/2204.05862) arXiv preprint arXiv:2204.05862\n",
            "(2022).\n",
            "\n",
            "[Guardrails](https://github.com/ShreyaR/guardrails) (2023)\n",
            "\n",
            "[NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) (2023)\n",
            "\n",
            "Manakul, Potsawee, Adian Liusie, and Mark JF Gales. [“Selfcheckgpt: Zero-\n",
            "resource black-box hallucination detection for generative large language\n",
            "models.”](https://arxiv.org/abs/2303.08896) arXiv preprint arXiv:2303.08896\n",
            "(2023).\n",
            "\n",
            "[Guidance](https://github.com/microsoft/guidance) (2023).\n",
            "\n",
            "Amershi, Saleema, et al. [“Guidelines for human-AI\n",
            "interaction.”](https://www.microsoft.com/en-\n",
            "us/research/publication/guidelines-for-human-ai-interaction/) Proceedings of\n",
            "the 2019 chi conference on human factors in computing systems. 2019.\n",
            "\n",
            "[People + AI Guidebook](https://pair.withgoogle.com/guidebook/) (2023).\n",
            "\n",
            "[Human Interface Guidelines for Machine\n",
            "Learning](https://developer.apple.com/design/human-interface-\n",
            "guidelines/machine-learning) (2023).\n",
            "\n",
            "Schendel, Zachary A., Faraz Farzin, and Siddhi Sundar. [“A Human Perspective\n",
            "on Algorithmic Similarity.”](https://slideslive.com/38934788/a-human-\n",
            "perspective-on-algorithmic-similarity?ref=folder-59726) Proceedings of the\n",
            "14th ACM Conference on Recommender Systems. 2020.\n",
            "\n",
            "  \n",
            "\n",
            "If you found this useful, please cite this write-up as:\n",
            "\n",
            "> Yan, Ziyou. (Jul 2023). Patterns for Building LLM-based Systems & Products.\n",
            "> eugeneyan.com. https://eugeneyan.com/writing/llm-patterns/.\n",
            "\n",
            "or\n",
            "\n",
            "    \n",
            "    \n",
            "    @article{yan2023llm-patterns,\n",
            "      title   = {Patterns for Building LLM-based Systems & Products},\n",
            "      author  = {Yan, Ziyou},\n",
            "      journal = {eugeneyan.com},\n",
            "      year    = {2023},\n",
            "      month   = {Jul},\n",
            "      url     = {https://eugeneyan.com/writing/llm-patterns/}\n",
            "    }\n",
            "\n",
            "  \n",
            "Share on:\n",
            "\n",
            "![](/assets/icon-twitter.svg)\n",
            "\n",
            "![](/assets/icon-linkedin.svg)\n",
            "\n",
            "![](/assets/icon-facebook.svg)\n",
            "\n",
            "![](/assets/icon-mail.svg)\n",
            "\n",
            "  \n",
            "Browse related tags: [ [llm](/tag/llm/) [engineering](/tag/engineering/)\n",
            "[production](/tag/production/) [🔥](/tag/🔥/) ]\n",
            "\n",
            "[ ![](/assets/icon-search.svg)Search](/search/ \"Search\")\n",
            "\n",
            "[« Obsidian-Copilot: An Assistant for Writing & Reflecting](/writing/obsidian-\n",
            "copilot/) [How to Match LLM Patterns to Problems »](/writing/llm-problems/)\n",
            "\n",
            "* * *\n",
            "\n",
            "Join **8,600+** readers getting updates on machine learning, RecSys, LLMs, and\n",
            "engineering.\n",
            "\n",
            "Get email updates\n",
            "\n",
            "* * *\n",
            "\n",
            "  * ![](/assets/icon-twitter.svg) [Twitter](https://twitter.com/eugeneyan \"Twitter\")\n",
            "  * ![](/assets/icon-linkedin.svg) [LinkedIn](https://www.linkedin.com/in/eugeneyan/ \"Linkedin\")\n",
            "  * ![](/assets/icon-threads.svg) [Threads](https://www.threads.net/@eugeneyan \"Threads\")\n",
            "  * ![](/assets/icon-github.svg) [GitHub](https://github.com/eugeneyan/ \"GitHub\")\n",
            "\n",
            "Eugene Yan designs, builds, and operates machine learning systems that serve\n",
            "customers at scale. He's currently a Senior Applied Scientist at Amazon.\n",
            "Previously, he led machine learning at Lazada (acquired by Alibaba) and a\n",
            "Healthtech Series A. He [writes](/writing/) & [speaks](/speaking/) about\n",
            "machine learning, recommenders, LLMs, and engineering at\n",
            "[eugeneyan.com](https://eugeneyan.com/) and\n",
            "[ApplyingML.com](https://applyingml.com/).\n",
            "\n",
            "© Eugene Yan 2015 - 2024 • [Feedback](/site-feedback/) • [RSS](/rss/)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orig_nodes = node_parser.get_nodes_from_documents(docs)"
      ],
      "metadata": {
        "id": "fP7QiYLLPDrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(orig_nodes[20:28][3].get_content(metadata_mode=\"all\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tgGH4_kTPRif",
        "outputId": "3e5a0cb7-6ad9-46f8-9c7f-ed1f3aed9361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "because evals were often conducted with untested, incorrect\n",
            "ROUGE implementations.\n",
            "\n",
            "![Dimensions of model evaluations with ROUGE](/assets/rogue-scores.jpg)\n",
            "\n",
            "Dimensions of model evaluations with ROUGE\n",
            "([source](https://aclanthology.org/2023.acl-long.107/))\n",
            "\n",
            "And even with recent benchmarks such as MMLU, **the same model can get\n",
            "significantly different scores based on the eval implementation**.\n",
            "[Huggingface compared the original MMLU\n",
            "implementation](https://huggingface.co/blog/evaluating-mmlu-leaderboard) with\n",
            "the HELM and EleutherAI implementations and found that the same example could\n",
            "have different prompts across various providers.\n",
            "\n",
            "![Different prompts for the same question across MMLU\n",
            "implementations](/assets/mmlu-prompt.jpg)\n",
            "\n",
            "Different prompts for the same question across MMLU implementations\n",
            "([source](https://huggingface.co/blog/evaluating-mmlu-leaderboard))\n",
            "\n",
            "Furthermore, the evaluation approach differed across all three benchmarks:\n",
            "\n",
            "  * Original MMLU: Compares predicted probabilities on the answers only (A, B, C, D)\n",
            "  * HELM: Uses the next token probabilities from the model and picks the token with the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_1 = node_parser.get_nodes_from_documents(docs)[20:28]\n",
        "nodes_1 = question_extractor(nodes_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4rIIzKDO1B6",
        "outputId": "4d85e103-0c48-494f-bcc5-c189f0a3287b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  2.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nodes_1[3].get_content(metadata_mode=\"all\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9hwQIL0Phea",
        "outputId": "9683feed-d8e6-4c1f-87b6-d0889e826e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Excerpt from document]\n",
            "questions_this_excerpt_can_answer: 1. How do different implementations of the MMLU benchmark affect the scores of the same model?\n",
            "2. What are the differences in evaluation approaches between the original MMLU benchmark, HELM, and EleutherAI implementations?\n",
            "3. How do varying prompts for the same question impact the evaluation of models in the MMLU benchmark?\n",
            "Excerpt:\n",
            "-----\n",
            "because evals were often conducted with untested, incorrect\n",
            "ROUGE implementations.\n",
            "\n",
            "![Dimensions of model evaluations with ROUGE](/assets/rogue-scores.jpg)\n",
            "\n",
            "Dimensions of model evaluations with ROUGE\n",
            "([source](https://aclanthology.org/2023.acl-long.107/))\n",
            "\n",
            "And even with recent benchmarks such as MMLU, **the same model can get\n",
            "significantly different scores based on the eval implementation**.\n",
            "[Huggingface compared the original MMLU\n",
            "implementation](https://huggingface.co/blog/evaluating-mmlu-leaderboard) with\n",
            "the HELM and EleutherAI implementations and found that the same example could\n",
            "have different prompts across various providers.\n",
            "\n",
            "![Different prompts for the same question across MMLU\n",
            "implementations](/assets/mmlu-prompt.jpg)\n",
            "\n",
            "Different prompts for the same question across MMLU implementations\n",
            "([source](https://huggingface.co/blog/evaluating-mmlu-leaderboard))\n",
            "\n",
            "Furthermore, the evaluation approach differed across all three benchmarks:\n",
            "\n",
            "  * Original MMLU: Compares predicted probabilities on the answers only (A, B, C, D)\n",
            "  * HELM: Uses the next token probabilities from the model and picks the token with the\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Indexes"
      ],
      "metadata": {
        "id": "kVbfWRWvQcC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.response.notebook_utils import (\n",
        "    display_source_node,\n",
        "    display_response,\n",
        ")\n",
        "index0 = VectorStoreIndex(orig_nodes)\n",
        "index1 = VectorStoreIndex(orig_nodes[:20] + nodes_1 + orig_nodes[28:])"
      ],
      "metadata": {
        "id": "-cFuA1cHPaLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Query Engine"
      ],
      "metadata": {
        "id": "lDt3yU_lQtfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine0 = index0.as_query_engine(similarity_top_k=1)\n",
        "query_engine1 = index1.as_query_engine(similarity_top_k=1)"
      ],
      "metadata": {
        "id": "B594oj-XQyR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = (\n",
        "    \"Can you describe metrics for evaluating text generation quality, compare\"\n",
        "    \" them, and tell me about their downsides\"\n",
        ")\n",
        "\n",
        "response0 = query_engine0.query(query_str)\n",
        "response1 = query_engine1.query(query_str)"
      ],
      "metadata": {
        "id": "qmDWwaKbQ3Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Response 0"
      ],
      "metadata": {
        "id": "7349CLfuRVYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_str, '\\n')\n",
        "display_response(\n",
        "    response0, source_length=1000, show_source=True, show_source_metadata=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "CaBzzq1cRR0Z",
        "outputId": "ddeeea67-3d43-41e0-98e7-0074b4bfaa14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you describe metrics for evaluating text generation quality, compare them, and tell me about their downsides \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Final Response:`** Metrics for evaluating text generation quality can be categorized as context-dependent or context-free. Context-dependent metrics consider the context of the task and may need adjustments for different tasks. On the other hand, context-free metrics are task-agnostic and compare the generated output with provided references, making them versatile for various tasks.\n\nSome commonly used metrics include BLEU, ROUGE, BERTScore, and MoverScore. BLEU is a precision-based metric that counts matching n-grams in the generated output and the reference. ROUGE evaluates the overlap of n-grams and word sequences between the generated text and the reference. BERTScore measures the similarity between the model's output and the reference using contextual embeddings. MoverScore assesses the similarity between the generated text and the reference based on the Earth Mover's Distance.\n\nEach metric has its downsides. For example, BLEU may not consider semantic similarity, ROUGE may not capture the overall meaning, BERTScore could be computationally expensive, and MoverScore may require additional computational resources. These downsides highlight the importance of understanding the limitations of each metric when evaluating text generation quality."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Source Node 1/1`**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** 63c32918-94ab-49aa-88ca-19774e78d081<br>**Similarity:** 0.8380886067511155<br>**Text:** GPT-4) prefers the output of one model over a reference model. Metrics include win rate, bias, latency, price, variance, etc. Validated to have high agreement with 20k human annotations.\n\nWe can group metrics into two categories: context-dependent or context-free.\n\n  * **Context-dependent** : These take context into account. They’re often proposed for a specific task; repurposing them for other tasks will require some adjustment.\n  * **Context-free** : These aren’t tied to the context when evaluating generated output; they only compare the output with the provided gold references. As they’re task agnostic, they’re easier to apply to a wide variety of tasks.\n\nTo get a better sense of these metrics (and their potential shortfalls), we’ll\nexplore a few of the commonly used metrics such as BLEU, ROUGE, BERTScore, and\nMoverScore.\n\n**[BLEU](https://dl.acm.org/doi/10.3115/1073083.1073135) (Bilingual Evaluation\nUnderstudy)** is a precision-based metric: It counts the number of n-grams in\nth...<br>**Metadata:** {}<br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Response 1"
      ],
      "metadata": {
        "id": "mBCMPaf3RZUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_str, '\\n')\n",
        "\n",
        "display_response(\n",
        "    response1, source_length=1000, show_source=True, show_source_metadata=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "bi5vrUOXReCW",
        "outputId": "e44aac8c-c846-4c5e-efd1-f89f673716ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you describe metrics for evaluating text generation quality, compare them, and tell me about their downsides \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Final Response:`** Metrics for evaluating text generation quality vary in their effectiveness depending on the task requirements. Some metrics, like BLEU and ROUGE, are commonly used but may not be suitable for tasks that demand creativity and diversity. These metrics rely on n-gram overlap between the generated text and a reference, which can limit their applicability in tasks such as abstractive summarization or dialogue generation where responses can vary widely. Additionally, these metrics may exhibit poor adaptability to different tasks and have issues with reproducibility, leading to challenges in reliably evaluating the quality of text generation models."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Source Node 1/1`**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** cfa77c3a-8b7e-41a4-9fd3-13eb1044cc8c<br>**Similarity:** 0.8512667214191415<br>**Text:** with tasks that require creativity and\ndiversity](https://arxiv.org/abs/2303.16634).\n\nSecond, these metrics often have **poor adaptability to a wider variety of\ntasks**. Adopting a metric proposed for one task to another is not always\nprudent. For example, exact match metrics such as BLEU and ROUGE are a poor\nfit for tasks like abstractive summarization or dialogue. Since they’re based\non n-gram overlap between output and reference, they don’t make sense for a\ndialogue task where a wide variety of responses are possible. An output can\nhave zero n-gram overlap with the reference but yet be a good response.\n\nThird, these metrics have **poor reproducibility**. Even for the same metric,\n[high variance is reported across different\nstudies](https://arxiv.org/abs/2008.12009), possibly due to variations in\nhuman judgment collection or metric parameter settings. Another study of\n[ROUGE scores](https://aclanthology.org/2023.acl-long.107/) across 2,000\nstudies found that scores were hard to re...<br>**Metadata:** {'questions_this_excerpt_can_answer': '1. How do existing metrics for evaluating natural language generation models perform when tasks require creativity and diversity?\\n2. Why is it not always appropriate to adopt a metric proposed for one task to evaluate performance on another task in natural language generation?\\n3. What challenges are associated with the reproducibility of metrics used to evaluate natural language generation models, and how do these challenges impact the reliability of research findings in the field?'}<br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zTbNeVjcWUSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata Extraction Usage Pattern\n",
        "\n",
        "You can use LLMs to automate metadata extraction with our Metadata Extractor modules.\n",
        "\n",
        "Our metadata extractor modules include the following \"feature extractors\":\n",
        "\n",
        "- `SummaryExtractor` - automatically extracts a summary over a set of Nodes\n",
        "- `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer\n",
        "- `TitleExtractor` - extracts a title over the context of each Node\n",
        "- `EntityExtractor` - extracts entities (i.e. names of places, people, things) mentioned in the content of each Node\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Then you can chain the Metadata Extractors with our node parser."
      ],
      "metadata": {
        "id": "p951vytzWNHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor/"
      ],
      "metadata": {
        "id": "6jonTcLwWD_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_index\n",
        "dir(llama_index.core.extractors.metadata_extractors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp0mfmj4Y0zI",
        "outputId": "994f577c-7965-4e35-9bd4-1bc0f3357e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Any',\n",
              " 'BaseExtractor',\n",
              " 'BaseNode',\n",
              " 'BasePydanticProgram',\n",
              " 'Callable',\n",
              " 'DEFAULT_ENTITY_MAP',\n",
              " 'DEFAULT_ENTITY_MODEL',\n",
              " 'DEFAULT_EXTRACT_TEMPLATE_STR',\n",
              " 'DEFAULT_KEYWORD_EXTRACT_TEMPLATE',\n",
              " 'DEFAULT_NUM_WORKERS',\n",
              " 'DEFAULT_QUESTION_GEN_TMPL',\n",
              " 'DEFAULT_SUMMARY_EXTRACT_TEMPLATE',\n",
              " 'DEFAULT_TITLE_COMBINE_TEMPLATE',\n",
              " 'DEFAULT_TITLE_NODE_TEMPLATE',\n",
              " 'Dict',\n",
              " 'Field',\n",
              " 'KeywordExtractor',\n",
              " 'LLM',\n",
              " 'List',\n",
              " 'Optional',\n",
              " 'PrivateAttr',\n",
              " 'PromptTemplate',\n",
              " 'PydanticProgramExtractor',\n",
              " 'QuestionsAnsweredExtractor',\n",
              " 'Sequence',\n",
              " 'SerializeAsAny',\n",
              " 'Settings',\n",
              " 'SummaryExtractor',\n",
              " 'TextNode',\n",
              " 'TitleExtractor',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " 'add_class_name',\n",
              " 'cast',\n",
              " 'run_jobs']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.extractors.metadata_extractors import KeywordExtractor #not in the docs\n",
        "kw_extractor = KeywordExtractor(llm=llm, keywords=10) #CAN ALSO PUT IN A KEYWORD EXTRACTION TEMPLATE FOR THE LLM"
      ],
      "metadata": {
        "id": "jGf63PXjYNU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.extractors import (\n",
        "    TitleExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        ")\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(\n",
        "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
        ")\n",
        "title_extractor = TitleExtractor(nodes=5)\n",
        "qa_extractor = QuestionsAnsweredExtractor(questions=3)\n",
        "\n",
        "# assume documents are defined -> extract nodes\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[text_splitter, title_extractor, qa_extractor, kw_extractor]\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(\n",
        "    documents=documents,\n",
        "    in_place=True,\n",
        "    show_progress=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "8c7d8045f03c46ffaa8765c94b4869d3",
            "042b585ff9a14c898e9b2bedd2c4d683",
            "60c4b511fe834196acac931f1ee54f67",
            "0fb964c801f24ea4b498a98c9e6bd76f",
            "6f7ac72aac864051975e676b691b9ac9",
            "98d48d53a3d845d1b935ebb8053bd2e5",
            "385b64f888b6441bb6a99945b2aef2f9",
            "e4f87aaefa274a53ace2d5fe5dc350c1",
            "9dcfd07f6a884282a60a5d9efcf9fdd8",
            "4911987bc2834b44bcf785a0c4952887",
            "b3974eb774924a1388792b13e7395015"
          ]
        },
        "id": "JSEYqT_xWaV1",
        "outputId": "42f07915-859c-42b2-a147-affbbf15801a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c7d8045f03c46ffaa8765c94b4869d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  4.48it/s]\n",
            "100%|██████████| 46/46 [00:19<00:00,  2.34it/s]\n",
            "100%|██████████| 46/46 [00:09<00:00,  5.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6cmVU21WD7-",
        "outputId": "18a656e4-e68a-4083-f79b-cbc941e10046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file_path': '/content/data/paul_graham/paul_graham_essay.txt',\n",
              " 'file_name': 'paul_graham_essay.txt',\n",
              " 'file_type': 'text/plain',\n",
              " 'file_size': 75042,\n",
              " 'creation_date': '2024-10-03',\n",
              " 'last_modified_date': '2024-10-03',\n",
              " 'document_title': '\"From Punch Cards to AI: A Journey in Programming, Philosophy, and the Illusion of Artificial Intelligence\"',\n",
              " 'questions_this_excerpt_can_answer': \"1. How did the transition from using punch cards on the IBM 1401 to microcomputers impact the author's programming experience and capabilities?\\n2. What were some of the challenges the author faced when working with the IBM 1401, and how did these limitations shape their early programming endeavors?\\n3. How did the author's early experiences with writing short stories and programming in their youth influence their later work and perspectives on technology and creativity?\",\n",
              " 'excerpt_keywords': 'punch cards, IBM 1401, programming, Fortran, microcomputers, data processing, creativity, technology, limitations, early experiences'}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQ7mNcbEWD4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZ4GcFUwWDz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYTxU70TWDvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JPRr9-GpWDqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zxQvCU7_WDjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdGLRNhmWDaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hnz9MdBaTRqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Metadata Using PydanticProgramExtractor"
      ],
      "metadata": {
        "id": "u8-53zdpTRXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class NodeMetadata(BaseModel):\n",
        "    \"\"\"Node metadata.\"\"\"\n",
        "\n",
        "    entities: List[str] = Field(\n",
        "        ..., description=\"Unique entities in this text chunk.\"\n",
        "    )\n",
        "    summary: str = Field(\n",
        "        ..., description=\"A concise summary of this text chunk.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "yH9fyJbvTZln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.program.openai import OpenAIPydanticProgram\n",
        "from llama_index.core.extractors import PydanticProgramExtractor\n",
        "\n",
        "EXTRACT_TEMPLATE_STR = \"\"\"\\\n",
        "Here is the content of the section:\n",
        "----------------\n",
        "{context_str}\n",
        "----------------\n",
        "Given the contextual information, extract out a {class_name} object.\\\n",
        "\"\"\"\n",
        "\n",
        "openai_program = OpenAIPydanticProgram.from_defaults(\n",
        "    output_cls=NodeMetadata,\n",
        "    prompt_template_str=\"{input}\",\n",
        "    extract_template_str=EXTRACT_TEMPLATE_STR,\n",
        ")\n",
        "\n",
        "metadata_extractor = PydanticProgramExtractor(\n",
        "    program=openai_program, input_key=\"input\", show_progress=True\n",
        ")\n",
        "\n",
        "extract_metadata = metadata_extractor.extract(orig_nodes[0:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw64ZfmSTfiC",
        "outputId": "dfd483a6-431c-447f-a91d-083662a1d44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsj4h1qWRgpm",
        "outputId": "c0dfe26e-2975-42bd-9e45-54eabaa5160e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entities': ['eugeneyan', 'llm', 'engineering', 'production'],\n",
              "  'summary': 'Patterns for Building LLM-based Systems & Products'}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_nodes = metadata_extractor.process_nodes(orig_nodes[0:1]);metadata_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vJku2sjUQc5",
        "outputId": "6e4d2b18-0914-485e-cd34-d42f4a7db505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TextNode(id_='a64e2060-42d5-48c9-b719-74c70f6a8b36', embedding=None, metadata={'entities': ['eugeneyan', 'llm', 'engineering', 'production'], 'summary': 'Patterns for Building LLM-based Systems & Products - Discussions on HackerNews, Twitter, and LinkedIn. Content includes discussions on self-driving technology.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='https://eugeneyan.com/writing/llm-patterns/', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='31bdcee06733c3d18c370cb5296006308d8e200cf59ec243654906e320b0825a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f8852ce8-d054-4c27-a20b-a51b0f3fa140', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='993e43bb060cf2f183f894f8dec6708eadcac2b7d2760a94916dc82c24255acc')}, text='# [eugeneyan](/)\\n\\n  * [Start Here](/start-here/ \"Start Here\")\\n  * [Writing](/writing/ \"Writing\")\\n  * [Speaking](/speaking/ \"Speaking\")\\n  * [Prototyping](/prototyping/ \"Prototyping\")\\n  * [About](/about/ \"About\")\\n\\n# Patterns for Building LLM-based Systems & Products\\n\\n[ [llm](/tag/llm/) [engineering](/tag/engineering/)\\n[production](/tag/production/) [🔥](/tag/🔥/) ]  · 66 min read\\n\\n> Discussions on [HackerNews](https://news.ycombinator.com/item?id=36965993),\\n> [Twitter](https://twitter.com/eugeneyan/status/1686531758701899776), and\\n> [LinkedIn](https://www.linkedin.com/posts/eugeneyan_patterns-for-building-\\n> llm-based-systems-activity-7092300473981927424-_wVo)\\n\\n“There is a large class of problems that are easy to imagine and build demos\\nfor, but extremely hard to make products out of. For example, self-driving:\\nIt’s easy to demo a', mimetype='text/plain', start_char_idx=0, end_char_idx=838, text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_nodes[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqas15i0TzQE",
        "outputId": "a33c5bd1-a3ce-4818-b669-4b3702ee888c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'entities': ['eugeneyan', 'llm', 'engineering', 'production'],\n",
              " 'summary': 'Patterns for Building LLM-based Systems & Products - Discussions on HackerNews, Twitter, and LinkedIn. Content includes discussions on self-driving technology.'}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ckjJUURIUX-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}