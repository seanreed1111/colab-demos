{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from torch import tensor\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI,Trace_ELBO\n",
    "from pyro.infer.autoguide  import AutoMultivariateNormal, AutoNormal, init_to_mean\n",
    "from pyro.optim import ClippedAdam\n",
    "\n",
    "assert pyro.__version__.startswith('1.8')\n",
    "pyro.set_rng_seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Set matplotlib settings\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - Student Awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/seanreed1111/datasets/master/count-regression-datasets/competition_awards_data.csv\")\n",
    "df.columns = ['award_count', 'math_score']\n",
    "X = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=X, x='award_count', palette='colorblind');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit this into a Poisson Distribution. Poisson should have equal mean and variance. \n",
    "\n",
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['award_count'].mean(), X['award_count'].var() # Variance larger than the mean. Overdispersed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample distribution where sample Poisson mean = data mean\n",
    "samples_1 = dist.Poisson(X['award_count'].mean()).sample(tensor([200]))\n",
    "sns.distplot(samples_1.numpy(), kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample distribution where sample Poisson mean = data variance\n",
    "samples_2 = dist.Poisson(X['award_count'].var()).sample(tensor([200])) \n",
    "sns.displot(samples_2.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, comparing the actual data with the above two poisson samples distributions above, it appears our data is both zero inflated AND overdispersed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a Poisson model(GLM) with 'log(award_count) ~ math_score' \n",
    "data = X[['math_score']]\n",
    "target = X['award_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generalized Linear Model with a Poisson distribution and log link.\n",
    "from sklearn.linear_model import PoissonRegressor \n",
    "\n",
    "reg = PoissonRegressor().fit(data.values, target.values)\n",
    "\n",
    "# these are MLE estimates of parameters we expect to recover\n",
    "print(reg.intercept_)\n",
    "print(reg.coef_)\n",
    "reg.score(data.values.reshape(-1,1), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Statsmodels\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "statsmod = smf.poisson(formula='award_count ~ math_score', data=X)\n",
    "result = statsmod.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bayesian Regression with SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data and target to torch tensors\n",
    "data = tensor(data.values, dtype=torch.float)\n",
    "target = tensor(target.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Size(), target.Size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can build a full PyTorch Model using torch.nn (not always needed)\n",
    "from torch import nn\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "\n",
    "# need to pass the priors for all models as parameters to the object.\n",
    "class BayesianPoissonRegression(PyroModule):\n",
    "    def __init__(self, in_features, out_features = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](in_features, out_features)\n",
    "        if bias:\n",
    "          self.linear.bias = PyroSample(dist.Normal(0., 5.).expand([out_features]).to_event(1))\n",
    "        self.linear.weight = PyroSample(dist.Normal(0., 0.05).expand([out_features, in_features]).to_event(2))\n",
    "\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        rate = self.linear(x).squeeze(-1).exp() #we are using log link, so apply inverse of log to the matrix multiplication, i.e. exp\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Poisson(rate), obs=y)\n",
    "        return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianPoissonRegression(data.size(1))\n",
    "from pyro.infer.autoguide import AutoMultivariateNormal\n",
    "\n",
    "# define black box autoguide for Stochastic Variational Inference using Pyro\n",
    "guide = AutoMultivariateNormal(model, init_loc_fn=init_to_mean)\n",
    "\n",
    "#define training loop using PyTorch infrastructure\n",
    "# Use PyTorch optimizer to build a Pyro Optimizer\n",
    "# Use Pyro to define default loss function 'Trace_ELBO'\n",
    "def train(model, guide, lr=0.01, n_steps=4000):\n",
    "    pyro.set_rng_seed(1)\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    gamma = 0.01  # final learning rate will be gamma * initial_lr\n",
    "    lrd = gamma ** (1 / n_steps)\n",
    "    adam = pyro.optim.ClippedAdam({'lr': lr, 'lrd': lrd})\n",
    "\n",
    "    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        elbo = svi.step(data, target)\n",
    "        if i % 500 == 0:\n",
    "          print(f\"Elbo loss: {elbo}\")\n",
    "    print(f\"Final Elbo loss: {elbo}\")\n",
    "\n",
    "train(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "num_samples = 1000\n",
    "predictive = Predictive(model, guide=guide, num_samples=num_samples)\n",
    "\n",
    "svi_samples = {k: v.reshape((num_samples,-1)).detach().cpu().numpy()\n",
    "               for k, v in predictive(data, target).items()\n",
    "               if k != \"obs\"}\n",
    "\n",
    "svi_samples.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now compare the distribution of the coefficients retrieved via SVI to the point estimates above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(reg.intercept_)\n",
    "sns.kdeplot(data = svi_samples['linear.bias']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.coef_)\n",
    "sns.kdeplot(data = svi_samples['linear.weight']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We used the black box guide for multivariate normal latent variables in this case. That will not always work! We can perform the same analysis using MCMC and it will give a similar answer in this case.(Yes, I checked!) \n",
    "\n",
    "#### But remember, MCMC will *always* be more precise in measuring the posterior. SVI is only an approximation, and the approximation is only as good as your guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
